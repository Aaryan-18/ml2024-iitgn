\documentclass{beamer}
\usepackage{tcolorbox}
\usepackage{forloop}
\usepackage{bm}

\usepackage{mathtools}
\mathtoolsset{showonlyrefs}  

%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}

\usetheme{metropolis}           % Use metropolis theme


\title{Multivariate Normal Distribution I}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle
  
  


\begin{frame}{Univariate Normal Distribution}

The probability density of univariate Gaussian is given as: $$f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$
	
also, given as 
$$f(x)\sim \mathcal{N}(\mu, \sigma^2)$$

with mean $\mu \in R$ and variance $\sigma^2 >0$ 

\end{frame}

\begin{frame}{Univariate Normal Distribution}
Pop Quiz: Why is the denominator the way it is? Let the normalizing constant be $c$ and let $g(x) = e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$.

\begin{gather}
\visible<1->{1 = \int_{-\infty}^{\infty} c \cdot g(x) dx\\}
\visible<2->{1 = \int_{-\infty}^{\infty} ce^{-\frac{(x - u)^2}{2\sigma^2}} dx}
\end{gather}
\visible<3->{Let's substitute $\frac{x - u}{\sqrt{2}\sigma}$ with $t$.}
\begin{gather}
\visible<4->{1 = \int_{-\infty}^{\infty} ce^{-t^2} dt \times \sqrt{2}\sigma\\}
\visible<5->{1 = \sqrt{2}\sigma c \times 2\int_{0}^{\infty} e^{-t^2} dt}
\end{gather}
\end{frame}

\begin{frame}{Univariate Normal Distribution}
	$$ \frac{2}{\sqrt{\pi}}\int_{0}^{\infty} e^{-t^2} dt $$
	The above expression is called error function and is it's value is denoted by $erf(t)$. In our case, we want $erf(\infty)$ which is equal to 1.
	
	\begin{gather}
	\visible<2->{1 = \sqrt{2\pi}\sigma c \times \frac{2}{\sqrt{\pi}}\int_{0}^{\infty} e^{-t^2} dt\\}
	\visible<3->{1 = \sqrt{2\pi}\sigma c \times 1\\}
	\visible<4->{\frac{1}{\sqrt{2\pi}\sigma} = c}
	\end{gather}
\end{frame}

\begin{frame}{Univariate Normal Distribution}
	\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{gp/1d-gp} 
\end{frame}

\begin{frame}{Univariate Normal Distribution}
	\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{gp/1d-gp-hist}
\end{frame}

%\begin{frame}{Univariate Normal Distribution}
%Add plots for 1d histogram with different bin-width
%\end{frame}

\begin{frame}{Univariate Normal Distribution}
	\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{gp/1d-gp-kde2}
\end{frame}


\begin{frame}{Bivariate Normal Distribution}
Bivariate normal distribution of two-dimensional random vector $\bf{X} =\begin{bmatrix}
X_{1} \\
X_{2} \\
\end{bmatrix}
$

\begin{gather}
	\bf{X} = \begin{pmatrix}
	X_1 \\
	X_2
	\end{pmatrix} \sim \mathcal{N}_2 (\mu , \Sigma)
\end{gather}

where, mean vector $\bm{\mu} =\begin{bmatrix}
\mu_{1} \\
\mu_{2} \\
\end{bmatrix}
=\begin{bmatrix}
\operatorname{E}[X_{1}] \\
\operatorname{E}[X_{2}] \\
\end{bmatrix}
$

and, covariance matrix $\Sigma$
$$\Sigma_{i,j} := \operatorname{E} [(X_i - \mu_i)( X_j - \mu_j)] = \operatorname{Cov}[X_i, X_j] $$
\end{frame}

\begin{frame}{Bivariate Normal Distribution}

Question: What is Cov(X, X)?

Answer: Var(X) = Cov(X, X) =  $\operatorname{E}[(X - \operatorname{E}[X])]^2$

In the case of univariate normal, Var(X) is written as $\sigma^2$

Question: What is the relation between $\Sigma_{i, j}$ and $\Sigma_{j, i}$?

Answer: They are the same!

Question: What can we say about the covariance matrix $\Sigma$?

Answer: It is symmetric. Thus $\Sigma = \Sigma^T$
\end{frame}

\begin{frame}{Correlation and Covariance}
If $X$ and $Y$ are two random variables, with means (expected values) $\mu_X$ and $\mu_Y$ and standard deviations $\sigma_X$ and $\sigma_Y$, respectively, then their covariance and correlation are as follows:

$$\text{cov}_{XY} = \sigma_{XY} = E[(X-\mu_X)\,(Y-\mu_Y)] $$

$$	\text{corr}_{XY} = \rho_{XY} = E[(X-\mu_X)\,(Y-\mu_Y)]/(\sigma_X \sigma_Y)
$$
so that
$$
\rho_{XY} = \sigma_{XY} / (\sigma_X \sigma_Y) $$

where $E$ is the expected value operator. 
\end{frame}

\begin{frame}{PDF of bivariate normal distribution}

We might have seen that 

$$f_X(X_1, X_2) = \frac{exp(\frac{-1}{2}(X-\mu)^T \Sigma^{-1}(X-\mu))}{2\pi |\Sigma|^\frac{1}{2}}$$

How do we get such a weird looking formula?!

\end{frame}

\begin{frame}{PDF of bivariate normal with no cross-correlation}

Let us assume no correlation between $X_1$ and $X_2$.

We have $\Sigma = \begin{bmatrix}
\sigma_1^2 & 0 \\
0 & \sigma_2^2 \\
\end{bmatrix}$

We have $f_X(X_1, X_2) = f_X(X_1)f_X(X_2)$

$$=\frac{1}{\sigma_1 \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{X_1-\mu_1}{\sigma_1}\right)^2} \times \frac{1}{\sigma_2 \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{X_2-\mu_2}{\sigma_2}\right)^2}$$

$$= \frac{1}{\sigma_1 \sigma_2 2\pi } e^{-\frac{1}{2}\{\left(\frac{X_1-\mu_1}{\sigma_1}\right)^2 + \left(\frac{X_2-\mu_2}{\sigma_2}\right)^2 \}}  $$
\end{frame}

\begin{frame}{PDF of bivariate normal with no cross-correlation}

Let us consider only the exponential part for now

$ Q =  \left(\frac{X_1-\mu_1}{\sigma_1}\right)^2 + \left(\frac{X_2-\mu_2}{\sigma_2}\right)^2 $

Question: Can you write Q in the form of vectors X and $\mu$?

$$
 = \begin{bmatrix}
	X_1 - \mu_1 &
	X_2 - \mu_2 \\
\end{bmatrix}_{1\times2}  g(\Sigma)_{2\times2} \begin{bmatrix}
X_1 - \mu_1 \\
X_2 - \mu_2 \\
\end{bmatrix}_{2\times1}
$$

Here $g(\Sigma)$ is a matrix function of $\Sigma$ that will result in $\sigma_1^2$ like terms in the denominator; also there is no cross-terms indicating zeros in right diagonal!

$g(\Sigma) = \begin{bmatrix}
 \frac{1}{\sigma_1^2}& 0  \\
 0 &  \frac{1}{\sigma_2^2} \\
\end{bmatrix}_{2\times2} = \frac{1}{\sigma_1^2 \sigma_2^2}\begin{bmatrix}
{\sigma_2^2}& 0  \\
0 &  {\sigma_1^2}   \\ 
\end{bmatrix}_{2\times2} = \frac{1}{|\Sigma|} \operatorname{adj(\Sigma)} = \Sigma^{-1}$
\end{frame}


\begin{frame}{PDF of bivariate normal with no cross-correlation}
Let us consider the normalizing constant part now.
$M = \frac{1}{\sigma_1 \sigma_2 2\pi }$
$=\frac{1}{2\pi \times |\Sigma|^{\frac{1}{2}}}$
\end{frame}


\begin{frame}{Bivariate Gaussian samples with cross-correlation  = 0}
	\begin{center}
		\includegraphics[width=\linewidth,height=\textheight - 10pt,keepaspectratio]{gp/2d-gp3}
	\end{center}
\end{frame}

\begin{frame}{Bivariate Gaussian samples with cross-correlation  $\neq$ 0}
	\begin{center}
		\includegraphics[width=\linewidth,height=\textheight - 10pt,keepaspectratio]{gp/2d-gp}
	\end{center}
\end{frame}

\begin{frame}{Intuition for Multivariate Gaussian}
	\begin{gather}
		p(\mathbf{x} ; \mu, \Sigma)=\frac{1}{(2 \pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(\mathbf{x}-\mu)^{T} \Sigma^{-1}(\mathbf{x}-\mu)\right)
	\end{gather}
	
	In the univariate case, the term $-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}$ in the exponential was a quadratic function of $x$. Being a quadratic function, it was a parabola which opened downwards because of the leading negative coefficient.
	
	In the case of multivariate Gaussian, the term in exponential is a quadratic form in vector $\bf {x}$. Since $\Sigma$ is positive definite, this quadratic opens a high dimensional ``bowl'' oriented downwards
	
\end{frame}

\urldef\urld\url{https://brilliant.org/wiki/multivariate-normal-distribution/}

\begin{frame}{Intuition for Multivariate Gaussian \footnote{Courtesy: \urld}}
	The normalizing factor used above is chosen in such a way that,
	$\frac{1}{(2 \pi)^{\frac{1}{2}}|\Sigma|^{\frac{1}{2}}} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \ldots \int_{-\infty}^{\infty} \exp \left(-\frac{1}{2}(\mathbf{x}-\mu)^{T} \Sigma^{-1}(\mathbf{x}-\mu)\right)=1$
\end{frame}

\end{document}