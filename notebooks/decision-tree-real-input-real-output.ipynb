{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Retina display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from latexify import latexify, format_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "x = np.array([1, 2, 3, 4, 5, 6])\n",
    "y = np.array([0, 0, 1, 1, 2, 2])\n",
    "\n",
    "# plot data\n",
    "latexify()\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(x, y, color='k')\n",
    "format_axes(plt.gca()) \n",
    "plt.savefig(\"../figures/decision-trees/ri-ro-dataset.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth 0 tree\n",
    "# Average of all y values\n",
    "y_pred = np.mean(y)\n",
    "# Plot data\n",
    "latexify()\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(x, y, color='C1', label='data')\n",
    "# Plot prediction\n",
    "plt.plot([0, 7], [y_pred, y_pred], color='k', linestyle='-', label='Prediction')\n",
    "format_axes(plt.gca())\n",
    "plt.legend()\n",
    "plt.savefig(\"../figures/decision-trees/ri-ro-depth-0.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def create_DT_Regressor(x, y, depth, filename):\n",
    "    dt = DecisionTreeRegressor(max_depth=depth)\n",
    "    dt.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "    # Plot data\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    latexify()\n",
    "    plt.scatter(x, y, color='C1', label='Data')\n",
    "\n",
    "    x_test = np.linspace(0, 7, 500)\n",
    "    y_test = dt.predict(x_test.reshape(-1, 1))\n",
    "    plt.plot(x_test, y_test, color='k', label='Prediction')\n",
    "    format_axes(plt.gca())\n",
    "    plt.legend()\n",
    "    plt.savefig(\"../figures/decision-trees/\"+filename+\".pdf\")\n",
    "    return dt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_one = create_DT_Regressor(x, y, 1, \"ri-ro-depth-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "def create_graph(dt, filename, feature_names=['x']):\n",
    "    dot_data = export_graphviz(dt, out_file=None, feature_names=feature_names, filled=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graph.format = 'pdf'\n",
    "    graph.render(\"../figures/decision-trees/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph(dt_one, \"ri-ro-depth-1-sklearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_two = create_DT_Regressor(x, y, 2, \"ri-ro-depth-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph(dt_two, \"ri-ro-depth-2-sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sine daatset\n",
    "x = np.linspace(0, 2*np.pi, 200)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "latexify()\n",
    "plt.scatter(x, y, color='k', s=1)\n",
    "format_axes(plt.gca())\n",
    "plt.savefig(\"../figures/decision-trees/sine-dataset.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_sine_one = create_DT_Regressor(x, y, 1, \"sine-depth-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph(dt_sine_one, \"sine-depth-1-sklearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_sine_four = create_DT_Regressor(x, y, 4, \"sine-depth-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias-Variance Tradeoff - Dataset I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset for showing bias-variance tradeoff\n",
    "X = np.array([[1, 1],[2, 1],[3, 1],[5, 1],\n",
    "              [6, 1],[7, 1],[1, 2],[2, 2],\n",
    "              [6, 2],[7, 2],[1, 4],[7, 4]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1, 0, 1, 0, 1 ,0, 1])\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "format_axes(plt.gca())\n",
    "plt.savefig(\"../figures/decision-trees/bias-variance-dataset.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def create_DT_Classifier(X,y,depth,filename):\n",
    "    dt = DecisionTreeClassifier(max_depth=depth)\n",
    "    dt.fit(X, y)\n",
    "\n",
    "    # Predict in entire 2d space and contour plot\n",
    "    x1 = np.linspace(0, 8, 100)\n",
    "    x2 = np.linspace(0, 5, 100)\n",
    "\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    X_test = np.stack([X1.flatten(), X2.flatten()], axis=1)\n",
    "    y_test = dt.predict(X_test)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.contourf(X1, X2, y_test.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\n",
    "    format_axes(plt.gca())\n",
    "    plt.savefig(\"../figures/decision-trees/\"+filename+\".pdf\")\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_bias_variance_one = create_DT_Classifier(X, y, 1, \"bias-variance-depth-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph(dt_bias_variance_one, \"bias-variance-depth-1-sklearn\", feature_names=['x1', 'x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_bias_variance_full_depth = create_DT_Classifier(X, y, None, \"bias-variance-full-depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph(dt_bias_variance_full_depth, \"bias-variance-full-depth-sklearn\", feature_names=['x1', 'x2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias-Variance Tradeoff - Dataset II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x246fd6eeb00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bias variance dataset 2\n",
    "# X is all integers from (1, 1) to (6, 6)\n",
    "X = np.array([[i, j] for i in range(1, 7) for j in range(1, 7)])\n",
    "y = np.zeros(len(X), dtype=int)\n",
    "y[(2 <= X[:, 0]) & (X[:, 0] <= 5) & (2 <= X[:, 1]) & (X[:, 1] <= 5)] = 1\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_condition = (X[:, 0] == 3) & (X[:, 1] == 3) | (X[:, 0] == 4) & (X[:, 1] == 4)\n",
    "y[special_condition] = 0\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y) \n",
    "format_axes(plt.gca())\n",
    "plt.savefig(\"../figures/decision-trees/bias-variance-dataset-2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test random uniform frmo (1, 1) to (6, 6) of size 1000\n",
    "X_test = np.random.uniform(1, 6, size=(1000, 2))\n",
    "y_test = np.zeros(len(X_test), dtype=int)\n",
    "y_test[(2 <= X_test[:, 0]) & (X_test[:, 0] <= 5) & (2 <= X_test[:, 1]) & (X_test[:, 1] <= 5)] = 1\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=0.1)\n",
    "format_axes(plt.gca())\n",
    "plt.savefig(\"../figures/decision-trees/bias-variance-dataset-2-test.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DT_Classifier_with_graph(X,y,depth,filename):\n",
    "    dt = DecisionTreeClassifier(max_depth=depth)\n",
    "    dt.fit(X, y)\n",
    "\n",
    "    # Predict in entire 2d space and contour plot\n",
    "    x1 = np.linspace(0.5, 6.5, 100)\n",
    "    x2 = np.linspace(0.5, 6.5, 100)\n",
    "\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    X_contour = np.stack([X1.flatten(), X2.flatten()], axis=1)\n",
    "    y_contour = dt.predict(X_contour)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.contourf(X1, X2, y_contour.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\n",
    "    format_axes(plt.gca())\n",
    "    plt.savefig(\"../figures/decision-trees/\" + filename + \".pdf\")\n",
    "\n",
    "    # Export tree\n",
    "    dot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], filled=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graph.format = 'pdf'\n",
    "    graph.render(\"../figures/decision-trees/\"+filename+\"-sklearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Underfitting\n",
    "create_DT_Classifier_with_graph(X, y, 2, \"bias-variance-depth-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overfitting\n",
    "create_DT_Classifier_with_graph(X, y, None, \"bias-variance-full-depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Good Fit\n",
    "create_DT_Classifier_with_graph(X, y, 4, \"bias-variance-good-fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "### Train and test accuracy vs depth\n",
    "depths = np.arange(2, 10)\n",
    "train_accs = {}\n",
    "test_accs = {}\n",
    "for depth in depths:\n",
    "    dt = DecisionTreeClassifier(max_depth=depth)\n",
    "    dt.fit(X, y)\n",
    "    train_accs[depth] = accuracy_score(y, dt.predict(X))\n",
    "    test_accs[depth] = accuracy_score(y_test, dt.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs = pd.Series(train_accs)\n",
    "test_accs = pd.Series(test_accs)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(depths, train_accs, label='Train')\n",
    "plt.plot(depths, test_accs, label='Test')\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.1)\n",
    "format_axes(plt.gca())\n",
    "plt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth.pdf\")\n",
    "\n",
    "# Highlight area of underfitting (depth < 4) fill with green \n",
    "plt.fill_between(depths, 0, 1, where=depths <= 4, color='g', alpha=0.1, label='Underfitting')\n",
    "plt.legend()\n",
    "plt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-underfitting.pdf\")\n",
    "\n",
    "\n",
    "# Highlight area of overfitting (depth >7 4) fill with red\n",
    "plt.fill_between(depths, 0, 1, where=depths >= 7, color='r', alpha=0.1, label='Overfitting')\n",
    "plt.legend()\n",
    "plt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-overfitting.pdf\")\n",
    "\n",
    "\n",
    "# Highlight good fit area (4 < depth < 7) fill with blue\n",
    "plt.fill_between(depths, 0, 1, where=(depths >= 4) & (depths <= 7), color='b', alpha=0.1, label='Good fit')\n",
    "plt.legend()\n",
    "plt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-good-fit.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slight variation of the dataset leads to a completely different tree\n",
    "y = np.zeros(len(X), dtype=int)\n",
    "y[(2 <= X[:, 0]) & (X[:, 0] <= 5) & (2 <= X[:, 1]) & (X[:, 1] <= 5)] = 1\n",
    "special_condition = (X[:, 0] == 3) & (X[:, 1] == 3) | (X[:, 0] == 4) & (X[:, 1] == 3)\n",
    "y[special_condition] = 0\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "format_axes(plt.gca())\n",
    "plt.savefig(\"../figures/decision-trees/bias-variance-dataset-2-2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_DT_Classifier_with_graph(X, y, None, \"bias-variance-full-depth-2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
