\documentclass{beamer}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[labelformat=empty]{caption}
\usepackage{subcaption}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 


%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}
	


\usetheme{metropolis}           % Use metropolis theme


\title{Ridge Regression}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle

\begin{frame}{Introduction}
A know measure of over-fitting can be the magnitude of the coefficient. \\ \bigskip
\pause
In $f(x) = c_0 + c_1x + c_2x^2 + \dots$ it is $\max|c_i|$ \\ \bigskip
\end{frame}
  
\begin{frame}{Introduction}
\vspace{0.4cm}
\only<1>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/lin_1.pdf}\caption{Base Data Set}
\end{figure}}
\only<2>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/lin_plot_1.pdf}\caption{Fit with Degree 1}
\end{figure}}
\only<3>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/lin_plot_3.pdf}\caption{Fit with Degree 3}
\end{figure}}
\only<4>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/lin_plot_6.pdf}\caption{Fit with Degree 6}
\end{figure}}
\only<5>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/lin_plot_11.pdf}\caption{Fit with Degree 11}
\end{figure}}
\end{frame}  

\begin{frame}{Introduction}
\vspace{0.4cm}
In the examples we notice that as the degree increase (as the prediction starts to overfit the base data), the maximum coefficient also increases.
%\vspace{-0.3cm}
\begin{figure}\includegraphics[width=0.7\linewidth]{ridge/lin_plot_coef.pdf}\caption{Trend of the coefficients}\end{figure}

\end{frame}
 
\begin{frame}{Introduction}
\vspace{0.4cm}
To prevent over fitting we place penalties on large $\theta_i$
\pause
 \\ \bigskip
\begin{tcolorbox}{Objective}
\begin{align*}
\text{Minimize } & \left(y-X\theta\right)^T\left(y-X\theta\right) \\
\text{s.t. } & \theta ^T\theta \leq S
\end{align*}
\end{tcolorbox}
\pause
This is equivalent to \vspace{-0.4cm}
$$
\text{Minimize } \left(y-X\theta\right)^T\left(y-X\theta\right) + \delta ^2\theta ^T\theta
$$
\end{frame}  

\begin{frame}{Introduction}
\begin{figure}
        \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\linewidth]{ridge/ridge_base_contour.pdf}
                \caption{Contour Plot}
        \end{subfigure}%
        \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\linewidth]{ridge/ridge_base_surface.pdf}
                \caption{Surface Plot}
        \end{subfigure}%
        \caption{Visualization of the Example}
\end{figure}
\end{frame}

\begin{frame}{KKT Conditions}
To implement this we use KKT Conditions
\pause
\begin{align*}
\text{Minimize } & \left(y-X\theta\right)^T\left(y-X\theta\right) \\
\text{s.t. } & \theta ^T\theta \leq S \\
L\left(\theta, \mu \right) =& \left(y-X\theta\right)^T\left(y-X\theta\right) + \mu\left(\theta^T\theta - S\right)
\end{align*}
where, $\mu \geq 0$ \bigskip

\pause
\begin{columns}
\begin{column}{0.4\textwidth}
If $\mu = 0$ \\
There is no regularization \\
No effect on constraint
\end{column}
\pause
\begin{column}{0.4\textwidth}
If $\mu\neq 0$ \\
$\implies \theta^T\theta - S = 0$ 
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Effect of $\mu$}
\vspace{0.4cm}
\only<1>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_1.pdf}\caption{$\mu = 1$}
\end{figure}}
\only<2>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_10.pdf}\caption{$\mu = 10$}
\end{figure}}
\only<3>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_1000.pdf}\caption{$\mu = 1000$}
\end{figure}}
\end{frame}

\begin{frame}{Effect of $\mu$ on higher degree fits}
\vspace{0.4cm}
\only<1>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_1_16.pdf}\caption{$\mu = 1$ when Degree $= 16$}
\end{figure}}
\only<2>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_100_16.pdf}\caption{$\mu = 100$  when Degree $= 16$}
\end{figure}}
\only<3>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_100000_16.pdf}\caption{$\mu = 100000$  when Degree $= 16$}
\end{figure}}
\end{frame}

\begin{frame}{Effect of $\mu$ - Regularization of Parameters}
\vspace{0.4cm}
\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/rid_reg.pdf}\caption{Comparing the magnitudes of the coefficients with varying $\mu$\\(on the \emph{Real Estate Data Set})}
\end{figure}
\end{frame}

\begin{frame}{Analytical Method}
\begin{tcolorbox}{Ridge Objective}
\vspace{-0.4cm}
$$
\min_\theta \left(y-X\theta\right)^T\left(y-X\theta\right)\delta^2\theta^T\theta
$$
\end{tcolorbox}
\begin{align*}
\frac{\partial L\left(\theta, \mu\right)}{\partial \theta} &= 0 \\ 
\frac{\partial}{\partial \theta}\left\lbrace y^Ty - 2y^TX\theta + \theta^TX^TX\theta \right\rbrace + \frac{\partial}{\partial \theta} \delta^2\theta^T\theta &= 0 \\
\implies -X^Ty + \left(X^TX + \delta^2I\right)\theta &= 0 \\
\implies \theta^* &= \left(X^TX + \delta^2I\right)^{-1}X^Ty
\end{align*}
\end{frame}

\begin{frame}{Bias/Variance}
\begin{columns}
\begin{column}{0.4\textwidth}
\begin{figure}
\includegraphics[width=\linewidth]{ridge/ridge_1_19.pdf}
\end{figure}
Fit High Order Polynomial \\
$\implies$ high variance \\
$\implies \, \delta^2 \rightarrow 0$
\end{column}
\begin{column}{0.4\textwidth}
\begin{figure}
\includegraphics[width=\linewidth]{ridge/ridge_10000000000_0_19.pdf}
\end{figure}
Fit High Order Polynomial \\
$\implies$ low variance \\
$\implies \, \delta^2 \rightarrow \infty$
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Example}
\vspace{0.4cm}
\textbf{Q.)} Solve Regularized ($\delta^2 = 2$) and Unregularized.
\vspace{-0.6cm}
\begin{figure}
\includegraphics[width=\linewidth]{ridge/temp.pdf}
\end{figure}
\end{frame}

\begin{frame}{Extension of the analytical model}
For ridge with no penalty on $\theta_0$
$$
\hat{\theta} = \left(X^TX+\delta^2I^*\right)^{-1}X^Ty
$$
where, $$I = \begin{bmatrix}
    \color{red}0 & 0 & 0 & \dots  & 0 \\
    0 & 1& 0 & \dots  & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots  & 1
\end{bmatrix}$$
\end{frame}

\end{document}