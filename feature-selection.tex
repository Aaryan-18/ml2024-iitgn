\documentclass{beamer}
\usepackage{tcolorbox}

%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}


\usetheme{metropolis}           % Use metropolis theme


\title{Feature Selection}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle
  
  
  
% \section{Linear Regression}


\begin{frame}{Ideas for Baselines}
    
    
    \begin{itemize}
        \item Mean Model, return the Mean of the training set
        \item Median or Mode of the training set
        \item Random (Min(training set), Max(training set))
    \end{itemize}
\end{frame}


\begin{frame}{Choosing Best features}
    To find the best set of features, one can do exhaustive enumeration(brute force) of features.    
    
    \begin{tabular}{|c|c|c|\dots|c|}
        $Feature_{1}$ & $Feature_{2}$ &\dots & $Feature_{d}$  \\
        \hline
        \hline
         True  & False &  \dots & False\\
         False &  True & \dots & False\\ 
         True &  True & \dots & False\\ 
         \vdots &  \vdots & \vdots\\
         True & True &\dots & True\\ 
         
    \end{tabular}
    
    The entries of the table denote if the feature is used for creating a model. In total we have $2^{d}$ models. This is the reason why it takes more time for training models using exhaustive enumeration.
\end{frame}


\begin{frame}{stepwise Forward Selection}
    It is a greedy algorithm.\\
    sel\_features = $\{\}$\\
    for i= 1 to d\\
    \hspace{1em} for feature in f_{1} \dots f_{d}:\\
    \hspace{2em} train model with [sel\_features $\cup$ feature]\\
    \hspace{2em} Validate the model\\
    
    \hspace{1em} best\_feature  = The feature with best validation performance\\
    sel\_features = sel\_features \cup best\_feature
    
\end{frame}

\begin{frame}{Stepwise Backward Selection}
    Same as SFS, but in opposite direction\\
    Remove feature, which reduces the accuracy the least(uninmportant).
\end{frame}

\begin{frame}{Time Complexity Analysis}
    
    Both SFS and SBS are $O(d^2)$ algorithms, where d is the number of features.
    
     \begin{equation*}
         \implies (d) + (d-1) + (d-2) + \dots + (1)\\
           
     \end{equation*}
     
     \begin{equation*}
          \implies \cfrac{d(d-1)}{2}\\
            
     \end{equation*}
     
     \begin{equation*}
         \implies d(d-1)  \implies d^{2}
           
           
     \end{equation*}
\end{frame}
\end{document}