\documentclass{beamer}
\usepackage{tcolorbox}

%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}


\usetheme{metropolis}           % Use metropolis theme


\title{Some Practical Tips And Feature Selection}
\date{\today}
\author{Nipun Batra and teaching staff}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle
  
  
  
% \section{Linear Regression}


\begin{frame}{Ideas for Baselines}
    
    
    \begin{itemize}[<+->]
        \item Mean Model: $\hat{y}$= Mean of the training set
        \item Median or Mode of the training set
        \item Random (Min(training set), Max(training set))
    \end{itemize}
\end{frame}


\begin{frame}{Choosing Best features}
    To find the best set of features, one can do exhaustive enumeration (brute force) of features.    
    
    \pause \begin{tabular}{c|c|c|c}
        $Feature_{1}$ & $Feature_{2}$ &\dots & $Feature_{d}$  \\
        \hline
      
         True  & False &  \dots & False\\
         False &  True & \dots & False\\ 
         True &  True & \dots & False\\ 
         \vdots &  \vdots & \vdots&\\
         True & True &\dots & True\\ 
         \hline 
         
    \end{tabular}
    
    \pause The entries of the table denote if the feature is used for creating a model. In total we have $2^{d}$ models: training models using exhaustive enumeration is very expensive!
\end{frame}


\begin{frame}{Stepwise Forward Selection}
    \pause It is a greedy algorithm.\\
    sel\_features = $\{\}$\\
    for i= 1 to d\\
    \hspace{1em} for feature in $f_{1} \dots f_{d}$:\\
    \hspace{2em} train model with [sel\_features $\cup$ feature]\\
    \hspace{2em} Validate the model\\
    
    \hspace{1em} best\_feature  = The feature with best validation performance\\
    sel\_features = sel\_features $\cup$ best\_feature
    
\end{frame}

\begin{frame}{Stepwise Backward Selection}
    Same as SFS, but in opposite direction\\
    Remove feature, which reduces the accuracy the least(uninmportant).
\end{frame}

\begin{frame}{Time Complexity Analysis}
    
    Both SFS and SBS are $O(d^2)$ algorithms, where d is the number of features.
    
     \begin{equation*}
         \implies (d) + (d-1) + (d-2) + \dots + (1)
           \end{equation*}
     
     \begin{equation*}
          \implies \cfrac{d(d-1)}{2}
            \end{equation*}
     
     \begin{equation*}
         \implies d(d-1)  \implies d^{2}
           \end{equation*}
\end{frame}
\end{document}