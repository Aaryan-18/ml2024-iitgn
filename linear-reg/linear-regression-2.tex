\documentclass{beamer}
\usepackage{calculator}

\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}
\pgfplotsset{width=6cm,compat=1.14}
\newcommand{\vecuvec}[2] %start point, end point (of vector)
{   \VECTORSUB(#2)(#1)(\sola,\solb,\solc)
	\UNITVECTOR(\sola, \solb, \solc)(\sola,\solb,\solc)
	%arrow in blue
	\draw[->,thick,blue] (#1) -- (#2); 
	%corresponding unit-vector in red:
	\edef\temp{\noexpand\draw[->, thick,red] (#1) -- ($(#1)+(\sola,\solb,\solc)$);}
	\temp
}

\usetikzlibrary{calc}

%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}


\usetheme{metropolis}           % Use metropolis theme


\title{Linear Regression II}
\date{\today}
\author{Nipun Batra and the teaching staff}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle
  
  
  







% \begin{frame}{Frame Title}
    
%     \begin{align}
        
%         \begin{split}
%             \theta &= \frac{1}{20}\begin{bmatrix}
%                 14 & -6\\
%                 -6 & 4
%             \end{bmatrix}
%             \begin{bmatrix}
%                 6\\
%                 14
%             \end{bmatrix}\\
%             &= \frac{1}{20} \begin{bmatrix}
%                 0 \\
%                 20
%             \end{bmatrix}\\
            
%             % \begin{bmatrix}
%             %     \theta_{0}\\
%             %     \theta_{1}
%             % \end{bmatrix}
%             %  = 
%             %  \begin{bmatrix}
%             %      0 \\
%             %      1
%             %  \end{bmatrix}
%         \end{split}
%     \end{align}
%     $$
% \end{frame}

%\begin{frame}{Quick Question}
%    
%    $$
%    y = X\theta;
%    $$
%    
%    Can we do
%    $$theta = X^{-1}y$$
%    
%    Why? or why not?
%    
%\end{frame}


%\begin{frame}{Quick Question}
%
%    X may not be a square matrix. So, $X^{-1}$ may not exist.
%    
%    Rectangular matrices have a left inverse and a right inverse.
%\end{frame}



\begin{frame}{Relation between \#instances and \# Variables}
    
    If N$<$ M, then it is an under-determined system\\
   \pause  Example: N=2; M=3\\
 
   \pause $$ \begin{bmatrix}
        30 \\
        40 
    \end{bmatrix}
     = \begin{bmatrix}
         1 & 6& 30\\
         1 & 5& 20
     \end{bmatrix}    \begin{bmatrix}
        \theta_{0}\\
        \theta_{1}\\
        \theta_{2}\\
    \end{bmatrix} $$
 
    
    
  \pause   \begin{align}
\label{eqn*:eqlabel}
\begin{split}
   30 &= \theta_{0} + 6\theta_{1} + 30\theta_{2} \\
40 &= \theta_{0} + 5\theta_{1} + 20\theta_{2}\\
\hline
-10 &=  -1 \theta_{1} -10\theta_{2}
\end{split}
\end{align}

    
        
    %     \begin{split}
    %     %     30 &= \theta_{0} + 6\theta_{1} + 30\theta_{2} \\
    %     %     40 &= \theta_{0} + 5\theta_{1} + 20\theta_{2} \\
        
    %     % -10 &=  -1 \theta_{1} -10\theta_{2} \\
            
    %     \end{split}
    
    
    % \end{align}
    % \\
    
    The above equation can have infinitely many solutions. \\
    Under-determined system: $\epsilon_{i} = 0$ for all $i$

\end{frame}

\begin{frame}{Relation between \#instances and \# Variables}
   What if N $>$ M\\
   \pause   Then it is an over determined system. So, the sum of squared residuals $>$ 0.
\end{frame}



\begin{frame}{Variable Transformation}
    Transform the data, by including the higher power terms in the feature space. 
    
       
    \begin{center}
 \begin{tabular}{||c c||} 
 \hline
 t  & s \\ [0.5ex] 
 \hline\hline
 0 & 0 \\
 1 & 6 \\
 3 & 24 \\
 4 & 36 \\
 \hline
\end{tabular}
\end{center}

The above table represents the data before transformation
\end{frame}


\begin{frame}{Variable Transformation}
Add the higher degree features to the previous table
    
       
    \begin{center}
 \begin{tabular}{||c c c||} 
 \hline
 t  & $t^{2}$ & s \\ [0.5ex] 
 \hline\hline
 0 & 0&0 \\
 1 & 1&6 \\
 3 & 9&24 \\
 4 & 16&36 \\
 \hline
\end{tabular}
\end{center}

\pause The above table represents the data after transformation \\
\pause Now, we can write $\hat{s}=f(t, t^2)$ \\
\pause Other transformations: $\log(x), x_1\times x_2$
\end{frame}

\begin{frame}{A big caveat: Linear in what?!\footnotemark}
\begin{enumerate}[<+->]
	\item $\hat{s}=\theta_0 + \theta_1*t$
	 is linear
	 \item Is $\hat{s}=\theta_0 + \theta_1*t + \theta_2*t^2$
	 linear?
	 \item Is $\hat{s}=\theta_0 + \theta_1*t + \theta_2*t^2 + \theta_3*\cos(t^3)$
	 linear?
	\item Is $\hat{s}=\theta_0 + \theta_1*t + {\rm e}^{\theta_2}*t$
	linear?
	\item All except \#4 are linear models! 
	\item Linear refers to the relationship between the parameters that you are estimating ($\theta$) and the outcome 
\end{enumerate}
\footnotetext[1]{\url{https://stats.stackexchange.com/questions/8689/what-does-linear-stand-for-in-linear-regression}}
\end{frame}


\begin{frame}{Class Exercise}
Solve the linear system below using normal equation method
\begin{center}
	\begin{tabular}{||c c | c||} 
		\hline
		$x_{1}$  & $x_{2}$ & y  \\ [0.5ex] 
		\hline\hline
		1 & 2 & 4 \\
		2 & 4 & 6\\
		3 & 6 & 8\\
		\hline
	\end{tabular}
\end{center}
\end{frame}
\begin{frame}{Multi-collinearity}
There can be situations where $X^{T}X$ is not computable. \\
\pause This condition arises when the $|X^{T}X|$ = 0.

\begin{equation}
X = \begin{bmatrix}
1 & 1& 2\\
1 & 2& 4\\
1 & 3& 6\\
\end{bmatrix}
\end{equation}

\pause The matrix X is not full rank. 
\end{frame}





\begin{frame}{Multi-collinearity}

It arises when one or more predictor varibale/feature in X can be expressed as a linear combinations of others\\
\vspace{5mm}



How to tackle it?
\begin{itemize}
    \item<+-> Regularize
    \item<+-> Drop variables
    \item<+-> Use different subsets of data
    \item<+-> Avoid dummy variable trap
\end{itemize}
\end{frame}
\begin{frame}{Dummy variables}
Say Pollution in Delhi = P
\pause \begin{center}
	P = $\theta_{0}$ + $\theta_{1}$*\#Vehicles + $\theta_{1}$*
	\textit{Wind speed} + $\theta_{3}$ * \textit{Wind Direction}
\end{center}

\pause But, wind direction is a categorical variable. \\
\pause It is denoted as follows \{N:0, E:1, W:2, S:3 \}\\
\vspace{3em}

\pause Can we use the direct encoding? \\
\pause Then this implies that S$>$W$>$E$>$N
\end{frame}

\begin{frame}{Dummy Variables}
\begin{center}

N-1 Variable encoding\\
\vspace{1em}
\begin{tabular}{|c|c|c|c|}
	\hline
	& Is it N? &Is it E? &Is it W?\\
	\hline
	\hline
	N & 1&0&0 \\
	E & 0&1&0\\
	W & 0&0&1\\
	S & 0&0&0\\
	\hline
\end{tabular}

\end{center}
\end{frame}


\begin{frame}{Dummy Variables}
\begin{center}

N Variable encoding\\
\vspace{1em}
\begin{tabular}{|c|c|c|c|c|}
\hline
& Is it N? &Is it E? &Is it W? & Is it S?\\
\hline
\hline
N & 1&0&0&0 \\
E & 0&1&0&0\\
W & 0&0&1&0\\
S & 0&0&0&1\\
\hline
\end{tabular}
\end{center}
\end{frame}



\begin{frame}{Dummy Variables}
Which is better N variable encoding or N-1 variable encoding? \\

\pause The N-1 variable encoding is better because the N variable encoding can cause multi-collinearity. \\

\pause Is it S = 1 - (Is it N + Is it W + Is it E) 

\end{frame}


\begin{frame}{Binary Encoding}

\begin{center}
\begin{tabular}{|c|c|}
\hline
N & 00 \\
E& 01\\
W & 10\\
S& 11\\
\hline
\end{tabular}\\
\end{center}


\vspace{1em}
\pause W and S are related by one bit. 

\pause This introduces dependencies between them, and this can confusion in classifiers.
\end{frame}

\begin{frame}{Interpreting Dummy variables}
\begin{center}
\begin{tabular}{c|c}
Gender& height\\
\hline
\hline
F & \dots \\
F & \dots \\
F & \dots \\
M & \dots \\
M & \dots \\
\end{tabular}

\end{center}

\pause Encoding

\begin{center}
\pause \begin{tabular}{c|c}
Is Female& height\\
\hline
\hline
1 & \dots \\
1 & \dots \\
1 & \dots \\
0 & \dots \\
0 & \dots \\
\end{tabular}
\end{center}

\end{frame}

\begin{frame}{Interpreting Dummy Variables}
\begin{center}
	\pause \begin{tabular}{c|c}
		Is Female& height\\
		\hline
		\hline
		1 & 5 \\
		1 & 5.2 \\
		1 & 5.4 \\
		0 & 5.8 \\
		0 & 6 \\
	\end{tabular}
\end{center}
\pause $height_{i}$ = $\theta_{0}$ + $\theta_{1}$ *  (Is Female) + $\epsilon_{i}$\\
\vspace{1em}
\pause We get $\theta_0$ = 5.8 and $\theta_0$ = 6\\
\pause $\theta_{0}$ = Avg height of Male = 5.9\\
\pause $\theta_{0} + \theta_{1}$ is chosen based (equal to) on 5, 5.2, 5.4 (for three records). \\
\pause $\theta_{1}$ is chosen based on 5-5.9, 5.2-5.9, 5.4-5.9
\pause $\theta_{1}$ = Avg. female height (5+5.2+5.4)/3 - Avg. male height(5.9)
\end{frame}

\begin{frame}{Interpreting Dummy Variables}
Alternatively, instead of a 0/1 coding scheme, we could create a dummy variable

\pause \(x_{i}=\left\{\begin{array}{ll}{1} & {\text { if } i \text { th person is female }} \\ {-1} & {\text { if } i \text { th person is male }}\end{array}\right.\)

\pause \(y_{i}=\theta_{0}+\theta_{1} x_{i}+\epsilon_{i}=\left\{\begin{array}{ll}{\theta_{0}+\theta_{1}+\epsilon_{i}} & {\text { if } i \text { th person is female }} \\ {\theta_{0}-\theta_{1}+\epsilon_{i}} & {\text { if } i \text { th person is male. }}\end{array}\right.\)


\pause Now, $\theta_{0}$ can be interpreted as average person height. $\theta_{1}$ as the amount that female height is above average and male height is below average.
\end{frame}

%\begin{frame}{Modelling Interaction}
%    
%    $$
%    y = \theta_{0} + x_{1}\theta_{1} +
%    x_{1}\theta_{1} +
%    \dots + x_{m}\theta_{m} 
%    $$
%    
%    If $x_{1}$ increases by one unit, then y increases by $\theta_{1}$ units, irrespective of the interactive with $x_{2},x_{3},\dots,d_{m}$.
%    
%    $$
%    y = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{1}x_{2} + \dots
%    $$
%
%    This way we can model the interactions. 
%\end{frame}
\begin{frame}{Alternative parameter estimation}
    $$
    \hat{y_{i}} = \theta_{0} + \theta_{1}x_{i}
    $$
    
    \pause $$
    \epsilon_{i} = y_{i} - \hat{y}_{i}
    $$
    
    \pause $$
    \sum \epsilon_{i}^{2} = \sum (y_{i} - \theta_{0} - \theta_{1}x_{i})^{2}
    $$
    
    \pause Now, we compute the derivative of it with all the  $\theta_{j}$. Let us solve for x being a scalar.
    
    
\end{frame}




\begin{frame}{Alternative parameter estimation}

\begin{align}
    \begin{split}
        \frac{\partial}{\partial \theta_{0}}\sum \epsilon_{i}^{2} &= 2\sum(y_{i} -  \theta_{0} - \theta_{1}x_{i})(-1) = 0 \\
        0 &= \sum y_{i} -  N\theta_{0} - \sum \theta_{1}x_{i}\\
        \theta_{0} &= \frac{\sum y_{i} - \theta_{1}\sum x_{i}}{N}
    \end{split}
\end{align}


\begin{tcolorbox}
\begin{center}
    $ \theta_{0} = \bar{y} - \theta_{1} \bar{x}$
\end{center}
\end{tcolorbox}
\end{frame}
\begin{frame}{Alternative parameter estimation}

$$
\frac{\partial}{\partial \theta_{1}}\sum \epsilon_{i}^{2} = 0
$$


$$
\implies 2 \sum_{i=1}^{N} (y_{i} - \theta_{0} - \theta_{1}x_{i})(-x_{i}) = 0
$$

$$
\implies \sum_{i=1}^{N} (x_{i}y_{i} - \theta_{0}x_{i} - \theta_{1}x_{i}^{2}) = 0
$$

$$
\implies \sum  \theta_{1}x_{i}^{2} = \sum x_{i}y_{i} - \sum \theta_{0}x_{i}
$$

$$
\implies \sum  \theta_{1}x_{i}^{2} = \sum x_{i}y_{i} - \sum (\bar{y} - \theta_{1}\bar{x})x_i
$$


\end{frame}

\begin{frame}{Alternative parameter estimation}

$$
\implies \sum  \theta_{1}x_{i}^{2} = \sum x_{i}y_{i} - \bar{y}\sum x_{i} + \theta_{1}\bar{x}\sum x_{i} 
$$

$$
\implies \sum  
x_{i}y_{i} - \sum x_{i}y = \theta_{1} (-\bar{x}\sum x_{i} + \sum x_{i}^{2})
$$

$$
\theta_{1} = \frac{x_{i}y_{i} - \sum x_{i}y}{\sum x_{i}^{2} -\bar{x}\sum x_{i}}
$$

    
\end{frame}

\begin{frame}{Alternative parameter estimation}
    
    $$
    \theta_{1} = \frac{ \frac{1}{N} \sum_{i=1}^{N}(x_{i} - \bar{x})(y_{i} - \bar{y})}{\frac{1}{N}(x_{i} - \bar{x})^{2}}
    $$
    
    \begin{tcolorbox}
    
    $$
    \theta_{1} = \frac{Cov(x,y)}{variance(x)}
    $$
    \end{tcolorbox}
\end{frame}
% \begin{frame}{Derivation of Normal Rule}
%     \begin{equation*}
%          = \\
        
%     \end{equation*}
% \end{frame}

% ${\text{minimize }} \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2$


% \begin{equation*}

%     \displaystyle{\minimize \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2 }
% \end{equation*}
    
% $\displaystyle{\minimize \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2 }$






% \begin{frame}{There can be multiple equa}
    
% \end{frame}

% =
% \begin{bmatrix}
%     x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
%     x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
%     \vdots & \vdots & \vdots & \ddots & \vdots \\
%     x_{d1} & x_{d2} & x_{d3} & \dots  & x_{dn}
% \end{bmatrix}





% \begin{frame}{Another example on Bayes rule}
% \end{frame}


% \begin{frame}{Bayes Rule for Machine Learning}
% \begin{itemize}


%     \item $P(A|B)P(B) = P(B|A)P(A)$
%     \item Let us consider for a machine learning problem:
%     \begin{itemize}
%     	\item A = Parameters ($\theta$)
%     	\item B = Data ($\mathcal{D}$)
%     \end{itemize}
% \item We can rewrite the Bayes rule as:
% \begin{itemize}
% 	\item $P(\theta|\mathcal{D}) = \frac{P(\mathcal{D}|\theta)P(\theta)}{P(\mathcal{D})}$
% 	\item Posterior: 
% 	\item Prior:
% 	\item Likelihood
% 	\item 
% \end{itemize}
% \end{itemize}
% \end{frame}

% \begin{frame}{Likelihood}
% \begin{itemize}
% 	\item Likelihood is a function of $\theta$
% 	\item Given a coin flip and 5 H and 1 T, what is more likely: P(H) = 0.5 or P(H) = 1
% \end{itemize}
% \end{frame}

% \begin{frame}{Bayesian Learning is well suited for online settings}
% content...
% \end{frame}

% \begin{frame}{Coin flipping}
% \begin{itemize}
% 	\item Assume we do a coin flip multiple times and we get the following observation: \{H, H, H, H, H, H, T, T, T, T\}: 6 Heads and 4 Tails
% 	\item  What is $P(Head)$?
% 	\item Is your answer: 6/10. Why?
% \end{itemize}

% \end{frame}

% \begin{frame}{Coin flipping: Maximum Likelihood Estimate (MLE)}
% \begin{itemize}
% 	\item We have $\mathcal{D} = \{\data_1, \data_2, ...\data_{N}\}$ for $N$ observations where each $\mathcal{D}_i \in \{H, T\}$
% 	\item Assume we have $n_H$ heads and $n_T$ tails, $n_H + n_T = N$
% 	\item Let us have $P(H) = \theta, P(T) = 1-\theta$
% 	\item We have Likelihood, $L(\theta) = P(\mathcal{D}|\theta) = P(\data_1, \data_2, ..., \data_N|\theta)$
% 	\item Since observations are i.i.d., $L(\theta) = P(\data_1|\theta).P(\data_2|\theta) ... P(\data_N|\theta)$
% \end{itemize}

% \end{frame}


% \begin{frame}{Coin flipping: Maximum Likelihood Estimate (MLE)}
% \begin{itemize}
% 	\item  
% \begin{align*}  
% P(\data_i|\theta) =  \left
% \{\begin{array}{lr} \theta, & \text{for~} \data_i =H \\
% 1-\theta, & \text{for~} \data_i = T
% \end{array}\right.\
% \end{align*}  
% \item Thus, $L(\theta) = \theta^{n_H}\times (1-\theta)^{n_T}$
% \item Log-Likelihood, $LL(\theta) = n_Hlog\theta + (n_T)(log(1-\theta))$
% \item $\frac{\partial LL(\theta)}{\partial \theta} = \frac{n_H}{\theta} - \frac{n_T}{1-\theta}$
% \item  For maxima, set derivative of LL to zero

% \item 	$\frac{n_H}{\theta} - \frac{n_T}{1-\theta} = 0 $
% \end{itemize}
% \begin{tcolorbox}
% 	\begin{align*}
% 	 \theta = \frac{n_H}{n_H + n_T}
% 	\end{align*}

% \end{tcolorbox}
% Question: Is this maxima or minima?

% \end{frame}

% \begin{frame}{}
% \begin{align*}
% \frac{\partial^2 LL(\theta)}{\partial \theta^2} = \frac{-n_H}{\theta^2} + \frac{-n_T}{(1-\theta)^2} \in \mathbb R_-
% \end{align*}
% Thus, the solution is a maxima
% \end{frame}

% \begin{frame}{Maximum A Posteriori estimate (MAP)}
% \begin{itemize}


% \item \textbf{MLE does not handle prior knowledge}: What if we know that our coin is biased towards head?
% \item \textbf{MLE can overfit}: What is the probability of heads when we have observed 6 heads and 0 tails?
% \end{itemize}

% \end{frame}


% \begin{frame}{Maximum A Posteriori estimate (MAP)}
% Goal: Maximize the Posterior
% \begin{tcolorbox}
% 	\begin{align}
% \hat{\theta}_{MAP} = \argmin_\theta P(\theta|\data)\\
% \hat{\theta}_{MAP}= \argmin_\theta P(\data|\theta)P(\theta)
% \end{align}
% \end{tcolorbox}

% \end{frame}

% \begin{frame}{Prior distributions}
% \end{frame}

% \begin{frame}{Beta Distribution}
% \end{frame}

% \begin{frame}{Beta Distribution}
% \end{frame}

% \begin{frame}{Coin toss: MAP estimate}
% \end{frame}

% \begin{frame}{Linear Regression: MLE}
% We previously saw 
% \begin{tcolorbox}
% \begin{align*}
% \hat{\theta}_{least-square} = \argmin_\theta \epsilon^T\epsilon = \argmin_\theta (y-X\theta)^T(y-X\theta)
% \end{align*}
% \end{tcolorbox}


% \end{frame}

\end{document}