\documentclass{beamer}
\usepackage{calculator}

\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}
\pgfplotsset{width=6cm,compat=1.14}
\newcommand{\vecuvec}[2] %start point, end point (of vector)
{   \VECTORSUB(#2)(#1)(\sola,\solb,\solc)
	\UNITVECTOR(\sola, \solb, \solc)(\sola,\solb,\solc)
	%arrow in blue
	\draw[->,thick,blue] (#1) -- (#2); 
	%corresponding unit-vector in red:
	\edef\temp{\noexpand\draw[->, thick,red] (#1) -- ($(#1)+(\sola,\solb,\solc)$);}
	\temp
}

\usetikzlibrary{calc}

%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}


\usetheme{metropolis}           % Use metropolis theme


\title{Linear Regression}
\date{\today}
\author{Nipun Batra and the teaching staff}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle
  

\section{Setup}
  
% \section{Linear Regression}

\begin{frame}{Linear Regression}
\begin{itemize}
	
	
	\item<+-> O/P is continuous in nature.
	\item<+-> Examples of linear systems:
	\begin{itemize}
		\item<+-> $F=ma$
		\item<+-> $v=u+at$
	\end{itemize}
	
\end{itemize}
\end{frame}

\begin{frame}{Task at hand}
\begin{itemize}

\item TASK: Predict Weight = f(height)
\end{itemize}
\begin{center}
    

\begin{tabular}{ |c|c|c|c| } 
\hline
 Height & Weight \\
\hline
3 & 29 \\ 
4 & 35 \\ 
5 & 39\\
2 & 20\\
6 & 41\\
\hline
\hline
7 & ?\\
8 & ?\\
1 &? \\
\hline
\end{tabular}

\end{center}
The first part of the dataset are the training points. The latter ones are testing points.	

\end{frame}


\begin{frame}{Scatter Plot}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$height~(ft)$,
ylabel=$weight~(kg)$,
xmin=0,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside
]

\addplot[only marks, mark=*]
coordinates{ % plot 1 data set
	(6, 80)
	(5.5, 80)
	(6, 70)
	(5, 40)
	(5, 60)
	(4, 30)
	(4, 45)
	(4, 20)
	(6, 120)
	(2, 15)
	(1, 5)
	(0.5, 4)
	(3, 20)
	(6, 62)
	(5, 57)
	

	% more points...
}; 
\node[label={180:{Outlier}},circle,fill,inner sep=2pt] at (axis cs:6,120) {};
\node[label={-90:{Outlier}},circle,fill,inner sep=2pt] at (axis cs:4,20) {};

% plot 1 legend entry


\end{axis}

\end{tikzpicture}


\end{frame}


\begin{frame}{Matrix representation of the expression}


\begin{itemize}
\item $weight_{1}$ $\approx$
$\theta_{0}$+$\theta_{1}$*$height_{1}$

\item $weight_{2}$ $\approx$
$\theta_{0}$+$\theta_{1}$*$height_{2}$


\item $weight_{N}$ $\approx$
$\theta_{0}$+$\theta_{1}$*$height_{N}$

\end{itemize}
\begin{center}
\pause	\begin{tcolorbox}
		$weight_{i}$ $\approx$
		$\theta_{0}$+$\theta_{1}$*$height_{i}$
	\end{tcolorbox}
\end{center}
\end{frame}



\begin{frame}{Matrix representation of the expression}



\[\begin{bmatrix}
    weight_{1}   \\
    weight_{2}   \\
    \dots \\
    weight_{N}
\end{bmatrix}
= \begin{bmatrix}
    1& height_{1}   \\
    1& height_{2}   \\
    \dots & \dots  \\
    1& height_{N}   \\
\end{bmatrix}
\begin{bmatrix}
    \theta_{0} \\
    \theta_{1}
\end{bmatrix}\] \\

\pause \[W_{N \times 1}=X_{N \times 2}\theta_{2 \times 1} \]


%W(N \times 1); X(N\times 2); \theta(2 \times 1);


\pause \begin{itemize}
    \item<+-> $\theta_{0}$ - Bias Term/Intercept Term
    \item<+-> $\theta_{1}$ - Slope
\end{itemize}
\end{frame}



\begin{frame}{Extension to multiple dimensions}

In the previous example y = f(x), where x is one-dimensional.\\
\pause Examples in multiple dimensions.\\
\pause One example is to predict the water demand of the IITGN campus

\small{
\begin{center}
  \pause  \begin{tcolorbox}
        Demand = f(\# occupants, Temperature)
    \end{tcolorbox}
\end{center}

\begin{center}
    \pause \begin{tcolorbox}
        Demand = Base Demand + $K_{1}$ * \# occupants + $K_{2}$ * Temperature
    \end{tcolorbox}
\end{center}
}

\end{frame}

\begin{frame}{Intuition}
    We hope to: 
    \begin{itemize}
        \item Learn $f$: $Demand$ = $f(\# occupants, Temperature)$
        \item From training dataset
        \item To predict the condition for the testing set
    \end{itemize}
\end{frame}


\begin{frame}{Linear Relationship}
    We have 
    \begin{itemize}[<+->]
    	\item $x_i = \begin{bmatrix}
    	Temperature_i\\\#Occupants_i
    	\end{bmatrix}$
        \item Estimated demand for $i^{th}$ sample is  $\hat{demand_{i}} = \theta_{0} + \theta_{1} Temperature_i + \theta_{2} Occupants_i$
        \item $\hat{demand_{i}} = x_{i}'^{T} \theta$
        \item where $\theta = \begin{bmatrix}\theta_0\\\theta_1\\ \theta_2
        \end{bmatrix}$
        \item and $x_{i}' = \begin{bmatrix}
        1\\Temperature_i\\\#Occupants_i
        \end{bmatrix} = \begin{bmatrix}
        1 \\ x_i
        \end{bmatrix}$
        \item Notice the transpose in the equation! This is because $x_{i}$ is a column vector
    \end{itemize}

\end{frame}





\begin{frame}{We can expect the following}
    \begin{itemize}
        \item<+-> Demand increases, if \# occupants increases, then $\theta_{2}$ is likely to be positive
        
        \item<+-> Demand increases, if temperature increases, then $\theta_{1}$ is likely to be positive
        
        \item<+-> Base demand is independent of the temperature and the \# occupants, but, likely positive, thus $\theta_0$ is likely positive.
        
    \end{itemize}
\end{frame}


\section{Normal Equation}
\begin{frame}{Generalized Linear Regression Format}
\begin{itemize}[<+->]
	\item Assuming $N$ samples for training
	\item \# Features = $M$
\end{itemize}


   \pause \[\begin{bmatrix}
        \hat{y_{1}}\\
        \hat{y_{2}} \\
        \vdots \\
        \hat{y_{N}}
    \end{bmatrix}_{N \times 1}
    =     \begin{bmatrix}
        1 & x_{1,1} & x_{1,2} & \dots & x_{1,M}\\
        1 & x_{2,1} & x_{2,2} & \dots & x_{2,M}\\
        \vdots & \vdots & \vdots & \dots & \vdots\\
        1 & x_{N,1} & x_{N,2} & \dots & x_{N,M}\\
        % y_{2} \\
        % \vdots \\
        % y_{N}
    \end{bmatrix}_{N \times (M+1)}
    \begin{bmatrix}
        \theta_{0}\\
        \theta_{1}\\
        \vdots \\
        \theta_{M}\\
    \end{bmatrix}_{(M+1)\times 1}
   \]
   
%   \begin{center}
%       \textbf{Y}: N\times1; \textbf{X}: N\times(M+1); $\mathbf{\theta}$: (M+1)\times1 
%   \end{center}
   
  \pause  \begin{tcolorbox}
   \begin{center}
       
   

   $ \hat{Y} = X\theta$
   \end{center}
   \end{tcolorbox}

   
\end{frame}


\begin{frame}{Relationships between feature and target variables}
    \begin{itemize}
    	\item  There could be different $\theta_{0}, \theta_{1} \dots \theta_{M}$. Each of them can represents a relationship.
    	\item  Given multiples values of $\theta_{0}, \theta_{1} \dots \theta_{M}$ how to choose which is the  best?
    	\item Let us consider an example in 2d
    \end{itemize}
   
    
   
    
\end{frame}

\begin{frame}{Relationships between feature and target variables}
Out of the three fits, which one do we choose? \vspace{10pt}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
xmin=-1,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]

\addplot[mark=none,blue] {x};\addlegendentry{$\hat{y}=0+1x$}
\addplot[mark=none,yellow] {x+2};\addlegendentry{$\hat{y}=2+1x$}
\addplot[mark=none,green] {2*x-2};\addlegendentry{$\hat{y}=-2+2x$}
\addplot[only marks, mark=*]
coordinates{ % plot 1 data set
	(0,0)
	(1,1)
	(2,2)
	(3,3)
	% more points...
}; \addlegendentry{Train data}


% plot 1 legend entry


\end{axis}

\end{tikzpicture}

\end{frame}


\begin{frame}{Relationships between feature and target variables}
We have $\hat{y}=2+1x$ as one relationship.\vspace{10pt}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
xmin=-1,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]


\addplot[mark=none,yellow] {x+2};\addlegendentry{$\hat{y}=2+1x$}
\addplot[only marks, mark=*]
coordinates{ % plot 1 data set
	(0,0)
	(1,1)
	(2,2)
	(3,3)
	% more points...
}; 
\addplot[only marks, mark=*, yellow]
coordinates{ % plot 1 data set
	(0,2)
	(1,3)
	(2,4)
	(3,5)
	% more points...
}; ;\addlegendentry{Train Data}

% plot 1 legend entry


\end{axis}

\end{tikzpicture}

\end{frame}


\begin{frame}{Relationships between feature and target variables}
How far is our estimated $\hat{y}$ from ground truth $y$?\vspace{10pt}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
xmin=-1,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]


\addplot[mark=none,yellow] {x+2};\addlegendentry{$\hat{y}=2+1x$}
\addplot[only marks, mark=*]
coordinates{ % plot 1 data set
	(0,0)
	(1,1)
	(2,2)
	(3,3)
	% more points...
}; 
\addplot[only marks, mark=*, yellow]
coordinates{ % plot 1 data set
	(0,2)
	(1,3)
	(2,4)
	(3,5)
	% more points...
}; 
\draw (axis cs:0,0) -- node[left, ]{$\epsilon_{1}$}(axis cs:0,2);
\draw (axis cs:1,1) -- node[left,]{$\epsilon_{2}$}(axis cs:1,3);
\draw (axis cs:2,2) -- node[left,]{$\epsilon_{3}$}(axis cs:2,4);
\draw (axis cs:3,3) -- node[left,]{$\epsilon_{4}$}(axis cs:3,5);

% plot 1 legend entry


\end{axis}

\end{tikzpicture}

\end{frame}


\begin{frame}{Error terms}


	\begin{itemize}[<+->]
		\item $y_{i}$ = $\hat{y_{i}}$ + $\epsilon_{i}$ where $\epsilon_{i} \sim\mathcal{N}(0, \sigma^2)$
		\item $y_{i}$ denotes the ground truth for $i^{th}$ sample
		\item $\hat{y_{i}}$ denotes the prediction for $i^{th}$ sample, where $\hat{y_{i}}$ = $x_{i}'^{T} \theta$
		\item $\epsilon_{i}$ denotes the error/residual for $i^{th}$ sample
		\item $\theta_{0}, \theta_{1}$: The parameters of the linear regression
		\item   $  \epsilon_{i} = y_{i} - \hat{y}_{i}$
		\item     $\epsilon_{i} = y_{i} - (\theta_{0} + x_{i}\times\theta_{1})$

\end{itemize}





\end{frame}



\begin{frame}{Good fit}

\begin{itemize}
    \item<+-> $|\epsilon_{1}|$, $|\epsilon_{2}|$, $|\epsilon_{3}|$, ... should be small.
    \item<+-> 
${\text{minimize }} \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2$ - $L_{2}$ Norm
    \item<+-> 
${\text{minimize }} |\epsilon_{1}| + |\epsilon_{2}| + \dots + |\epsilon_{n}|$ - $L_{1}$ Norm
\end{itemize}
\end{frame}





\begin{frame}{Normal Equation}
    
    
   \pause  \begin{tcolorbox}
       $ Y = X\theta + \epsilon$
    \end{tcolorbox}
    
    \pause To Learn: $\theta$ \\
    \pause Objective: ${\text{minimize }} \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2$  
\end{frame}

\begin{frame}{Normal Equation}
    
\begin{equation*}
 \epsilon = 
\begin{bmatrix}
    \epsilon_{1} \\
    \epsilon_{1} \\
    \vdots \\
    \epsilon_{N} \\
\end{bmatrix}
\end{equation*}
\\
\begin{center}
 \pause Objective:   Minimize $\epsilon^{T}\epsilon$    
\end{center}
\end{frame}

\begin{frame}{Derivation of Normal Equation}
\begin{align*}
\label{eqn*:eqlabel}
\begin{split}
   \epsilon &= y - X\theta \\ 
\epsilon^{T} &= (y-X\theta)^{T} = y^{T} - \theta^{T}X^{T}\\
\epsilon^{T}\epsilon &= (y^{T} - \theta^{T}X^{T})(y - X\theta)\\
&=y^{T}y - \theta^{T}X^{T}y - y^{T}X\theta+\theta^{T}X^{T}X\theta\\
&=y^{T}y - 2y^{T}X\theta+\theta^{T}X^{T}X\theta
\end{split}
\end{align*}

This is what we wish to minimize
\end{frame}

\begin{frame}{Minimizing the objective function}
    
    
    \begin{equation}
        \frac{\partial \epsilon^{T} \epsilon}{\partial \theta} = 0    
    \end{equation}
    
    
    
    \begin{itemize}
        \item $
    \cfrac{\partial }{\partial \theta} y^{T}y= 0
    $
    \item $
    \cfrac{\partial }{\partial \theta}(-2y^{T}X\theta ) = (-2y^{T}X)^{T} = -2X^{T}y
    $
    \item $ \cfrac{\partial}{\partial\theta} (\theta^{T}X^{T}X\theta) = 2X^{T}X\theta$
    \end{itemize}
    
    Substitute the values in the top equation
    
\end{frame}

\begin{frame}{Normal Equation derivation}
$$
    0 = -2X^{T}y + 2X^{T}X\theta
$$

$$
    X^{T}y  = X^{T}X\theta
$$

\begin{tcolorbox}
\begin{center}
    

        $\hat{\theta}_{OLS} = (X^{T}X)^{-1}X^{T}y$
\end{center}
\end{tcolorbox}

\end{frame}

\begin{frame}{Worked out example}
    \begin{center}
 \begin{tabular}{||c c||} 
 \hline
 x  & y \\ [0.5ex] 
 \hline\hline
 0 & 0 \\
 1 & 1 \\
 2 & 2 \\
 3 & 3 \\
 \hline
\end{tabular}
\end{center}

Given the data above, find $\theta_{0}$ and $\theta_{1}$.

\end{frame}


\begin{frame}{Scatter Plot}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
	xlabel=$x$,
	ylabel=$y$,
	axis x line*=bottom,
	axis y line*=left,
	xtick align=outside,
	ytick align=outside
	]
	\addplot[only marks, mark=*]
	coordinates{ % plot 1 data set
		(0,0)
		(1,1)
		(2,2)
		(3,3)
		% more points...
	}; 
	
	% plot 1 legend entry
	\addlegendimage{/pgfplots/refstyle=plot_one}

\end{axis}
\end{tikzpicture}
    
\end{frame}


\begin{frame}{Worked out example}
\begin{align}
    \begin{split}
        X &= \begin{bmatrix}
            1 & 0\\
            1 & 1\\
            1 & 2\\
            1 & 3
        \end{bmatrix}\\
        X^{T} &= \begin{bmatrix}
            1&1&1&1\\
            0&1&2&3
        \end{bmatrix}\\
        X^{T}X &= \begin{bmatrix}
            4 &6\\6&14
        \end{bmatrix}
    \end{split}
\end{align}
Given the data above, find $\theta_{0}$ and $\theta_{1}$.
\end{frame}


\begin{frame}{Worked out example}
    \begin{align}
        \begin{split}
            (X^{T}X)^{-1} &= \frac{1}{20} \begin{bmatrix}
                14 & -6\\
                -6& 4
            \end{bmatrix}\\
            X^{T}y &= \begin{bmatrix}
            1&1&1&1\\
            0&1&2&3
            \end{bmatrix}\begin{bmatrix}
            0\\1\\2\\3
            \end{bmatrix}=\begin{bmatrix}
                6\\
                14
            \end{bmatrix}
        \end{split}
    \end{align}
\end{frame}


\begin{frame}{Worked out example}
    \begin{align}
        \begin{split}
            \theta &= (X^{T}X)^{-1}(X^{T}y)\\
           \begin{bmatrix}
        \theta_{0}\\
        \theta_{1}
    \end{bmatrix} &= \frac{1}{20} \begin{bmatrix}
    14 & -6\\
    -6& 4
    \end{bmatrix}\begin{bmatrix}
    6\\
    14
    \end{bmatrix} =
    \begin{bmatrix}
        0\\
        1
    \end{bmatrix} 
        \end{split}
    \end{align}
\end{frame}

\begin{frame}{Scatter Plot}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
xmin=0,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]
\addplot[mark=none, gray] {x};\addlegendentry{Fit ($\hat{y}=x)$}
\addplot[only marks, mark=*]
coordinates{ % plot 1 data set
	(0,0)
	(1,1)
	(2,2)
	(3,3)
	% more points...
}; 


% plot 1 legend entry


\end{axis}

\end{tikzpicture}


\end{frame}



\begin{frame}{Effect of outlier}

    \begin{center}
 \begin{tabular}{||c c||} 
 \hline
 x  & y \\ [0.5ex] 
 \hline\hline
 1 & 1 \\
 2 & 2 \\
 3 & 3 \\
 4 & 0 \\
 \hline
\end{tabular}
\end{center}

Compute the $\theta_{0}$ and $\theta_{1}$.
\end{frame}

\begin{frame}{Scatter Plot}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]
\addplot[only marks, mark=*]
coordinates{ % plot 1 data set
	(1,1)
	(2,2)
	(3,3)
	(4,0)
	% more points...
}; 
\node[label={180:{Outlier}},circle,fill,inner sep=2pt] at (axis cs:4,0) {};

% plot 1 legend entry
\addlegendimage{/pgfplots/refstyle=plot_one}

\end{axis}
\end{tikzpicture}

\end{frame}

\begin{frame}{Worked out example}
\begin{align}
\begin{split}
X &= \begin{bmatrix}
1 & 1\\
1 & 2\\
1 & 3 \\
1 & 4
\end{bmatrix}\\
X^{T} &= \begin{bmatrix}
1&1&1&1\\
1&2&3&4
\end{bmatrix}\\
X^{T}X &= \begin{bmatrix}
4 &10\\10&30
\end{bmatrix}
\end{split}
\end{align}
Given the data above, find $\theta_{0}$ and $\theta_{1}$.
\end{frame}


\begin{frame}{Worked out example}
\begin{align}
\begin{split}
(X^{T}X)^{-1} &= \frac{1}{20} \begin{bmatrix}
30 & -10\\
-10& 4
\end{bmatrix}\\
X^{T}y &= \begin{bmatrix}
6\\
14
\end{bmatrix}
\end{split}
\end{align}
\end{frame}


\begin{frame}{Worked out example}
\begin{align}
\begin{split}
\theta &= (X^{T}X)^{-1}(X^{T}y)\\
\begin{bmatrix}
\theta_{0}\\
\theta_{1}
\end{bmatrix} &= 
\begin{bmatrix}
2\\
(-1/5)
\end{bmatrix} 
\end{split}
\end{align}
\end{frame}

\begin{frame}{Scatter Plot}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
xmin=0.5,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]
\addplot[mark=none, gray] {2-x/5};\addlegendentry{Fit ($\hat{y}=2-x/5$)}
\addplot[only marks, mark=*]
coordinates{ % plot 1 data set
	(1,1)
	(2,2)
	(3,3)
	(4,0)
	% more points...
}; 
\node[label={180:{Outlier}},circle,fill,inner sep=2pt] at (axis cs:4,0) {};

% plot 1 legend entry


\end{axis}

\end{tikzpicture}


\end{frame}



\section{Basis Expansion}

\begin{frame}{Variable Transformation}
    Transform the data, by including the higher power terms in the feature space. 
    
       
    \begin{center}
 \begin{tabular}{||c c||} 
 \hline
 t  & s \\ [0.5ex] 
 \hline\hline
 0 & 0 \\
 1 & 6 \\
 3 & 24 \\
 4 & 36 \\
 \hline
\end{tabular}
\end{center}

The above table represents the data before transformation
\end{frame}


\begin{frame}{Variable Transformation}
Add the higher degree features to the previous table
    
       
    \begin{center}
 \begin{tabular}{||c c c||} 
 \hline
 t  & $t^{2}$ & s \\ [0.5ex] 
 \hline\hline
 0 & 0&0 \\
 1 & 1&6 \\
 3 & 9&24 \\
 4 & 16&36 \\
 \hline
\end{tabular}
\end{center}

\pause The above table represents the data after transformation \\
\pause Now, we can write $\hat{s}=f(t, t^2)$ \\
\pause Other transformations: $\log(x), x_1\times x_2$
\end{frame}

\begin{frame}{A big caveat: Linear in what?!\footnotemark}
\begin{enumerate}[<+->]
	\item $\hat{s}=\theta_0 + \theta_1*t$
	 is linear
	 \item Is $\hat{s}=\theta_0 + \theta_1*t + \theta_2*t^2$
	 linear?
	 \item Is $\hat{s}=\theta_0 + \theta_1*t + \theta_2*t^2 + \theta_3*\cos(t^3)$
	 linear?
	\item Is $\hat{s}=\theta_0 + \theta_1*t + {\rm e}^{\theta_2}*t$
	linear?
	\item All except \#4 are linear models! 
	\item Linear refers to the relationship between the parameters that you are estimating ($\theta$) and the outcome 
\end{enumerate}
\footnotetext[1]{\url{https://stats.stackexchange.com/questions/8689/what-does-linear-stand-for-in-linear-regression}}
\end{frame}

\begin{frame}{Basis Functions}
    \begin{itemize}
        \item Linear regression only refers to linear in the parameters
        \item We can perform an arbitrary nonlinear transformation $\phi(x)$ of the inputs $x$ and then linearly combine the components of this transformation.
        \item $\phi: \mathbb{R}^D \rightarrow \mathbb{R}^K$ is called the basis function
    \end{itemize} 
    
\end{frame}

\begin{frame}{Basis Functions}
    Some examples of basis functions:
    \begin{itemize}
        \item Polynomial basis: $\phi(x) = \{1, x, x^2, x^3, \dots\}$
        \item Fourier basis: $\phi(x) = \{1, \sin(x), \cos(x), \sin(2x), \cos(2x), \dots\}$
        \item Gaussian basis: $\phi(x) = \{1, \exp(-\frac{(x-\mu_1)^2}{2\sigma^2}), \exp(-\frac{(x-\mu_2)^2}{2\sigma^2}), \dots\}$
        \item Sigmoid basis: $\phi(x) = \{1, \sigma(x-\mu_1), \sigma(x-\mu_2), \dots\}$ where $\sigma(x) = \frac{1}{1+e^{-x}}$
    \end{itemize}
    
    
\end{frame}



\end{document}