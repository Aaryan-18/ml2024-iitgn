\documentclass{beamer}
\usepackage{tcolorbox}
\usepackage{notation} % Custom package
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amssymb}
%\beamerdefaultoverlayspecification{<+->}
% \newcommand{\data}{\mathcal{D}}
% \newcommand\Item[1][]{%
% 	\ifx\relax#1\relax  \item \else \item[#1] \fi
% 	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}


\usetheme{metropolis}           % Use metropolis theme


\title{Logistic Regression}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
	\maketitle
	
	\begin{frame}{Classification Technique}
    
    Aim: Probability(Apple $|$ Radius) ? or
    \begin{tabular}{|c|} \hline
        More generally, P($1 | X$)  \\ \hline
      \end{tabular}
    
	\end{frame}

	\begin{frame}{Idea: Use Linear Regression}
	\begin{equation*}
		P(Fruit = Apple | Radius) = \theta_{0} + \theta_{1} \times Radius
	\end{equation*}
	Generally,
	\begin{equation*}
	    P(y = 1 | x) = X\theta
	\end{equation*}
	\end{frame}
	\begin{frame}{Idea: Use Linear Regression}
	Prediction:\\
	If $\theta_{0} + \theta_{1}\times Radius > 0.5 \rightarrow$ Apple \\
	\hspace{3.3cm} Else $\rightarrow$ Orange\\
	Problem:\\
	Range of $X\theta$ is $(-\infty, \infty)$\\
	But $P(y = 1 | \ldots) \in [0, 1]$
	\end{frame}
	\begin{frame}{Idea: Use Linear Regression}
	Question. Can we still use Linear Regression? \\
	Answer. Yes! Transform $\hat{y} \rightarrow [0, 1]$
	\end{frame}
	\begin{frame}{Idea: Use Linear Regression}
	Question. Can we still use Linear Regression? \\
	Answer. Yes! Transform $\hat{y} \rightarrow [0, 1]$
	\end{frame}
	\begin{frame}{Logistic / Sigmoid Function}
	    $\hat{y} \in (-\infty, \infty)$ \\
	    $\phi =$ Sigmoid / Logistic Function $(\sigma)$ \\
	    $\phi(\hat{y}) \in [0, 1]$
	    \begin{equation*}
	        \sigma(z) = \frac{1}{1 + e^{-z}}
	    \end{equation*}
	    \hspace{8cm} $z \rightarrow \infty$\\
	    \hspace{8cm} $\sigma(z) \rightarrow 1$\\
	    \hspace{8cm} $z \rightarrow -\infty$\\
	    \hspace{8cm} $\sigma(z) \rightarrow 0$\\
	    \hspace{8cm} $z = 0$\\
	    \hspace{8cm} $\sigma(z) = 0.5$
	\end{frame}
	\begin{frame}{Logistic / Sigmoid Function}
	Question. Could you use some other transformation $(\phi)$ of $\hat{y}$ s.t. \\
	\begin{equation*}
	    \phi(\hat{y}) \in [0, 1]
	\end{equation*}
	Yes! But Logistic Regression works.
	\end{frame}
	\begin{frame}{Logistic / Sigmoid Function}
	    \begin{equation*}
	        P(y = 1 | X) = \sigma(X\theta) = \frac{1}{1 + e^{-X\theta}}    
	    \end{equation*}
	    Q. Write $X\theta$ in a more convenient form (as $P(y = 1|X)$, $P(y = 0 | X)$)\\
	\end{frame}
	\begin{frame}{Logistic / Sigmoid Function}
	    \begin{equation*}
	        P(y = 1 | X) = \sigma(X\theta) = \frac{1}{1 + e^{-X\theta}}    
	    \end{equation*}
	    Q. Write $X\theta$ in a more convenient form (as $P(y = 1|X)$, $P(y = 0 | X)$)\\
	    \begin{equation*}
	        P(y = 0 | X) = 1 - P(y = 1 | X) = 1 - \frac{1}{1 + e^{-X\theta}} = \frac{e^{-X\theta}}{1 + e^{-X\theta}} 
	        \therefore \frac{P(y = 1|X)}{1 - P(y = 1|X)} = e^{X\theta}
	        \implies X\theta = \log\frac{P(y = 1|X)}{1 - P(y = 1 | X)}
	    \end{equation*}
	\end{frame}
	\begin{frame}{Odds (Used in betting)}
	    $$\frac{P(win)}{P(loss)}$$ \\
	    \hspace{3cm} Here,\\
	    $$Odds = \frac{P(y = 1)}{P(y = 0)}$$ \\
	    \centering
	    \begin{tabular}{|c|} \hline
        log-odds = $\log\frac{P(y = 1)}{P(y = 0)}$  \\ \hline
        \end{tabular}
	\end{frame}
	\begin{frame}{Logistic Regression}
	    Q. What is decision boundary for Logistic Regression?
	\end{frame}
	\begin{frame}{Logistic Regression}
	    Q. What is decision boundary for Logistic Regression? \\
	    \hspace{0.4cm} Decision Boundary: $P(y = 1|X) = P(y = 0 | X)$\\
	    \vspace{0.3cm}
	    \hspace{4cm} or $\frac{1}{1 + e^{-X\theta}} = \frac{e^{-X\theta}}{1 + e^{-X\theta}}$  \\
	    \vspace{0.3cm}
	    \hspace{4cm} or $e^{X\theta} = 1$\\
	    \vspace{0.3cm}
	    \hspace{4cm} or $X\theta = 0$
	\end{frame}
	\begin{frame}{Learning Parameters}
	    Could we use cost function as:
	    \begin{equation*}
	        J(\theta) = \sum (y_{i} - \hat{y}_{i})^{2}
	    \end{equation*}
	    \begin{equation*}
	        \hat{y}_{i} = \sigma(X\theta)
	    \end{equation*}
	    Answer: No (Non-Convex)  \hspace{1.2cm} (See Jupyter Notebook)
	\end{frame}
	\begin{frame}{Learning Parameters}
	    Likelihood = $P(D | \theta)$ \\
	    \vspace{0.2cm}
	    $P(y | X, \theta) = \prod_{i=1}^{n} P(y_{i} | x_{i}, \theta)$
	\end{frame}
	\begin{frame}{Learning Parameters}
	    Likelihood = $P(D | \theta)$ \\
	    \vspace{0.2cm}
	    $P(y | X, \theta) = \prod_{i=1}^{n} P(y_{i} | x_{i}, \theta)$ \\
	    \vspace{0.2cm}
	    \hspace{1.55cm} $ = \prod_{i=1}^{n} \Big\{\frac{1}{1 + e^{-x_{i}\theta}}\Big\}^{y_{i}}\Big\{1 - \frac{1}{1 + e^{-x_{i}\theta}}\Big\}^{1 - y_{i}}$ \\
	    \vspace{0.2cm}
	    [Above: Similar to $P(D|\theta)$ for Linear Regression; \\
	    \hspace{1.3cm} Difference Bernoulli instead of Gaussian]\\
	    \begin{equation*}
	        -\log P(y | X, \theta) = \text{Negative Log Likelihood}
	        \hspace{3cm} = \text{Cost function will be minimising} 
	        \hspace{3cm} = J(\theta)
	    \end{equation*}
	\end{frame}
	\begin{frame}{Learning Parameters}
	    \begin{equation*}
	        J(\theta) = -\log \bigg\{\prod_{i=1}^{n} \Big\{\frac{1}{1 + e^{-x_{i}\theta}}\Big\}^{y_{i}}\Big\{1 - \frac{1}{1 + e^{-x_{i}\theta}}\Big\}^{1 - y_{i}}\bigg\}
	        \hspace{0.5cm} J(\theta) = -\bigg\{\sum_{i=1}^{n} y_{i} \log(\sigma_{\theta}(x_{i})) + (1 - y_{i})\log(1 - \sigma_{\theta}(x_{i}))\bigg\} 
	    \end{equation*}
	    \begin{equation*}
	        \hspace{0.5cm} \frac{\partial J(\theta)}{\partial \theta_{j}} = -\frac{\partial }{\partial \theta_{j}} \bigg\{\sum_{i=1}^{n} y_{i} log(\sigma_{\theta}(x_{i})) + (1 - y_{i})log(1 - \sigma{\theta}(x_{i}))\bigg\}
	    \end{equation*}
	    \begin{equation*}
	        = -\sum_{i=1}^{n}\bigg[y_{i}\frac{\partial}{\partial \theta_{j}} \log(\sigma_{\theta}(x_{i})) + (1-y_{i})\frac{\partial}{\partial \theta_{j}} log(1 - \sigma_{\theta}(x_{i}))\bigg]
	    \end{equation*}
	\end{frame}
	\begin{frame}{Learning Parameters}
	    \begin{equation*}
	        = -\sum_{i=1}^{n}\bigg[y_{i}\frac{\partial}{\partial \theta_{j}} \log(\sigma_{\theta}(x_{i})) + (1-y_{i})\frac{\partial}{\partial \theta_{j}} log(1 - \sigma_{\theta}(x_{i}))\bigg]
	    \end{equation*}
	    \begin{equation}
	        = -\sum_{i=1}^{n}\bigg[\frac{y_{i}}{\sigma_{\theta}(x_{i})} \frac{\partial}{\partial \theta_{j}} \sigma_{\theta}(x_{i}) + \frac{1 - y_{i}}{1 - \sigma_{\theta}(x_{i})} \frac{\partial}{\partial \theta_{j}}(1 - \sigma_{\theta}(x_{i}))\bigg]
	    \end{equation}
	    Aside,
	    \begin{equation*}
	        \frac{\partial}{\partial z}\sigma(z) = \frac{\partial}{\partial z}\frac{1}{1 + e^{-z}} = -(1 + e^{-z})^{-z}\frac{\partial}{\partial z}(1 + e^{-z})
	    \end{equation*}
	    \begin{equation*}
	        = \frac{e^{-z}}{(1 + e^{-z})^{2}} = \bigg(\frac{1}{1 + e^{-z}}\bigg)\bigg(\frac{e^{-z}}{1 + e^{-z}}\bigg) = \sigma(z)\bigg\{\frac{1 + e^{-z}}{a + e^{-z}} - \frac{1}{1 + e^{-z}}\bigg\}
	    \end{equation*}
	    \begin{equation*}
	        \hspace{5.5cm}= \sigma(z)(1 - \sigma(z))
	    \end{equation*}
	\end{frame}
	\begin{frame}{Learning Parameters}
	    Resuming from (1)
	    \begin{equation*}
	        \frac{\partial J(\theta)}{\partial \theta_{j}} = -\sum_{i=1}^{n}\bigg[\frac{y_{i}}{\sigma_{\theta}(x_{i})} \frac{\partial}{\partial \theta_{j}} \sigma_{\theta}(x_{i}) + \frac{1 - y_{i}}{1 - \sigma_{\theta}(x_{i})} \frac{\partial}{\partial \theta_{j}}(1 - \sigma_{\theta}(x_{i}))\bigg]
	    \end{equation*}
	    \begin{equation*}
	        \hspace{-0.7cm}= -\sum_{i=1}^{n}\bigg[\frac{y_{i}\sigma_{\theta}(x_{i})}{\sigma_{\theta}(x_{i})}(1 - \sigma_{\theta}(x_{i}))\frac{\partial}{\partial \theta_{j}} (x_{i}\theta) + \frac{1 - y_{i}}{1 - \sigma_{\theta}(x_{i})}(1 - \sigma_{\theta}(x_{i})) \frac{\partial}{\partial \theta_{j}}(1 - \sigma_{\theta}(x_{i}))\bigg]
	    \end{equation*}
	    \begin{equation*}
	        = -\sum_{i=1}^{n}\bigg[y_{i}(1 - \sigma_{\theta}(x_{i}))x_{i}^{j} - (1 - y_{i})\sigma_{\theta}(x_{i})x_{i}^{j}\bigg]
	    \end{equation*}
	    \begin{equation*}
	        = -\sum_{i=1}^{n}\bigg[(y_{i} - y_{i}\sigma_{\theta}(x_{i}) - \sigma_{\theta}(x_{i}) + y_{i}\sigma_{\theta}(x_{i}))x_{i}^{j}\bigg]
	    \end{equation*}
	    \begin{equation*}
	        = -\sum_{i=1}^{n}\bigg[\sigma_{\theta}(x_{i}) - y_{i}\bigg]x_{i}^{j}
	    \end{equation*}
	\end{frame}
	\begin{frame}{Learning Parameters}
	\centering
	  \begin{tabular}{|c|} \hline
        $\frac{\partial J(\theta)}{\theta_{j}} = \sum_{i=1}^{N}\big[\sigma_{\theta}(x_{i}) - y_{i}\big]x_{i}^{j}$\\ \hline
      \end{tabular}\\
      Now, just use Gradient Descent!
	\end{frame}
	\begin{frame}{Regularized Logistic Regression}
	    Unregularised:
	    \begin{equation*}
	        J_{1}(\theta) = -\bigg\{\sum_{i=1}^{n}y_{i}\log(\sigma_{theta}(x_{i})) + (1 - y_{i})\log(1 - \sigma_{\theta}(x_{i}))\bigg\}
	    \end{equation*}
	    L2 Regularization:
	    \begin{equation*}
	        J(\theta) = J_{1}(\theta) + \lambda\theta^{T}\theta
	    \end{equation*}
	    L1 Regularization:
	    \begin{equation*}
	        J(\theta) = J_{1}(\theta) + \lambda|\theta|
	    \end{equation*}
	\end{frame}
	\begin{frame}{Multi-Class Prediction}
	    \begin{enumerate}
	        \item Use one-vs.-all on Binary Logistic Regression
	        \item Use one-vs.-one on Binary Logistic Regression
	        \item Extend Binary Logistic Regression to Multi-Class Logistic Regression
	    \end{enumerate}
	\end{frame}
	\begin{frame}{Softmax}
	    \begin{equation*}
	        Z \in \mathbb{R}^{d}
	    \end{equation*}
	    \begin{equation*}
	        f(z_{i}) = \frac{e^{z_{i}}}{\sum_{i=1}^{d}e^{z_{i}}}
	    \end{equation*}
	    \begin{equation*}
	        \therefore \sum f(z_{i}) = 1
	    \end{equation*}
	    
	    $f(z_{i})$ refers to probability of class i
	\end{frame}
	\begin{frame}{Softmax for Multi-Class Logistic Regression}
	    \begin{equation*}
	       k = 1, \ldots, k \text{classes}
	    \end{equation*}
	    \centering
	  \begin{tabular}{|c|} \hline
        $P(y = k|x, \theta) = \frac{e^{x\theta_{k}}}{\sum_{k=1}^{K}e^{x\theta_{k}}}$\\ \hline
      \end{tabular}\\
	\end{frame}
	\begin{frame}{Softmax for Multi-Class Logistic Regression}
	    For K = 2 classes,
	    \begin{equation*}
	        P(y = k|x, \theta) = \frac{e^{x\theta_{k}}}{\sum_{k=1}^{K}e^{x\theta_{k}}}
	    \end{equation*}
	    \begin{equation*}
	        P(y = 0|x, \theta) = \frac{e^{x\theta_{0}}}{e^{x\theta_{0}} + e^{x\theta_{1}}}
	    \end{equation*}
	    \begin{equation*}
	        P(y = 1|x, \theta) = \frac{e^{}x\theta_{1}}{e^{x\theta_{0}} + e^{x\theta_{1}}} = \frac{e^{x\theta_{1}}}{e^{x\theta_{1}}\{1 + e^{x(\theta_{0} - \theta_{1})}\}}
	    \end{equation*}
	    \begin{equation*}
	       = \frac{1}{1 + e^{-x\theta^{'}}}
	    \end{equation*}
	    \begin{equation*}
	        = \text{Sigmoid!}
	    \end{equation*}
	\end{frame}
%	\begin{frame}{Multi-Class Logistic Regression Cost}
%	    For 2 class we had:
%	    \begin{equation*}
%	        J(\theta) = -\bigg\{\sum_{i=1}^{n}y_{i}\log(\sigma_{\theta}(x_{i})) + (1 - y_{i})\log(1 - \sigma_{\theta}(x_{i}))\bigg\}
%	    \end{equation*}
%	    Extend to K-class:
%	    \begin{equation*}
%	        J(\theta) = \bigg\{\sum_{i=1}^{n}\sum_{k=1}^{K}I\{y_{i} = k\}\log\frac{e^{x_{i}\theta_{k}}}{\sum_{k=1}^{K}e^{x_{i}\theta_{k}}}\bigg\}
%	    \end{equation*}
%	    \begin{equation*}
%	        i \rightarrow \text{Sample #} \hspace{2cm} \text{I: Identity Function}
%	    \end{equation*}
%	    \begin{equation*}
%	        k \rightarrow Class \hspace{2cm} \text{I(true) = 1; I(false) = 0}
%	    \end{equation*}
%	\end{frame}
%	\begin{frame}{Multi-Class Logistic Regression Cost}
%	    \begin{equation*}
%	        \text{Now:}
%	    \end{equation*}
%	    \begin{equation*}
%	        \frac{\partial J(\theta)}{\partial \theta_{k}} = \sum_{i=1}^{n}\bigg[x_{i}\bigg\{I(y_{i} = k) - P(y_{i} = k | x_{i}, \theta)\bigg\}\bigg]
%	    \end{equation*}
%	\end{frame}
\end{document}