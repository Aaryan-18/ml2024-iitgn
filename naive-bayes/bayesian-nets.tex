\documentclass{beamer}
\usepackage{tcolorbox}

%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}


\usetheme{metropolis}           % Use metropolis theme


\title{Naive Bayes}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle
  
  
  
% \section{Linear Regression}

\begin{frame}{Bayesian Networks}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
        %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
        
        %Shape: Ellipse [id:dp7372359450600154] 
        \draw   (100,142.5) .. controls (100,130.07) and (122.27,120) .. (149.75,120) .. controls (177.23,120) and (199.5,130.07) .. (199.5,142.5) .. controls (199.5,154.93) and (177.23,165) .. (149.75,165) .. controls (122.27,165) and (100,154.93) .. (100,142.5) -- cycle ;
        %Shape: Ellipse [id:dp773307115438193] 
        \draw   (279,142) .. controls (279,130.95) and (294.67,122) .. (314,122) .. controls (333.33,122) and (349,130.95) .. (349,142) .. controls (349,153.05) and (333.33,162) .. (314,162) .. controls (294.67,162) and (279,153.05) .. (279,142) -- cycle ;
        %Straight Lines [id:da30385042687371255] 
        \draw    (279,142) -- (201.5,142.49) ;
        \draw [shift={(199.5,142.5)}, rotate = 359.64] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
        %Shape: Ellipse [id:dp08595332977182091] 
        \draw   (182,236.5) .. controls (182,225.18) and (206.29,216) .. (236.25,216) .. controls (266.21,216) and (290.5,225.18) .. (290.5,236.5) .. controls (290.5,247.82) and (266.21,257) .. (236.25,257) .. controls (206.29,257) and (182,247.82) .. (182,236.5) -- cycle ;
        %Straight Lines [id:da6920360079941172] 
        \draw    (299,162) -- (271.38,218.2) ;
        \draw [shift={(270.5,220)}, rotate = 296.17] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
        %Straight Lines [id:da8073837163630633] 
        \draw    (156,166) -- (206.15,220.53) ;
        \draw [shift={(207.5,222)}, rotate = 227.4] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
        
        % Text Node
        \draw (122,131) node [anchor=north west][inner sep=0.75pt]   [align=left] {Sprinkler};
        % Text Node
        \draw (297,131) node [anchor=north west][inner sep=0.75pt]   [align=left] {Rain};
        % Text Node
        \draw (197,226) node [anchor=north west][inner sep=0.75pt]   [align=left] {Grass Wet};
        
        
        \end{tikzpicture}
        
\begin{itemize}
	
	
	\item Nodes are random variables.
	\item Edges denote direct impact

	
\end{itemize}
\end{frame}

\begin{frame}{Example}
\begin{itemize}

\item Grass can be wet due to multiple reasons:
\begin{itemize}
    \item Rain
    \item Sprinkler
\end{itemize}
\item Also, if it rains, then sprinkler need not be used.
\end{itemize}

    


	

\end{frame}

\begin{frame}{Bayesian Nets}
    $P(X_{1},X_{2},X_{3},\dots,X_{N})$ denotes the joint probability, where $X_{i}$ are random variables.
    \begin{equation*}
        P(X_{1},X_{2},X_{3},\dots,X_{N}) = \Pi_{k=1}^{N} P(X_{k} | parents(X_{k}))
    \end{equation*}
    
    
    \begin{equation*}
        P(S,G,R) =  P(G|S,R)P(S|R)P(R)
    \end{equation*}
    
\end{frame}


\begin{frame}{Spam Email Classification}
    \begin{itemize}[<+->]
        \item $y \in\{0,1\}$ where 0 means not spam and 1 means spam
        \item From the emails construct a vector $X$.
        \item $\left[\begin{array}{c}a \\ a n \\ \vdots \\ \text { computer } \\ \vdots \\ \text { lotery } \\ \vdots \\ 200\end{array}\right]\}$ N words
        \item The vector has ones if the word is present, and zeros is the word is absent.
        \item Each email corresponds to vector/feature of length N containing zeros or ones.
    \end{itemize}
     
\end{frame}

\begin{frame}{Naive Bayes}
    \begin{itemize}
        \item Classification model
        \item Scalable
        \item Generative and Bayesian
        \item Usually a simple/good baselines
        \item  We want to model $P(class (y) \mid$ features (x)) 
        \item We can use Bayes rule as follows:
        $P(class(y) \mid \text { features (x) })=\frac{P(\text { features (x) } \mid class(y)) P(class(y))}{P(\text { features (x) })}$
    \end{itemize}
    
   

\end{frame}

\begin{frame}{Quick Question}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
        %uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
        
        %Shape: Ellipse [id:dp7372359450600154] 
        \draw   (100,142.5) .. controls (100,130.07) and (122.27,120) .. (149.75,120) .. controls (177.23,120) and (199.5,130.07) .. (199.5,142.5) .. controls (199.5,154.93) and (177.23,165) .. (149.75,165) .. controls (122.27,165) and (100,154.93) .. (100,142.5) -- cycle ;
        %Shape: Ellipse [id:dp08595332977182091] 
        \draw   (60,244.5) .. controls (60,233.18) and (69.96,224) .. (82.25,224) .. controls (94.54,224) and (104.5,233.18) .. (104.5,244.5) .. controls (104.5,255.82) and (94.54,265) .. (82.25,265) .. controls (69.96,265) and (60,255.82) .. (60,244.5) -- cycle ;
        %Straight Lines [id:da8073837163630633] 
        \draw    (124.5,161) -- (92.43,222.23) ;
        \draw [shift={(91.5,224)}, rotate = 297.65] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
        %Shape: Ellipse [id:dp023537100462790228] 
        \draw   (123,245.5) .. controls (123,234.18) and (132.96,225) .. (145.25,225) .. controls (157.54,225) and (167.5,234.18) .. (167.5,245.5) .. controls (167.5,256.82) and (157.54,266) .. (145.25,266) .. controls (132.96,266) and (123,256.82) .. (123,245.5) -- cycle ;
        %Shape: Ellipse [id:dp4079980413843134] 
        \draw   (182,246.5) .. controls (182,235.18) and (191.96,226) .. (204.25,226) .. controls (216.54,226) and (226.5,235.18) .. (226.5,246.5) .. controls (226.5,257.82) and (216.54,267) .. (204.25,267) .. controls (191.96,267) and (182,257.82) .. (182,246.5) -- cycle ;
        %Shape: Ellipse [id:dp6596498120094669] 
        \draw   (293,241.5) .. controls (293,230.18) and (302.96,221) .. (315.25,221) .. controls (327.54,221) and (337.5,230.18) .. (337.5,241.5) .. controls (337.5,252.82) and (327.54,262) .. (315.25,262) .. controls (302.96,262) and (293,252.82) .. (293,241.5) -- cycle ;
        %Straight Lines [id:da15595132436398473] 
        \draw    (144.5,167) -- (145.22,223) ;
        \draw [shift={(145.25,225)}, rotate = 269.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
        %Straight Lines [id:da8789748623945954] 
        \draw    (165.5,164) -- (203.19,224.3) ;
        \draw [shift={(204.25,226)}, rotate = 237.99] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
        %Straight Lines [id:da7453498794319566] 
        \draw    (187.5,160) -- (306.72,221.09) ;
        \draw [shift={(308.5,222)}, rotate = 207.13] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
        
        % Text Node
        \draw (122,131) node [anchor=north west][inner sep=0.75pt]   [align=left] {Class};
        % Text Node
        \draw (70,237) node [anchor=north west][inner sep=0.75pt]   [align=left] {x1};
        % Text Node
        \draw (133,238) node [anchor=north west][inner sep=0.75pt]   [align=left] {x2};
        % Text Node
        \draw (192,239) node [anchor=north west][inner sep=0.75pt]   [align=left] {x3};
        % Text Node
        \draw (303,234) node [anchor=north west][inner sep=0.75pt]   [align=left] {xn};
        
        
        \end{tikzpicture}
    
    
    \pause \begin{equation*}
        P(x_{1},x_{2},x_{3},\dots,x_{N} \vert y) = P(x_{1}|y) P(x_{2}|y) \dots P(x_{N}|y)
    \end{equation*}
    \pause Why is Naive Bayes model called Naive? \\

    \pause Naive assumption $x_{i}$ and $x_{i+1}$ are independent given y
    \[
    \text { i.e. }  p\left(x_{2} \mid x_{1}, y\right)=p\left(x_{2} \mid y\right)
    \]

\end{frame}


\begin{frame}{Frame Title}
    It assumes that the features are independent during modelling, which is generally not the case.
\end{frame}

\begin{frame}{What do we need to predict?}
    \begin{equation*}
        P(y|x_{1},x_{2},\dots,x_{N}) =  \frac{P(x_{1},x_{2},\dots,x_{N}|y)P(y)}{P(x_{1},x_{2},\dots,x_{N})}
    \end{equation*}
\end{frame}


\begin{frame}{Spam Mail Classification}
    Probability of $x_{i}$ being a spam email\\
    $$
    P(x_{i} = 1|y = 1) = \frac{\text{Count} (x_{i} = 1 \text{ and } y = 1)}{\text{Count }(y=1)}
    $$
    
    Similarly,
    
    $$
    P(x_{i} = 0|y = 1) = \frac{\text{Count} (x_{i} = 0 \text{ and } y = 1)}{\text{Count }(y=1)}
    $$
    
    
\end{frame}


\begin{frame}{Spam Mail classification}
    $$
        P(y = 1) = \frac{\text{Count }(y=1) }{\text{Count }(y=1) +\text{Count }(y=0) }
    $$
    
    Similarly,
    
    $$
        P(y = 0) = \frac{\text{Count }(y=0) }{\text{Count }(y=1) +\text{Count }(y=0) }
    $$
    
    
    
\end{frame}

\begin{frame}{Example}
    lets assume that dictionary is $[w_{1},w_{2},w_{3}]$
    
    
    \begin{tabular}{c|c|c|c|c}
    Index&$w_{1}$&$w_{2}$&$w_{3}$&y\\
    \hline
    \hline
         1 & 0 & 0 & 0 & 1  \\
         2 & 0 & 0 & 0 & 0  \\
         3 & 0 & 0 & 0 & 1  \\
         4 & 1 & 0 & 0 & 0  \\
         5 & 1 & 0 & 1 & 1  \\
         6 & 1 & 1 & 1 & 0  \\
         7 & 1 & 1 & 1 & 1  \\
         8 & 1 & 1 & 0 & 0  \\
         9 & 0 & 1 & 1 & 0  \\
         10 & 0 & 1 & 1 & 1  \\
    \end{tabular}
\end{frame}

\begin{frame}{Spam Classification}
    if y=0
    \begin{itemize}
        \item $P(w_{1}=0\vert y=0)$ = $\frac{3}{5}$ = 0.6 \\
        \item $P(w_{2}=0\vert y=0)$ = $\frac{2}{5}$ = 0.4 \\
        \item $P(w_{3}=0\vert y=0)$ = $\frac{3}{5}$ = 0.6 \\
    \end{itemize}
    P(y=0) = 0.5\\
    Similarly, if y=1
    \begin{itemize}
        \item $P(w_{1}=1\vert y=1)$ = $\frac{2}{5}$ = 0.4 \\
        \item $P(w_{2}=1\vert y=1)$ = $\frac{1}{5}$ = 0.2 \\
        \item $P(w_{3}=1\vert y=1)$ = $\frac{3}{5}$ = 0.6 \\
    \end{itemize}
    P(y=1) = 0.5
\end{frame}

\begin{frame}{Spam Classification}
    
    Given, test email {0,0,1}, classify using naive bayes
    \pause 
$$
        P(y=1\vert w_{1}=0,w_{2} = 0,w_{3}=1) 
$$
$$ = \frac{P(w_{1}=0|y=1) P(w_{2}=0|y=1) P(w_{3}=1|y=1) P(y=1)}{P(w_{1}=0, w_{2}=0, w_{3}=1)}
    $$
    $$ = \frac{0.6\times 0.8 \times 0.6 \times 0.5}{Z}
    $$
    
\pause Similarly, we can calculate $P(y=0\vert w_{1}=0,w_{2} = 0,w_{3}=1) = \frac{0.6*0.4*0.6*0.5}{Z} $

\pause $\frac{P\left(y=1 \mid w_{1}=0, w_{2}=0, w_{3}=1\right)}{P\left(y=0 \mid w_{1}=0, w_{2}=0, w_{3}=1\right)} = 2 > 1$. Thus, classified as a spam example.
    
\end{frame}

%\begin{frame}{Spam Classification}
%    
%    Calculate, $P(y=1 \vert w_{1}=0, w_{2}=0, w_{3}=1)$ and $P(y=1 \vert w_{1}=0, w_{2}=0, w_{3}=1)$
%    
%    if $P(y=1 \vert w_{1}=0, w_{2}=0, w_{3}=1)$ $\gt$ $P(y=0 \vert w_{1}=0, w_{2}=0, w_{3}=1)$, then it is a spam mail
%\end{frame}

\begin{frame}{Gaussian naive Bayes}
    We have classes $C_{1}, C_{2}, C_{3},\dots, C_{k}$\\
    There is a continuous attribute x\\
    For Class k 
    \begin{itemize}
        \item $\mu_{k} = Mean(x \vert y(x) = C_{k})$
        \item $\sigma_{k}^{2} = Variance(x \vert y(x)=C_{k})$
    \end{itemize}
    
\end{frame}

\begin{frame}{Guassian Naive Bayes}
    Now for x = some observation 'v'\\
    \begin{equation*}
        P(x=v\vert C_{k}) = \frac{1}{\sqrt{2\pi \sigma_{k}^{2}}} \exp^{\frac{-(v-\mu_{k})^{2}}{2\sigma_{k}^{2}}}
    \end{equation*}
\end{frame}

\begin{frame}{Wikipedia Example}

    \begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Height&Weight&Footsize&Gender\\
    \hline
    \hline
         6 & 180& 12& M \\
         5.92 & 190& 11& M \\
         5.58 & 170& 12& M \\
         5.92 & 165& 10& M \\
         5 & 100& 6& F \\
         5.5 & 100& 6& F \\
         5.42 & 130& 7& F \\
         5.75 & 150& 7& F \\
         \hline
    \end{tabular}
    
    \end{center}
    
\end{frame}

\begin{frame}{Example}
    \begin{center}
        
    
    \begin{tabular}{|c|c|c|}
    \hline
     &Male&Female\\
     \hline
     \hline
     Mean (height) & 5.855 & 5.41  \\
     Variance (height) & 3.5 $\times$ $10^{-2}$ & 9.7 $\times$ $10^{-2}$  \\
     Mean (weight) & 176.25 & 132.5  \\
     Variance (weight) & 1.22 $\times$ $10^{2}$ & 5.5 $\times$ $10^{2}$   \\
     Mean (Foot) & 11.25 & 7.5  \\
     Variance (Foot) & 9.7 $\times$ $10^{-1}$ & 1.67  \\
    \hline
    \hline
    \end{tabular}
    \end{center}
\end{frame}


\begin{frame}{Classify the Person}

    Given height = 6ft, weight = 13.5 lbs, feet = 8 inches, classify if it's male or female.
    
\end{frame}

\begin{frame}{Classify the Person}
    Given height = 6ft, weight = 13.5 lbs, feet = 8 inches, classify if it's male or female.\\
    It is female!
\end{frame}
% \begin{frame}{Matrix representation of the expression}
% \begin{center}
% \begin{tcolorbox}
% $weight_{i}$ $\approx$
% $\theta_{0}$+$\theta_{1}$*$height_{i}$
% \end{tcolorbox}
% \end{center}

% \begin{itemize}
% \item $weight_{1}$ $\approx$
% $\theta_{0}$+$\theta_{1}$*$height_{1}$

% \item $weight_{2}$ $\approx$
% $\theta_{0}$+$\theta_{1}$*$height_{2}$


% \item $weight_{N}$ $\approx$
% $\theta_{0}$+$\theta_{1}$*$height_{N}$

% \end{itemize}
% \end{frame}

% \begin{frame}{Matrix representation of the expression}



% \[\begin{bmatrix}
%     weight_{1}   \\
%     weight_{2}   \\
%     \dots \\
%     weight_{N}
% \end{bmatrix}
% = \begin{bmatrix}
%     1& height_{1}   \\
%     1& height_{2}   \\
%     \dots \\
%     1& height_{N}   \\
% \end{bmatrix}
% \begin{bmatrix}
%     \theta_{0} \\
%     \theta_{1}
% \end{bmatrix}\]


% \begin{itemize}
%     \item $\theta_{0}$ - Bias Term
%     \item $\theta_{1}$ - Intercept Term
% \end{itemize}
% \end{frame}


% \begin{frame}{Extension to multiple dimensions}

% In the previous example y = F(x), where x is one-dimensional.\\
% Examples in multiple dimensions.\\
% One example is to predict the water demand of the IITGN campus

% \small{
% \begin{center}
%     \begin{tcolorbox}
%         Demand = F(\# occupants, Temperature)
%     \end{tcolorbox}
% \end{center}

% \begin{center}
%     \begin{tcolorbox}
%         Demand = Base Demand + $K_{1}$ * \# occupants + $K_{2}$ * Temperature
%     \end{tcolorbox}
% \end{center}
% }

% \end{frame}

% \begin{frame}{We can expect the following}
%     \begin{itemize}
%         \item Demand increases, if \# occupants increases, then $K_{1}$ is likely to be positive
        
%         \item Demand increases, if temperature increases, then $K_{2}$ is likely to be positive
        
%         \item Base demand is independent of the temperature and the \# occupants.
        
%     \end{itemize}
% \end{frame}


% \begin{frame}{Generalized Linear Regression Format}


%     \[\begin{bmatrix}
%         y_{1}\\
%         y_{2} \\
%         \vdots \\
%         y_{N}
%     \end{bmatrix}
%     =     \begin{bmatrix}
%         1 & x_{11} & x_{12} & \dots & x_{1m}\\
%         1 & x_{21} & x_{22} & \dots & x_{2m}\\
%         \vdots & \vdots & \vdots & \dots & \vdots\\
%         1 & x_{N1} & x_{N2} & \dots & x_{Nm}\\
%         % y_{2} \\
%         % \vdots \\
%         % y_{N}
%     \end{bmatrix}
%     [\begin{bmatrix}
%         theta_{0}\\
%         theta_{1}\\
%         \vdots{5} \\
%         theta_{m}\\
%     \end{bmatrix}
%   \]
   
   
%   \begin{tcolorbox}
%   \begin{center}
       
   
       
   
%   $ Y \approx X\theta$
%   \end{center}
%   \end{tcolorbox}
   
%   \begin{itemize}
%       \item We have N knows, (or N equations)
%       \item M unknowns (or M parameters/ variables)
%   \end{itemize}
   
% \end{frame}


% \begin{frame}{Relationships between feature and target variables}
    
%     There are differenct $\theta_{0}$ $\&$ $\theta_{1}$. Each of them can represents a relationship.\\
%     \vspace{0.5em}
%     Given multiples values of $\theta_{0}$ $\&$ $\theta_{1}$ how to choose which is the  best?
    
% \end{frame}
% \begin{frame}{Error terms}
% \begin{itemize}
%     \item $\theta_{0}, \theta_{1}$: The parameters of the linear regressions
%     \item $\epsilon_{i}$: Variable residual per example 
% \end{itemize}


% \begin{equation*}
%     \epsilon_{i} = y_{i} - \hat{y}_{i}
% \end{equation*}
% \begin{equation*}
%     \epsilon_{i} = y_{i} - (\theta_{0} + x_{i}\times\theta_{1})
% \end{equation*}
% \end{frame}



% \begin{frame}{Good fit}

% \begin{itemize}
%     \item $|\epsilon_{1}|$, $|\epsilon_{2}|$, $|\epsilon_{3}|$, ... should be small.
%     \item 
% ${\text{minimize }} \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2$ - $L_{2}$ Norm
%     \item 
% ${\text{minimize }} |\epsilon_{1}| + |\epsilon_{1}| + \dots + |\epsilon_{1}|$ - $L_{1}$ Norm
% \end{itemize}
% \end{frame}



% \begin{frame}{Normal Equation}
    
    
%     \begin{tcolorbox}
%       $ Y = X\theta + \epsilon$
%     \end{tcolorbox}
    
%     To Learn: $\theta$ \\
%     Objective: ${\text{minimize }} \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2$  
% \end{frame}

% \begin{frame}{Normal Equation}
    
% \begin{equation*}
%  \epsilon = 
% \begin{bmatrix}
%     \epsilon_{1} \\
%     \epsilon_{1} \\
%     \vdots \\
%     \epsilon_{N} \\
% \end{bmatrix}
% \end{equation*}
% \\
% \begin{center}
%     Minimize $\epsilon^{T}\epsilon$    
% \end{center}
% \end{frame}

% \begin{frame}{Derivation of Normal Equation}
% \begin{align}
% \label{eqn*:eqlabel}
% \begin{split}
%   \epsilon &= y - X\theta \\
% \epsilon^{T} &= (y-X\theta)^{T} = y^{T} - \theta^{T}X^{T}\\
% \epsilon^{T}\epsilon &= (y^{T} - \theta^{T}X^{T})(y - X\theta)\\
% &=y^{T}y - \theta^{T}X^{T}y - y^{T}X\theta+\theta^{T}X^{T}X\theta\\
% &=y^{T}y - 2y^{T}X\theta+\theta^{T}X^{T}X\theta
% \end{split}
% \end{align}

% This is what we wish to minimize
% \end{frame}

% \begin{frame}{Minimizing the objective function}
    
    
%     \begin{equation}
%         \frac{\partial \epsilon^{T} \epsilon}{\partial \theta} = 0    
%     \end{equation}
    
    
    
%     \begin{itemize}
%         \item $
%     \frac{\partial }{\partial \theta} y^{T}y= 0
%     $
%     \item $
%     \frac{\partial }{\partial \theta}(-2y^{T}X\theta ) = (-2y^{T}X)^{T} = -2X^{T}y
%     $
%     \item $ \frac{\partial}{\partial\theta} (\theta^{T}X^{T}X\theta) = 2X^{T}X\theta$
%     \end{itemize}
    
%     Substitute the values in the top equation
    
% \end{frame}

% \begin{frame}{Normal Equation derivation}
% $$
%     0 = -2X^{T}y + 2X^{T}X\theta
% $$

% $$
%     X^{T}y  = X^{T}X\theta
% $$

% \begin{tcolorbox}
% \begin{center}
    

%         $\theta = (X^{T}X)^{-1}X^{T}$
% \end{center}
% \end{tcolorbox}

% \end{frame}

% \begin{frame}{Worked out example}
%     \begin{center}
%  \begin{tabular}{||c c||} 
%  \hline
%  x  & y \\ [0.5ex] 
%  \hline\hline
%  0 & 0 \\
%  1 & 1 \\
%  2 & 2 \\
%  3 & 3 \\
%  \hline
% \end{tabular}
% \end{center}

% Given the data above, find $\theta_{0}$ and $\theta_{1}$.

% \end{frame}



% \begin{frame}{Worked out example}
% \begin{align}
%     \begin{split}
%         X &= \begin{bmatrix}
%             1 & 0\\
%             1 & 1\\
%             1 & 2\\
%             1 & 3
%         \end{bmatrix}\\
%         X^{T} &= \begin{bmatrix}
%             1&1&1&1\\
%             1&2&3&4
%         \end{bmatrix}\\
%         X^{T}X &= \begin{bmatrix}
%             4 &6\\6&14
%         \end{bmatrix}
%     \end{split}
% \end{align}
% Given the data above, find $\theta_{0}$ and $\theta_{1}$.
% \end{frame}


% \begin{frame}{Worked out example}
%     \begin{align}
%         \begin{split}
%             (X^{T}X)^{-1} &= \frac{1}{20} \begin{bmatrix}
%                 14 & -6\\
%                 -6& 4
%             \end{bmatrix}\\
%             X^{T}y &= \begin{bmatrix}
%                 6\\
%                 14
%             \end{bmatrix}\\
%             \theta &= (X^{T}X)^{-1}(X^{T}y)
%         \end{split}
%     \end{align}
% \end{frame}


% % \begin{frame}{Frame Title}
    
% %     \begin{align}
        
% %         \begin{split}
% %             \theta &= \frac{1}{20}\begin{bmatrix}
% %                 14 & -6\\
% %                 -6 & 4
% %             \end{bmatrix}
% %             \begin{bmatrix}
% %                 6\\
% %                 14
% %             \end{bmatrix}\\
% %             &= \frac{1}{20} \begin{bmatrix}
% %                 0 \\
% %                 20
% %             \end{bmatrix}\\
            
% %             % \begin{bmatrix}
% %             %     \theta_{0}\\
% %             %     \theta_{1}
% %             % \end{bmatrix}
% %             %  = 
% %             %  \begin{bmatrix}
% %             %      0 \\
% %             %      1
% %             %  \end{bmatrix}
% %         \end{split}
% %     \end{align}
% %     $$
% % \end{frame}

% \begin{frame}{Quick Question}
    
%     $$
%     y = X\theta;
%     $$
    
%     Can we do
%     $$theta = X^{-1}y$$
    
%     Why? or why not?
    
% \end{frame}


% \begin{frame}{Quick Question}

%     X may not be a square matrix. So, $X^{-1}$ may not exist.
    
%     Rectangular matrices have a left inverse and a right inverse.
% \end{frame}



% \begin{frame}{Relation between \#instances and \# Variables}
    
%     If N$<$ M, then it is an under-determined system\\
%     Example: N=2; M=3\\
 
%   $$ \begin{bmatrix}
%         30 \\
%         40 
%     \end{bmatrix}
%      = \begin{bmatrix}
%          1 & 6& 30\\
%          1 & 5& 20
%      \end{bmatrix}    \begin{bmatrix}
%         \theta_{0}\\
%         \theta_{1}\\
%         \theta_{2}\\
%     \end{bmatrix} $$
 
    
    
%     \begin{align}
% \label{eqn*:eqlabel}
% \begin{split}
%   30 &= \theta_{0} + 6\theta_{1} + 30\theta_{2} \\
% 40 &= \theta_{0} + 5\theta_{1} + 20\theta_{2}\\
% \hline
% -10 &=  -1 \theta_{1} -10\theta_{2}
% \end{split}
% \end{align}

    
        
%     %     \begin{split}
%     %     %     30 &= \theta_{0} + 6\theta_{1} + 30\theta_{2} \\
%     %     %     40 &= \theta_{0} + 5\theta_{1} + 20\theta_{2} \\
        
%     %     % -10 &=  -1 \theta_{1} -10\theta_{2} \\
            
%     %     \end{split}
    
    
%     % \end{align}
%     % \\
    
%     The above equation can have infinitely many solutions. \\
%     Under-determined system: $\epsilon_{i} = 0$ for all $i$

% \end{frame}

% \begin{frame}{Relation between \#instances and \# Variables}
%     What if N $>$ M\\
%     Then it is an over determined system. So, the sum of squared residuals > 0.
% \end{frame}
% \begin{frame}{Special Cases}
% There can be situations where $X^{T}X$ is not computable. This condition arises when the $|X^{T}X|$ = 0.

% \begin{equation}
%     X = \begin{bmatrix}
%         1 & 1& 2\\
%         1 & 2& 4\\
%         1 & 3& 6\\
%     \end{bmatrix}
% \end{equation}

% The matrix X is not full rank. 
% \end{frame}


% \begin{frame}{Things to know about Linear Regression}
%     What to do When relationship is non-linear?
% \end{frame}


% \begin{frame}{Things to know about Linear Regression}
%     Transform the data, by including the higher power terms in the feature space. 
    
       
%     \begin{center}
%  \begin{tabular}{||c c||} 
%  \hline
%  t  & s \\ [0.5ex] 
%  \hline\hline
%  0 & 0 \\
%  1 & 6 \\
%  3 & 24 \\
%  4 & 36 \\
%  \hline
% \end{tabular}
% \end{center}

% The above table represents the data before transformation
% \end{frame}


% \begin{frame}{Things to know about Linear Regression}
% Add the higher degree features to the previous table
    
       
%     \begin{center}
%  \begin{tabular}{||c c c||} 
%  \hline
%  t  & $t^{2}$ & s \\ [0.5ex] 
%  \hline\hline
%  0 & 0&0 \\
%  1 & 1&6 \\
%  3 & 9&24 \\
%  4 & 16&36 \\
%  \hline
% \end{tabular}
% \end{center}

% The above table represents the data after transformation
% \end{frame}






% \begin{frame}{Multi - Collinearity}

% It arises when one or more predictor varibale/feature in X can be expressed as a linear combinations of others\\
% \vspace{5mm}



% How to tackle it?
% \begin{itemize}
%     \item Regularize
%     \item Drop variables
%     \item use different subsets of data
%     \item Avoid dummy variable trap
% \end{itemize}
% \end{frame}

% \begin{frame}{Modelling Interaction}
    
%     $$
%     y = \theta_{0} + x_{1}\theta_{1} +
%     x_{1}\theta_{1} +
%     \dots + x_{m}\theta_{m} 
%     $$
    
%     If x_{1} increases by one unit, then y increases by \theta_{1} units, irrespective of the interactive with x_{2},x_{3},\dots,d_{m}.
    
%     $$
%     y = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{1}x_{2} + \dots
%     $$

%     This way we can model the interactions. 
% \end{frame}
% \begin{frame}{Alternative parameter estimation}
%     $$
%     y_{i} = \theta_{0} + \theta_{1}x_{i}
%     $$
    
%     $$
%     \epsilon_{i} = y_{i} - \hat{y}_{i}
%     $$
    
%     $$
%     \sum \epsilon_{i}^{2} = \sum (y_{i} - \theta_{0} - \theta_{1}x_{i})^{2}
%     $$
    
%     Now, we compute the derivative of it with all the  $\theta_{j}$
    
    
% \end{frame}

% \begin{frame}{Alternative approach}

% \begin{align}
%     \begin{split}
%         \frac{\partial}{\partial \theta_{0}}\sum \epsilon_{i}^{2} &= 2\sum(y_{i} -  \theta_{0} - \theta_{1}x_{i})(-1) = 0 \\
%         0 &= \sum y_{i} -  N\theta_{0} - \sum \theta_{1}x_{i}\\
%         \theta_{0} &= \frac{\sum y_{i} - \theta_{1}\sum x_{i}}{N}
%     \end{split}
% \end{align}


% \begin{tcolorbox}
% \begin{center}
%     $ \theta_{0} = \bar{y} - \theta_{1} \bar{x}$
% \end{center}
% \end{tcolorbox}
        


% \end{frame}
% \begin{frame}{Alternative approach}

% $$
% \frac{\partial}{\partial \theta_{1}}\sum \epsilon_{i}^{2} = 0
% $$


% $$
% \implies 2 \sum_{i=1}^{N} (y_{i} - \theta_{0} \ \theta_{1}x_{i})(-x_{i}) = 0
% $$

% $$
% \implies \sum_{i=1}^{N} (x_{i}y_{i} - \theta_{0}x_{i} \ \theta_{1}x_{i}^{2}) = 0
% $$

% $$
% \implies \sum  \theta_{1}x_{i}^{2} = \sum x_{i}y_{i} - \sum \theta_{0}x_{i}
% $$

% $$
% \implies \sum  \theta_{1}x_{i}^{2} = \sum x_{i}y_{i} - \sum (\bar{y} - \theta_{1}\bar{x})
% $$


% \end{frame}

% \begin{frame}{alternative approach}


% $$
% \implies \sum  \theta_{1}x_{i}^{2} = \sum x_{i}y_{i} - \bar{y}\sum x_{i} + \theta_{1}\bar{x}\sum x_{i} 
% $$

% $$
% \implies \sum  
% x_{i}y_{i} - \sum x_{i}y = \theta_{1} (-\bar{x}\sum x_{i} + \sum x_{i}^{2})
% $$

% $$
% \theta_{1} = \frac{x_{i}y_{i} - \sum x_{i}y}{\sum x_{i}^{2} -\bar{x}\sum x_{i}}
% $$

    
% \end{frame}

% \begin{frame}{Alternative approach}
    
%     $$
%     \theta_{i} = \frac{ \frac{1}{N} \sum_{i=1}^{N}(X_{i} - \bar{X})(Y_{i} - \bar{y})}{\frac{1}{N}(X_{i} - \bar{x})^{2}}
%     $$
    
%     $$
%     \theta_{1} = \frac{Cov(x,y)}{variance(x)}
%     $$
    
% \end{frame}
% % \begin{frame}{Derivation of Normal Rule}
% %     \begin{equation*}
% %          = \\
        
% %     \end{equation*}
% % \end{frame}

% % ${\text{minimize }} \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2$


% % \begin{equation*}

% %     \displaystyle{\minimize \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2 }
% % \end{equation*}
    
% % $\displaystyle{\minimize \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2 }$

    




% % \begin{frame}{There can be multiple equa}
    
% % \end{frame}

% % =
% % \begin{bmatrix}
% %     x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
% %     x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
% %     \vdots & \vdots & \vdots & \ddots & \vdots \\
% %     x_{d1} & x_{d2} & x_{d3} & \dots  & x_{dn}
% % \end{bmatrix}





% % \begin{frame}{Another example on Bayes rule}
% % \end{frame}


% % \begin{frame}{Bayes Rule for Machine Learning}
% % \begin{itemize}


% %     \item $P(A|B)P(B) = P(B|A)P(A)$
% %     \item Let us consider for a machine learning problem:
% %     \begin{itemize}
% %     	\item A = Parameters ($\theta$)
% %     	\item B = Data ($\mathcal{D}$)
% %     \end{itemize}
% % \item We can rewrite the Bayes rule as:
% % \begin{itemize}
% % 	\item $P(\theta|\mathcal{D}) = \frac{P(\mathcal{D}|\theta)P(\theta)}{P(\mathcal{D})}$
% % 	\item Posterior: 
% % 	\item Prior:
% % 	\item Likelihood
% % 	\item 
% % \end{itemize}
% % \end{itemize}
% % \end{frame}

% % \begin{frame}{Likelihood}
% % \begin{itemize}
% % 	\item Likelihood is a function of $\theta$
% % 	\item Given a coin flip and 5 H and 1 T, what is more likely: P(H) = 0.5 or P(H) = 1
% % \end{itemize}
% % \end{frame}

% % \begin{frame}{Bayesian Learning is well suited for online settings}
% % content...
% % \end{frame}

% % \begin{frame}{Coin flipping}
% % \begin{itemize}
% % 	\item Assume we do a coin flip multiple times and we get the following observation: \{H, H, H, H, H, H, T, T, T, T\}: 6 Heads and 4 Tails
% % 	\item  What is $P(Head)$?
% % 	\item Is your answer: 6/10. Why?
% % \end{itemize}

% % \end{frame}

% % \begin{frame}{Coin flipping: Maximum Likelihood Estimate (MLE)}
% % \begin{itemize}
% % 	\item We have $\mathcal{D} = \{\data_1, \data_2, ...\data_{N}\}$ for $N$ observations where each $\mathcal{D}_i \in \{H, T\}$
% % 	\item Assume we have $n_H$ heads and $n_T$ tails, $n_H + n_T = N$
% % 	\item Let us have $P(H) = \theta, P(T) = 1-\theta$
% % 	\item We have Likelihood, $L(\theta) = P(\mathcal{D}|\theta) = P(\data_1, \data_2, ..., \data_N|\theta)$
% % 	\item Since observations are i.i.d., $L(\theta) = P(\data_1|\theta).P(\data_2|\theta) ... P(\data_N|\theta)$
% % \end{itemize}

% % \end{frame}


% % \begin{frame}{Coin flipping: Maximum Likelihood Estimate (MLE)}
% % \begin{itemize}
% % 	\item  
% % \begin{align*}  
% % P(\data_i|\theta) =  \left
% % \{\begin{array}{lr} \theta, & \text{for~} \data_i =H \\
% % 1-\theta, & \text{for~} \data_i = T
% % \end{array}\right.\
% % \end{align*}  
% % \item Thus, $L(\theta) = \theta^{n_H}\times (1-\theta)^{n_T}$
% % \item Log-Likelihood, $LL(\theta) = n_Hlog\theta + (n_T)(log(1-\theta))$
% % \item $\frac{\partial LL(\theta)}{\partial \theta} = \frac{n_H}{\theta} - \frac{n_T}{1-\theta}$
% % \item  For maxima, set derivative of LL to zero

% % \item 	$\frac{n_H}{\theta} - \frac{n_T}{1-\theta} = 0 $
% % \end{itemize}
% % \begin{tcolorbox}
% % 	\begin{align*}
% % 	 \theta = \frac{n_H}{n_H + n_T}
% % 	\end{align*}

% % \end{tcolorbox}
% % Question: Is this maxima or minima?

% % \end{frame}

% % \begin{frame}{}
% % \begin{align*}
% % \frac{\partial^2 LL(\theta)}{\partial \theta^2} = \frac{-n_H}{\theta^2} + \frac{-n_T}{(1-\theta)^2} \in \mathbb R_-
% % \end{align*}
% % Thus, the solution is a maxima
% % \end{frame}

% % \begin{frame}{Maximum A Posteriori estimate (MAP)}
% % \begin{itemize}


% % \item \textbf{MLE does not handle prior knowledge}: What if we know that our coin is biased towards head?
% % \item \textbf{MLE can overfit}: What is the probability of heads when we have observed 6 heads and 0 tails?
% % \end{itemize}

% % \end{frame}


% % \begin{frame}{Maximum A Posteriori estimate (MAP)}
% % Goal: Maximize the Posterior
% % \begin{tcolorbox}
% % 	\begin{align}
% % \hat{\theta}_{MAP} = \argmin_\theta P(\theta|\data)\\
% % \hat{\theta}_{MAP}= \argmin_\theta P(\data|\theta)P(\theta)
% % \end{align}
% % \end{tcolorbox}

% % \end{frame}

% % \begin{frame}{Prior distributions}
% % \end{frame}

% % \begin{frame}{Beta Distribution}
% % \end{frame}

% % \begin{frame}{Beta Distribution}
% % \end{frame}

% % \begin{frame}{Coin toss: MAP estimate}
% % \end{frame}

% % \begin{frame}{Linear Regression: MLE}
% % We previously saw 
% % \begin{tcolorbox}
% % \begin{align*}
% % \hat{\theta}_{least-square} = \argmin_\theta \epsilon^T\epsilon = \argmin_\theta (y-X\theta)^T(y-X\theta)
% % \end{align*}
% % \end{tcolorbox}


% % \end{frame}

\end{document}
