1
00:00:00,000 --> 00:00:05,000
 Please look at the code mentioned above and please sign up on the Google Cloud.

2
00:00:05,000 --> 00:00:08,000
 We've already started making some announcements.

3
00:00:08,000 --> 00:00:14,000
 You will likely end up missing the announcements and you'll have no one else to play with.

4
00:00:14,000 --> 00:00:20,000
 The second quick logistical announcement is that we'll have an extra lecture on Saturday,

5
00:00:20,000 --> 00:00:23,000
 11th Jan at 11am in 1.101.

6
00:00:23,000 --> 00:00:26,000
 So a lot of ones over there.

7
00:00:26,000 --> 00:00:31,000
 And I think one or two people still have conflict, but in the larger,

8
00:00:31,000 --> 00:00:35,000
 in the larger phone we'll have almost everyone available, so we'll have to stick with this.

9
00:00:38,000 --> 00:00:42,000
 FAQ and the projects which were earlier shared on Google Docs,

10
00:00:42,000 --> 00:00:47,000
 I'll give all of you comment access on it so that if you have any questions, queries,

11
00:00:47,000 --> 00:00:52,000
 things like what should be the, what are the maps, what are the main group size?

12
00:00:52,000 --> 00:00:55,000
 You can ask situations if they're already not there.

13
00:00:55,000 --> 00:00:59,000
 Also about projects, if you have any questions, like what is the expectation?

14
00:00:59,000 --> 00:01:04,000
 If it's something that's not mentioned clearly, you can please comment on the Google Docs

15
00:01:04,000 --> 00:01:06,000
 and we'll get back to you soon.

16
00:01:08,000 --> 00:01:11,000
 Also the video and the slides from the first lecture,

17
00:01:11,000 --> 00:01:15,000
 because I think we actually haven't put up on the code website,

18
00:01:15,000 --> 00:01:19,000
 which is at the maintenance, as mentioned in the slide above.

19
00:01:20,000 --> 00:01:23,000
 This course website has also been now put on Google Cloud.

20
00:01:23,000 --> 00:01:26,000
 So, you can also get to that.

21
00:01:28,000 --> 00:01:34,000
 Before we go forward, we should quickly revise what we started last time.

22
00:01:34,000 --> 00:01:38,000
 And someone tell you what is machine learning based on what we learned last time.

23
00:01:38,000 --> 00:01:43,000
 When you looked at a couple of definitions, one was from Arthur Sandler,

24
00:01:43,000 --> 00:01:49,000
 who by the way was the first person to point with the machine learning and he did that in 1959.

25
00:01:49,000 --> 00:01:51,000
 So, a long, long time back.

26
00:01:52,000 --> 00:01:54,000
 Anyone, what is machine learning?

27
00:02:05,000 --> 00:02:10,000
 Okay, the ability to learn without explicitly being to ask, correct?

28
00:02:10,000 --> 00:02:12,000
 Any other definitions you want to get?

29
00:02:14,000 --> 00:02:16,000
 There's a more technical definition also.

30
00:02:16,000 --> 00:02:18,000
 But we'll get to that later.

31
00:02:18,000 --> 00:02:23,000
 First, start with again this same study, same definition of the learning code.

32
00:02:23,000 --> 00:02:29,000
 So, the period of study and the computers, they are ready to learn without being explicitly programmed.

33
00:02:29,000 --> 00:02:33,000
 Okay, anyone tell you what does being explicitly programmed being there?

34
00:02:37,000 --> 00:02:40,000
 Does it mean that machine learning involves no programming?

35
00:02:40,000 --> 00:02:43,000
 So, all programming assignments are the ways of learning.

36
00:02:43,000 --> 00:02:47,000
 Program itself.

37
00:02:47,000 --> 00:02:48,000
 Program itself.

38
00:02:48,000 --> 00:02:53,000
 So, is it some, some article which ends in writing the code?

39
00:02:53,000 --> 00:02:55,000
 It's so writing the program.

40
00:02:57,000 --> 00:03:02,000
 Of course, there's one area where there is something on the computer architecture, so we're not going into that.

41
00:03:02,000 --> 00:03:04,000
 But who I'd say is the machine learning program.

42
00:03:05,000 --> 00:03:07,000
 So, do you actually say the program?

43
00:03:07,000 --> 00:03:08,000
 What is it?

44
00:03:08,000 --> 00:03:12,000
 What is the exact meaning of the learning in the computer program?

45
00:03:14,000 --> 00:03:16,000
 You don't have to put it in.

46
00:03:16,000 --> 00:03:17,000
 Okay.

47
00:03:17,000 --> 00:03:19,000
 Can you explain what does it mean?

48
00:03:19,000 --> 00:03:20,000
 Okay.

49
00:03:20,000 --> 00:03:21,000
 You don't want to say anything.

50
00:03:21,000 --> 00:03:23,000
 This case says that it's right in the computer.

51
00:03:23,000 --> 00:03:24,000
 So, okay.

52
00:03:24,000 --> 00:03:26,000
 There is more freedom in the computer.

53
00:03:26,000 --> 00:03:27,000
 Okay.

54
00:03:27,000 --> 00:03:29,000
 But so, he's on the right track.

55
00:03:29,000 --> 00:03:33,000
 Let's take an example to get this concept even better.

56
00:03:34,000 --> 00:03:36,000
 Can you see these model digits?

57
00:03:36,000 --> 00:03:39,000
 So, these are digits from 0 to 9.

58
00:03:39,000 --> 00:03:42,000
 And these are from the dataset for the MS.

59
00:03:42,000 --> 00:03:45,000
 One of the most popular machine learning datasets.

60
00:03:45,000 --> 00:03:54,000
 Now, the first task for all of us is we want to now write a program to recognize the digits.

61
00:03:54,000 --> 00:03:56,000
 And we'll start with code.

62
00:03:56,000 --> 00:04:01,000
 Can someone tell me how they'll recognize if a digit is 4 or not?

63
00:04:01,000 --> 00:04:03,000
 We're looking at these specific rules.

64
00:04:03,000 --> 00:04:06,000
 What does how the attributes are called?

65
00:04:06,000 --> 00:04:07,000
 Not in here.

66
00:04:08,000 --> 00:04:09,000
 Okay.

67
00:04:09,000 --> 00:04:11,000
 Can we start off as 4 can be

68
00:04:11,000 --> 00:04:18,000
 for the class of vertical line or horizontal line or vertical line?

69
00:04:18,000 --> 00:04:19,000
 All of them are jointed.

70
00:04:19,000 --> 00:04:25,000
 And then another vertical line going down from the first, from the last vertical line.

71
00:04:25,000 --> 00:04:32,000
 Everything of that is that all that is there to 4 or if there are any models.

72
00:04:32,000 --> 00:04:33,000
 Okay.

73
00:04:34,000 --> 00:04:39,000
 What about the fact that the height of each of the vertical lines

74
00:04:39,000 --> 00:04:41,000
 need to be very similar?

75
00:04:41,000 --> 00:04:44,000
 Can you write this kind of a rule?

76
00:04:44,000 --> 00:04:51,000
 Or do you see a force where one of the lines is very, very long compared to the other?

77
00:04:51,000 --> 00:04:52,000
 Generally not.

78
00:04:52,000 --> 00:04:56,000
 So, you have to report some of these constraints.

79
00:04:56,000 --> 00:04:59,000
 But are you done with it anything more?

80
00:05:00,000 --> 00:05:05,000
 Just look through the example for do you see any force which would violate the

81
00:05:05,000 --> 00:05:08,000
 things that you've seen it or specify the thought?

82
00:05:10,000 --> 00:05:12,000
 And I thought you looked at.

83
00:05:12,000 --> 00:05:13,000
 Okay.

84
00:05:13,000 --> 00:05:16,000
 The second last is one case.

85
00:05:16,000 --> 00:05:19,000
 What about this one?

86
00:05:21,000 --> 00:05:22,000
 Okay.

87
00:05:22,000 --> 00:05:26,000
 So, each of the vertical lines would be a little smaller.

88
00:05:27,000 --> 00:05:34,000
 In many cases, it would be a, and if I were to write, it would not understand it or the other.

89
00:05:34,000 --> 00:05:38,000
 So, because people write different things.

90
00:05:38,000 --> 00:05:40,000
 So, that's another rule that I like.

91
00:05:40,000 --> 00:05:42,000
 Now, what do you mean by slight slant?

92
00:05:42,000 --> 00:05:47,000
 Does it mean that it can have an inclination of some 10 degrees, 20 degrees, 30 degrees?

93
00:05:47,000 --> 00:05:49,000
 Very slight and none of them.

94
00:05:49,000 --> 00:05:53,000
 So, let's say you come up with some number of ways or there's experience based on some

95
00:05:53,000 --> 00:05:54,000
 rules of thumb.

96
00:05:54,000 --> 00:05:56,000
 But is that all?

97
00:05:56,000 --> 00:05:57,000
 No.

98
00:05:57,000 --> 00:05:59,000
 Some people write four, they just start.

99
00:05:59,000 --> 00:06:00,000
 Right?

100
00:06:00,000 --> 00:06:06,000
 If you look at this, one, two, three, four, the fifth, four, you have this particular line.

101
00:06:06,000 --> 00:06:07,000
 We have to join that.

102
00:06:07,000 --> 00:06:09,000
 This is for perfect union.

103
00:06:09,000 --> 00:06:10,000
 Right?

104
00:06:10,000 --> 00:06:13,000
 So, that is now another rule that I've written.

105
00:06:13,000 --> 00:06:17,000
 You have already come up with five, six of such rules.

106
00:06:17,000 --> 00:06:19,000
 But that's not all.

107
00:06:19,000 --> 00:06:20,000
 Anything else you can think of?

108
00:06:21,000 --> 00:06:26,000
 That's why we have not talked about the bits of the lines.

109
00:06:26,000 --> 00:06:28,000
 What can we think about that?

110
00:06:28,000 --> 00:06:31,000
 Can we cover some rules?

111
00:06:31,000 --> 00:06:39,000
 Let's say if I'm writing the different mark or if I'm using the pen in different fashion,

112
00:06:39,000 --> 00:06:43,000
 where some of my scores we fired will be ticker.

113
00:06:43,000 --> 00:06:44,000
 Right?

114
00:06:44,000 --> 00:06:47,000
 Maybe that's another particular rule.

115
00:06:48,000 --> 00:06:52,000
 They can in some cases where the bit of each of the stroke is a little different.

116
00:06:52,000 --> 00:07:02,000
 You have to again capture some of these characteristics while writing some rules or writing a program to recognize four.

117
00:07:02,000 --> 00:07:11,000
 So, what we have done thus far is explicitly programmed to classify order to recognize four.

118
00:07:11,000 --> 00:07:12,000
 Right?

119
00:07:12,000 --> 00:07:14,000
 So, now we understand what is explicit programming.

120
00:07:14,000 --> 00:07:17,000
 What we know is completely different from this.

121
00:07:17,000 --> 00:07:21,000
 So, what we have thus far done is we had data.

122
00:07:21,000 --> 00:07:26,000
 Data was these examples that we already had.

123
00:07:26,000 --> 00:07:29,000
 We came up with some rules.

124
00:07:29,000 --> 00:07:33,000
 So, these were rules which we as experts suggested.

125
00:07:33,000 --> 00:07:41,000
 And in traditional programming we have some kind of a Python or some programming line will be right, which will recognize all of these.

126
00:07:41,000 --> 00:07:48,000
 Of course, what is presented as a vertical line or called one-to-line are still higher constructs.

127
00:07:48,000 --> 00:07:54,000
 For example, vertical line a program computer does not know what is a vertical line.

128
00:07:54,000 --> 00:07:56,000
 So, you have to again boil it down to the computer.

129
00:07:56,000 --> 00:07:59,000
 What is it like to work it a line to a computer means?

130
00:07:59,000 --> 00:08:00,000
 Same.

131
00:08:00,000 --> 00:08:01,000
 Same.

132
00:08:01,000 --> 00:08:02,000
 Same.

133
00:08:02,000 --> 00:08:03,000
 Same.

134
00:08:03,000 --> 00:08:04,000
 Same.

135
00:08:04,000 --> 00:08:05,000
 Same.

136
00:08:05,000 --> 00:08:06,000
 Ex-axis.

137
00:08:06,000 --> 00:08:08,000
 So, think of it as pixels.

138
00:08:08,000 --> 00:08:11,000
 And all of the pixels will be of similar shape.

139
00:08:11,000 --> 00:08:12,000
 Right?

140
00:08:12,000 --> 00:08:14,000
 What do they do in all of those?

141
00:08:14,000 --> 00:08:21,000
 So, we have data rules and traditional programming that gives us the answers.

142
00:08:21,000 --> 00:08:23,000
 Now, let's go back to the definition.

143
00:08:23,000 --> 00:08:26,000
 The genome is a period of study as computers.

144
00:08:26,000 --> 00:08:31,000
 We have computers a vertical line without being explicit in the program.

145
00:08:31,000 --> 00:08:35,000
 And now to make this particular flow chart.

146
00:08:36,000 --> 00:08:41,000
 To tell me what, if you have to be able to do this, traditional programming is machine learning.

147
00:08:41,000 --> 00:08:43,000
 What do we need to do?

148
00:08:43,000 --> 00:08:45,000
 So, we are not explicitly programming.

149
00:08:45,000 --> 00:08:46,000
 What changes?

150
00:08:46,000 --> 00:08:50,000
 The rules are gone there.

151
00:08:50,000 --> 00:09:00,000
 So, what we are saying is, how do we learn with the only explicitly program?

152
00:09:01,000 --> 00:09:03,000
 So, we don't need the data.

153
00:09:03,000 --> 00:09:04,000
 We don't need the answers.

154
00:09:04,000 --> 00:09:11,000
 And what we end up with is automatically some function of rules that are being written.

155
00:09:11,000 --> 00:09:12,000
 Right?

156
00:09:12,000 --> 00:09:14,000
 So, this is how traditional programming is.

157
00:09:14,000 --> 00:09:17,000
 The bar line of the system program is very different from machine learning.

158
00:09:17,000 --> 00:09:19,000
 Which I have still done this.

159
00:09:19,000 --> 00:09:21,000
 Done this in a very abstract sense.

160
00:09:21,000 --> 00:09:24,000
 We are slowly going to go deeper from this.

161
00:09:24,000 --> 00:09:29,000
 We also looked at another definition which was of a more formal definition of machine learning.

162
00:09:30,000 --> 00:09:31,000
 Machine learning.

163
00:09:31,000 --> 00:09:32,000
 Which was given by Tom.

164
00:09:32,000 --> 00:09:33,000
 Which I like.

165
00:09:33,000 --> 00:09:37,000
 Tom is learning from experience E.

166
00:09:37,000 --> 00:09:43,000
 If you look at the keywords experience E, tasks E and a performance measure of P.

167
00:09:43,000 --> 00:09:50,000
 If the performance is improving in the particular task, as to measure for the performance measure.

168
00:09:50,000 --> 00:09:53,000
 And it is improving with the experience.

169
00:09:53,000 --> 00:09:54,000
 Right?

170
00:09:55,000 --> 00:09:58,000
 So, let's say in this particular case, the task is what?

171
00:09:58,000 --> 00:10:00,000
 For machine learning.

172
00:10:00,000 --> 00:10:02,000
 To classify digits.

173
00:10:02,000 --> 00:10:05,000
 And what is the input that is typically given?

174
00:10:05,000 --> 00:10:06,000
 You have some.

175
00:10:06,000 --> 00:10:09,000
 So, you said that you have some experience.

176
00:10:09,000 --> 00:10:16,000
 The experience can be you have some some images along with the true label.

177
00:10:16,000 --> 00:10:22,000
 So, you have elements like this 0 along with that label that is actually 0.

178
00:10:23,000 --> 00:10:26,000
 We will have various different examples.

179
00:10:26,000 --> 00:10:31,000
 And the performance measure P, what do you think is the performance measure P?

180
00:10:31,000 --> 00:10:33,000
 What do you want to optimize on?

181
00:10:33,000 --> 00:10:34,000
 Correct.

182
00:10:34,000 --> 00:10:37,000
 How correctly you are using classified digits.

183
00:10:37,000 --> 00:10:38,000
 Right?

184
00:10:38,000 --> 00:10:43,000
 So, putting in channel with a more scientific set in term for correctness.

185
00:10:43,000 --> 00:10:44,000
 Accuracy.

186
00:10:44,000 --> 00:10:45,000
 Or similar circumstances.

187
00:10:45,000 --> 00:10:48,000
 We will look at some of these metrics in two-way.

188
00:10:49,000 --> 00:10:54,000
 So, we will start a place that is up to our machine learning.

189
00:10:54,000 --> 00:10:57,000
 We will start, we are now starting a company.

190
00:10:57,000 --> 00:10:59,000
 All of us are starting a company.

191
00:10:59,000 --> 00:11:04,000
 And we want to be the basket of those words or some similar words we saw.

192
00:11:04,000 --> 00:11:05,000
 We want to scale.

193
00:11:05,000 --> 00:11:11,000
 So, if you remember one of the key words which we use a lot in the previous lecture was scaling.

194
00:11:11,000 --> 00:11:15,000
 The problem statement is that you want to predict the quality or condition for computer.

195
00:11:16,000 --> 00:11:17,000
 Given its visual features.

196
00:11:17,000 --> 00:11:25,000
 So, we said that our business use cases that growth rates are similar such grocery stores.

197
00:11:25,000 --> 00:11:29,000
 They have some women in the loop who looks at each of the tomatoes.

198
00:11:29,000 --> 00:11:32,000
 And that is in let's say a per minute per tomato.

199
00:11:32,000 --> 00:11:33,000
 Right?

200
00:11:33,000 --> 00:11:38,000
 So, there is a lot of human input involved which is making the whole process look.

201
00:11:38,000 --> 00:11:43,000
 We have had the speed by using computer vision visual features between the data.

202
00:11:44,000 --> 00:11:47,000
 What we will say is we have now an assembly line.

203
00:11:47,000 --> 00:11:52,000
 You put the tomatoes in the assembly line as the pass of snapshots are taken.

204
00:11:52,000 --> 00:11:56,000
 And you automatically classify whether it is a good tomato, a bad tomato.

205
00:11:56,000 --> 00:11:57,000
 And back to the data.

206
00:11:57,000 --> 00:11:58,000
 So, from the back.

207
00:11:58,000 --> 00:11:59,000
 Right?

208
00:11:59,000 --> 00:12:00,000
 So, you are saying that you are saying that you are saying that you are saying that you

209
00:12:00,000 --> 00:12:01,000
 are using a huge amount of human effort.

210
00:12:01,000 --> 00:12:03,000
 And what are you making your process greater?

211
00:12:03,000 --> 00:12:09,000
 So, saying that via this process you are able to make the living experience.

212
00:12:09,000 --> 00:12:10,000
 Right?

213
00:12:10,000 --> 00:12:15,000
 So, let's now do one of the machine learning aspects of this problem state.

214
00:12:15,000 --> 00:12:21,000
 So, if you remember we have thus far just spoken that there is an enerity of data.

215
00:12:21,000 --> 00:12:22,000
 Right?

216
00:12:22,000 --> 00:12:24,000
 There is an enerity of experience and data.

217
00:12:24,000 --> 00:12:28,000
 So, let's say that we have some pass data on the quality of tomatoes.

218
00:12:28,000 --> 00:12:31,000
 We have collected thousands of tomatoes.

219
00:12:31,000 --> 00:12:37,000
 And for each of the tomatoes some human expert quantified whether it is a good tomato or bad tomato.

220
00:12:38,000 --> 00:12:43,000
 For now it is just that you can put a bad tomato on the scale of algal and bad tomatoes.

221
00:12:43,000 --> 00:12:44,000
 Right?

222
00:12:44,000 --> 00:12:45,000
 There are two classes.

223
00:12:45,000 --> 00:12:46,000
 Two classes.

224
00:12:46,000 --> 00:12:51,000
 What visual features do you think would be useful to characterize the tomato?

225
00:12:51,000 --> 00:12:52,000
 Color?

226
00:12:52,000 --> 00:12:56,000
 Any, so what, what do you think would be a good tomato?

227
00:12:56,000 --> 00:12:59,000
 It is just more red or more.

228
00:12:59,000 --> 00:13:01,000
 There is something on the shade of red.

229
00:13:01,000 --> 00:13:04,000
 What about that tomato?

230
00:13:05,000 --> 00:13:11,000
 Something which is either showing some greenish shade it may be and right?

231
00:13:11,000 --> 00:13:12,000
 Yeah.

232
00:13:12,000 --> 00:13:16,000
 Black definitely or if it has some fungus or some other attributes.

233
00:13:16,000 --> 00:13:19,000
 What are the other attributes which make it a little bit of bad?

234
00:13:19,000 --> 00:13:21,000
 It is a shade.

235
00:13:21,000 --> 00:13:24,000
 Is there a chance to do it?

236
00:13:24,000 --> 00:13:28,000
 It is a perfect surface.

237
00:13:28,000 --> 00:13:30,000
 So, okay some, some.

238
00:13:31,000 --> 00:13:33,000
 So the size is another attribute.

239
00:13:33,000 --> 00:13:40,000
 Let's say that we assume for now that all tomatoes are roughly over and smaller tomatoes are bad.

240
00:13:40,000 --> 00:13:42,000
 Very big tomatoes are also bad.

241
00:13:42,000 --> 00:13:47,000
 They are very injected with some growth, growth chemicals.

242
00:13:47,000 --> 00:13:52,000
 Size, whether we have seen, what are the other things you typically have?

243
00:13:52,000 --> 00:13:53,000
 Fine.

244
00:13:53,000 --> 00:13:56,000
 Yeah, but visual features will not look at the people.

245
00:13:56,000 --> 00:13:59,000
 So you will have to get some proxy for that.

246
00:13:59,000 --> 00:14:03,000
 So that is another thing which I want all of us to take from the machine learning course.

247
00:14:03,000 --> 00:14:06,000
 What are some of the proxy features we have to take from?

248
00:14:06,000 --> 00:14:07,000
 Yes.

249
00:14:07,000 --> 00:14:08,000
 Texture.

250
00:14:08,000 --> 00:14:11,000
 So texture will tell us something about the field of the community.

251
00:14:11,000 --> 00:14:16,000
 Whether it is very rough, very smooth or very lighted, etc.

252
00:14:16,000 --> 00:14:19,000
 So we look at these three specific features.

253
00:14:19,000 --> 00:14:20,000
 Would there be others?

254
00:14:20,000 --> 00:14:22,000
 Yes, there might be thousands of other features.

255
00:14:22,000 --> 00:14:27,000
 But for the purposes of this example, it is only these three.

256
00:14:27,000 --> 00:14:32,000
 So we were talking about some vast data or some vast experience that we already have.

257
00:14:32,000 --> 00:14:36,000
 Maybe it exists in a form of a table like this.

258
00:14:36,000 --> 00:14:37,000
 You have some sample.

259
00:14:37,000 --> 00:14:41,000
 You have the color, size, texture, and you have the condition.

260
00:14:41,000 --> 00:14:49,000
 So this condition has been manually and a data written down by a human expert.

261
00:14:49,000 --> 00:14:56,000
 So in this particular table, you think sample would be a useful attribute of feature.

262
00:14:56,000 --> 00:15:01,000
 To predict the condition, how many things yes?

263
00:15:01,000 --> 00:15:06,000
 How many things no?

264
00:15:06,000 --> 00:15:07,000
 Okay.

265
00:15:07,000 --> 00:15:09,000
 Now tell me that it could be a useful feature.

266
00:15:09,000 --> 00:15:14,000
 Now, how would the sample number be useful feature?

267
00:15:14,000 --> 00:15:21,000
 Sorry.

268
00:15:22,000 --> 00:15:31,000
 No, so the equation is orange.

269
00:15:31,000 --> 00:15:35,000
 If the other is orange, decide the small, the texture is smooth.

270
00:15:35,000 --> 00:15:37,000
 I can roughly say it is good.

271
00:15:37,000 --> 00:15:42,000
 But does sample the variable to 1,10 because the variable is good or bad?

272
00:15:42,000 --> 00:15:43,000
 Yes.

273
00:15:43,000 --> 00:15:44,000
 One extra word.

274
00:15:44,000 --> 00:15:45,000
 We have to go back.

275
00:15:45,000 --> 00:15:49,000
 It sequentially adds to all the two elements of the first parameter.

276
00:15:49,000 --> 00:15:50,000
 Okay.

277
00:15:51,000 --> 00:15:54,000
 We have to try to find the data.

278
00:15:54,000 --> 00:15:55,000
 Okay.

279
00:15:55,000 --> 00:15:59,000
 But why do you think the sample number could be useful?

280
00:15:59,000 --> 00:16:04,000
 So some of you are getting to the point that let's say it will sequentially arrange, then we will get something.

281
00:16:04,000 --> 00:16:05,000
 But are we getting something?

282
00:16:05,000 --> 00:16:06,000
 Yes.

283
00:16:06,000 --> 00:16:11,000
 I thought this time the feature might change and can't qualify for something because I'm fine.

284
00:16:11,000 --> 00:16:12,000
 Okay.

285
00:16:12,000 --> 00:16:19,000
 So he is answering that maybe there is a notion of time associated with samples, very likely they could be.

286
00:16:19,000 --> 00:16:26,000
 And imagine a scenario where there is one specific time of the year, maybe when all of the events are bad.

287
00:16:26,000 --> 00:16:32,000
 Maybe the producer's band, maybe the leader, which was bringing the tomatoes was hard, had gone wrong.

288
00:16:32,000 --> 00:16:34,000
 Something would have happened.

289
00:16:34,000 --> 00:16:38,000
 So in some very limited cases, the sample number could be a useful feature.

290
00:16:38,000 --> 00:16:47,000
 But it's more likely that it's probably better to include more features, which are capturing the nice things we are looking at.

291
00:16:47,000 --> 00:16:49,000
 For example, what was the vehicle conditioner?

292
00:16:49,000 --> 00:16:53,000
 How many hours have passed in three tomatoes right now?

293
00:16:53,000 --> 00:16:55,000
 Things like that.

294
00:16:55,000 --> 00:16:58,000
 The sample number might be giving them this up.

295
00:16:58,000 --> 00:17:06,000
 But then this could be the same example as we saw in the last lecture, where more ice cream means more sharp attacks.

296
00:17:06,000 --> 00:17:10,000
 There was some correlation, but there was no correlation.

297
00:17:10,000 --> 00:17:20,000
 So we have thus discussed that the sample number is likely or unlikely to be a good feature depending on how we model the phone process.

298
00:17:20,000 --> 00:17:23,000
 So for now, let's ignore the sample number.

299
00:17:23,000 --> 00:17:26,000
 Imagine it does not provide any useful information.

300
00:17:26,000 --> 00:17:30,000
 So then we have some data table, which looks like the following.

301
00:17:30,000 --> 00:17:31,000
 Okay.

302
00:17:31,000 --> 00:17:36,000
 We call this entire table the training set.

303
00:17:37,000 --> 00:17:42,000
 And if you noted it, I have labeled it different products.

304
00:17:42,000 --> 00:17:45,000
 Anyone wants to tell why you are looking for it?

305
00:17:47,000 --> 00:17:48,000
 Okay.

306
00:17:48,000 --> 00:17:50,000
 And putting output is a very technical done.

307
00:17:50,000 --> 00:17:56,000
 Any other term you could talk about these two parts of the so-called way index.

308
00:17:56,000 --> 00:17:58,000
 So this looks like a way index.

309
00:17:58,000 --> 00:17:59,000
 We need way index.

310
00:17:59,000 --> 00:18:00,000
 I think they're in the platform.

311
00:18:00,000 --> 00:18:02,000
 Experience and performance.

312
00:18:02,000 --> 00:18:05,000
 So the conditioner is not very good performance.

313
00:18:05,000 --> 00:18:12,000
 The conditioner is the annotation or the label or the output assigned to this particular item.

314
00:18:12,000 --> 00:18:14,000
 So this is one parameter.

315
00:18:14,000 --> 00:18:16,000
 The next one is one parameter.

316
00:18:16,000 --> 00:18:19,000
 It's not very good performance.

317
00:18:20,000 --> 00:18:25,000
 We call these things as features, item keys or code gates.

318
00:18:25,000 --> 00:18:30,000
 If we go back, let's look at the equation which I was asking.

319
00:18:30,000 --> 00:18:31,000
 What do you do to features?

320
00:18:31,000 --> 00:18:33,000
 Make sense?

321
00:18:33,000 --> 00:18:36,000
 These things are called features.

322
00:18:36,000 --> 00:18:39,000
 The first three columns in this table are called features.

323
00:18:39,000 --> 00:18:43,000
 They are telling us something useful about the termator.

324
00:18:43,000 --> 00:18:46,000
 What are the four features that are going on?

325
00:18:46,000 --> 00:18:52,000
 So it's also the first, by the features that are over here, they're all used anonymously.

326
00:18:52,000 --> 00:18:54,000
 But you generally use features that are handy.

327
00:18:54,000 --> 00:18:57,000
 Obviates is generally used in some of the features.

328
00:18:57,000 --> 00:19:02,000
 And the output of the condition in this case is sort of the output.

329
00:19:02,000 --> 00:19:05,000
 The response wave is sort of the output of the response wave.

330
00:19:05,000 --> 00:19:09,000
 What is the response once you've observed some features?

331
00:19:09,000 --> 00:19:13,000
 Everyone, here, now.

332
00:19:13,000 --> 00:19:17,000
 Now, we call this a trading set.

333
00:19:17,000 --> 00:19:20,000
 And for now, it could be a bit of populism.

334
00:19:20,000 --> 00:19:25,000
 We call this entire matrix as D.

335
00:19:25,000 --> 00:19:28,000
 We call this feature matrix.

336
00:19:29,000 --> 00:19:31,000
 It contains n samples.

337
00:19:31,000 --> 00:19:36,000
 What is any equal to four samples?

338
00:19:36,000 --> 00:19:41,000
 And what is the features in this?

339
00:19:41,000 --> 00:19:45,000
 The number of features is color, size, and texture, which is three features.

340
00:19:45,000 --> 00:19:55,000
 So the matrix X shown in the shader pane is four rows of the columns matrix.

341
00:19:56,000 --> 00:19:59,000
 It contains n samples with tau p and m.

342
00:19:59,000 --> 00:20:07,000
 We can write an individual sample as something like this.

343
00:20:07,000 --> 00:20:12,000
 Like X1, we can write as orange, small, smooth.

344
00:20:12,000 --> 00:20:19,000
 Does anyone want to tell you why X1 is written in a volume format where in this particular

345
00:20:19,000 --> 00:20:23,000
 matrix X1 appears in a row?

346
00:20:23,000 --> 00:20:24,000
 Why?

347
00:20:24,000 --> 00:20:26,000
 That is the operation.

348
00:20:26,000 --> 00:20:27,000
 Yes.

349
00:20:27,000 --> 00:20:33,000
 So typically, when you consider the features or you consider a particular sample, you consider

350
00:20:33,000 --> 00:20:35,000
 that to be a form vector.

351
00:20:35,000 --> 00:20:40,000
 So this is where now we started to produce a little bit of an integrals.

352
00:20:40,000 --> 00:20:46,000
 Each sample is a column sector, which is what I'm interested in.

353
00:20:46,000 --> 00:20:49,000
 What is the dimension of this?

354
00:20:49,000 --> 00:20:55,000
 So the value of x is equal to three, this case all in small, small, smooth, three.

355
00:20:55,000 --> 00:21:00,000
 That's camera X as X i transpose where i is from month to month.

356
00:21:00,000 --> 00:21:02,000
 This is X1 transpose.

357
00:21:02,000 --> 00:21:03,000
 This is X2 transpose.

358
00:21:03,000 --> 00:21:11,000
 The third row is X3 transpose and X4 transpose.

359
00:21:11,000 --> 00:21:18,000
 We're able to now write the matrix X in terms of the individual.

360
00:21:19,000 --> 00:21:26,000
 So when we have an output vector Y, now this is going to be Y non-verbal.

361
00:21:26,000 --> 00:21:30,000
 For this simple example, we have a single output.

362
00:21:30,000 --> 00:21:36,000
 Single output variable or response variable, which is going to be Rn.

363
00:21:36,000 --> 00:21:47,000
 Because we have any examples, can we thus write this training set D as a set where we have

364
00:21:47,000 --> 00:21:52,000
 X i transpose from a Y i and i release from month to month.

365
00:21:52,000 --> 00:21:57,000
 So this way we're able to create this entire training set.

366
00:21:57,000 --> 00:22:01,000
 It can size comma depending on the i X sample.

367
00:22:01,000 --> 00:22:09,000
 So here, X i belongs to is a B R integral vector and Y is an integral vector.

368
00:22:09,000 --> 00:22:13,000
 Everyone clear the spot?

369
00:22:13,000 --> 00:22:20,000
 Now, the prediction tasks, this is where machine learning lengths usually break.

370
00:22:20,000 --> 00:22:24,000
 If you only have training set, there is no access used for it.

371
00:22:24,000 --> 00:22:29,000
 What you wanted to do was for unseen samples of our scheme for these samples,

372
00:22:29,000 --> 00:22:34,000
 for which a human has not yet annotated as a good commit or bad commit.

373
00:22:34,000 --> 00:22:41,000
 You want them to be fast in the seminary and compute the vision approach, which is based on camera,

374
00:22:41,000 --> 00:22:45,000
 which is looking at these tomatoes, they want their autotheated condition for them.

375
00:22:45,000 --> 00:22:46,000
 Right?

376
00:22:46,000 --> 00:22:49,000
 This is a goal clear to everyone.

377
00:22:49,000 --> 00:22:55,000
 For future unseen tomatoes, for which we don't have a human annotated answer,

378
00:22:55,000 --> 00:23:00,000
 we want tell whether the condition is better bad and then we want to accordingly process.

379
00:23:00,000 --> 00:23:07,000
 So we have for these unseen samples that I would draw a line to set where it is out.

380
00:23:07,000 --> 00:23:10,000
 For these unseen samples, we still observe the input features.

381
00:23:10,000 --> 00:23:16,000
 We still observe the color, size and texture as given by the computer vision system.

382
00:23:16,000 --> 00:23:17,000
 Right?

383
00:23:17,000 --> 00:23:19,000
 But we don't know the condition.

384
00:23:19,000 --> 00:23:21,000
 That is what we're trying to do.

385
00:23:21,000 --> 00:23:22,000
 Yeah?

386
00:23:22,000 --> 00:23:28,000
 We now break this whole matrix into two subsets.

387
00:23:28,000 --> 00:23:35,000
 The first we've already discussed is the training set, which we set most of the matrix D.

388
00:23:36,000 --> 00:23:42,000
 And now we have the test set where we have these specific entries on the condition as unknown,

389
00:23:42,000 --> 00:23:45,000
 which we're trying to estimate today.

390
00:23:45,000 --> 00:23:46,000
 Right?

391
00:23:46,000 --> 00:23:52,000
 So the testing set will be very similar to the training set, but it does not mean the variables for the output variable.

392
00:23:52,000 --> 00:23:53,000
 Right?

393
00:23:53,000 --> 00:23:56,000
 Everyone clear the distinction between training and testing?

394
00:23:56,000 --> 00:23:57,000
 Right?

395
00:23:57,000 --> 00:23:58,000
 Okay.

396
00:23:59,000 --> 00:24:00,000
 Okay.

397
00:24:00,000 --> 00:24:09,000
 So given the background that we have thus far, could we now tell what we're trying to do from this example in a more succinct fashion?

398
00:24:09,000 --> 00:24:15,000
 What do we hope to do given the training set, given the test set?

399
00:24:15,000 --> 00:24:19,000
 And we have to have some learning output.

400
00:24:19,000 --> 00:24:21,000
 It's the relating line.

401
00:24:21,000 --> 00:24:27,000
 How do you relate the input to the output?

402
00:24:27,000 --> 00:24:34,000
 I don't need a precise answer, but you can relate.

403
00:24:34,000 --> 00:24:39,000
 You can write the output as some function of the input.

404
00:24:39,000 --> 00:24:42,000
 Thus far, we don't know what kind of function is this.

405
00:24:42,000 --> 00:24:47,000
 And different algorithms we have with different functions relating the output to the input.

406
00:24:47,000 --> 00:24:53,000
 But we want to be able to predict the output using some functional form, let's say, for now.

407
00:24:54,000 --> 00:24:57,000
 And where do we learn this F from?

408
00:24:57,000 --> 00:24:59,000
 This function F from?

409
00:24:59,000 --> 00:25:01,000
 From the training set?

410
00:25:01,000 --> 00:25:05,000
 And where do we apply this function F on?

411
00:25:05,000 --> 00:25:07,000
 On the testing set.

412
00:25:07,000 --> 00:25:15,000
 So that given this, for this particular sample, given the inputs red, red, and red, you want to predict the condition.

413
00:25:15,000 --> 00:25:16,000
 Right?

414
00:25:16,000 --> 00:25:21,000
 And the general rule would be that we're able to do this accurately, otherwise it does not make any sense.

415
00:25:21,000 --> 00:25:22,000
 Right?

416
00:25:23,000 --> 00:25:27,000
 Okay, now a big question.

417
00:25:27,000 --> 00:25:34,000
 Is predicting on the test set enough to say that the model is invalidating?

418
00:25:34,000 --> 00:25:37,000
 Why or why not?

419
00:25:37,000 --> 00:25:40,000
 The test set might be missing on the output.

420
00:25:40,000 --> 00:25:43,000
 The test set might be missing on the output.

421
00:25:43,000 --> 00:25:44,000
 Missing on the output.

422
00:25:44,000 --> 00:25:45,000
 Missing on the output.

423
00:25:45,000 --> 00:25:46,000
 Okay?

424
00:25:46,000 --> 00:25:49,000
 So we will say that the test set might be missing on the output.

425
00:25:49,000 --> 00:25:53,000
 So it can add a little more weight rate.

426
00:25:53,000 --> 00:25:55,000
 The answer is therefore some data set.

427
00:25:55,000 --> 00:25:56,000
 Okay?

428
00:25:56,000 --> 00:25:57,000
 Okay.

429
00:25:57,000 --> 00:26:02,000
 Okay, so the answer is the answer that he is giving is that there could be some exceptions.

430
00:26:02,000 --> 00:26:04,000
 There could be some outriages.

431
00:26:04,000 --> 00:26:07,000
 But that is getting to the right answer.

432
00:26:07,000 --> 00:26:09,000
 You need to make it move there.

433
00:26:09,000 --> 00:26:12,000
 The test set, not the output is missing.

434
00:26:12,000 --> 00:26:15,000
 The test set, it does not have annotations.

435
00:26:15,000 --> 00:26:17,000
 So what will be the test set?

436
00:26:17,000 --> 00:26:21,000
 How will you get the accurate?

437
00:26:21,000 --> 00:26:25,000
 So we come to the specific question of how we do the check the accuracy.

438
00:26:25,000 --> 00:26:27,000
 Yeah, so we come to that.

439
00:26:27,000 --> 00:26:28,000
 Bye.

440
00:26:28,000 --> 00:26:29,000
 Yes, yes.

441
00:26:29,000 --> 00:26:36,000
 Okay.

442
00:26:36,000 --> 00:26:41,000
 The test set may be a subset of the brain set and your answer was that the test set might

443
00:26:41,000 --> 00:26:42,000
 have some outliers.

444
00:26:42,000 --> 00:26:45,000
 Both of you are really there.

445
00:26:45,000 --> 00:26:46,000
 That's good.

446
00:26:46,000 --> 00:26:51,000
 So we have not thus introduced it on the output.

447
00:26:51,000 --> 00:26:56,000
 Can you use some simpler numbers we have seen thus far?

448
00:26:56,000 --> 00:27:06,000
 Think of sometimes you might have covered in some probability scores.

449
00:27:07,000 --> 00:27:18,000
 So the ideal thing that we want to do is to be creating well of all possible inputs.

450
00:27:18,000 --> 00:27:19,000
 Right?

451
00:27:19,000 --> 00:27:23,000
 The emphasis on all possible inputs.

452
00:27:23,000 --> 00:27:25,000
 But can we test that?

453
00:27:25,000 --> 00:27:29,000
 Can the test set be all possible inputs?

454
00:27:29,000 --> 00:27:31,000
 Maybe an ideal case, yes.

455
00:27:31,000 --> 00:27:40,000
 But in a more practical case, you will never be able to enumerate all possible tests.

456
00:27:40,000 --> 00:27:45,000
 And now relating to the answer, which some of you have already given.

457
00:27:45,000 --> 00:27:50,000
 So one way to put it would be that the test set is only a sample from all possible inputs.

458
00:27:50,000 --> 00:27:51,000
 Right?

459
00:27:51,000 --> 00:27:57,000
 So this now relates to the answer we are talking about outliers.

460
00:27:57,000 --> 00:28:02,000
 You could end up using a test set which is a very expensive case around a half-wide

461
00:28:02,000 --> 00:28:07,000
 and does not contain outliers which might be present in all possible inputs.

462
00:28:07,000 --> 00:28:13,000
 And also relates to your specific answer that the test set could be a part of the brain set

463
00:28:13,000 --> 00:28:17,000
 because it's now a sample from all possible inputs.

464
00:28:17,000 --> 00:28:24,000
 More generally we have to think, let's think of it as there is some oracle or there is some

465
00:28:24,000 --> 00:28:31,000
 operating process which generates the brain data which I am calling there as the empirical

466
00:28:31,000 --> 00:28:32,000
 data sample.

467
00:28:32,000 --> 00:28:35,000
 And I have written a technical term here, IID.

468
00:28:35,000 --> 00:28:38,000
 So this is identity and independence distributed.

469
00:28:38,000 --> 00:28:44,000
 If I would recommend that if you don't know this term again, go back and study some of

470
00:28:44,000 --> 00:28:46,000
 the three records mentioned.

471
00:28:46,000 --> 00:28:53,000
 You sample from the hidden proof or you generate data from the computer process, you are able

472
00:28:53,000 --> 00:29:00,000
 to get some pretty data which you learn, you will get a model which in our previous case

473
00:29:00,000 --> 00:29:02,000
 was some function S that we will learn in.

474
00:29:02,000 --> 00:29:08,000
 Once you learn by function you predict using unseen data, you predict another sample.

475
00:29:08,000 --> 00:29:15,000
 So that sample is again also coming from the same data set, from the same underlying distribution

476
00:29:15,000 --> 00:29:20,000
 of the same data rating process by all the data.

477
00:29:20,000 --> 00:29:25,000
 So now the data set is the training set and the test for samples are not created into

478
00:29:25,000 --> 00:29:26,000
 distribution.

479
00:29:26,000 --> 00:29:29,000
 Sometimes it is also known as population.

480
00:29:29,000 --> 00:29:33,000
 You are getting some samples from the population.

481
00:29:33,000 --> 00:29:36,000
 So the test set will not contain all the samples.

482
00:29:36,000 --> 00:29:42,000
 In order to say that our model generalizes perfectly, we would have had to see the entire

483
00:29:42,000 --> 00:29:47,000
 population which is never the case.

484
00:29:47,000 --> 00:29:54,000
 And you have much more deeper differences between the test set and the group population.

485
00:29:54,000 --> 00:30:01,000
 Once we study biasing rates for some topics which hopefully we will discuss in the next

486
00:30:01,000 --> 00:30:02,000
 lecture.

487
00:30:02,000 --> 00:30:07,000
 Everyone clear the line?

488
00:30:07,000 --> 00:30:12,000
 Okay, so we have last part seen one particular type of machine learning task where you are

489
00:30:12,000 --> 00:30:17,000
 trying to classify whether the object or whether the particular the metals would have

490
00:30:17,000 --> 00:30:18,000
 had.

491
00:30:18,000 --> 00:30:20,000
 But now we can a very different example.

492
00:30:20,000 --> 00:30:25,000
 We want to predict the analytical number of IT on the other campus.

493
00:30:25,000 --> 00:30:27,000
 So again same exercise.

494
00:30:27,000 --> 00:30:32,000
 What factors do you think the analytical number should be based on?

495
00:30:32,000 --> 00:30:34,000
 Can you quantify that?

496
00:30:34,000 --> 00:30:39,000
 Because whether it is again one specific aspect of a data.

497
00:30:39,000 --> 00:30:40,000
 You have a real temperature.

498
00:30:40,000 --> 00:30:41,000
 Okay.

499
00:30:41,000 --> 00:30:43,000
 What do you expect the humidity is more?

500
00:30:43,000 --> 00:30:46,000
 Do you think they are doing more of it?

501
00:30:46,000 --> 00:30:47,000
 More?

502
00:30:47,000 --> 00:30:49,000
 Okay, more of a temperature.

503
00:30:49,000 --> 00:30:54,000
 Temperature is more, what energy do you want?

504
00:30:54,000 --> 00:30:55,000
 Fine.

505
00:30:55,000 --> 00:30:56,000
 More energy is less.

506
00:30:56,000 --> 00:31:01,000
 Okay, what are the other factors in opportunity to develop?

507
00:31:01,000 --> 00:31:04,000
 Health of the game.

508
00:31:05,000 --> 00:31:11,000
 Okay, number of people, number of occupants and if there are more occupants do you expect

509
00:31:11,000 --> 00:31:13,000
 more energy to develop?

510
00:31:13,000 --> 00:31:14,000
 Generally yes.

511
00:31:14,000 --> 00:31:16,000
 Any other factor?

512
00:31:16,000 --> 00:31:17,000
 No.

513
00:31:17,000 --> 00:31:18,000
 Sorry.

514
00:31:18,000 --> 00:31:27,000
 Okay, on the side of the airport there are some other aspects of a campus energy they

515
00:31:27,000 --> 00:31:31,000
 develop and they develop more

516
00:31:31,000 --> 00:31:32,000
 energy.

517
00:31:32,000 --> 00:31:33,000
 What are the aspects?

518
00:31:33,000 --> 00:31:34,000
 I think it is less.

519
00:31:34,000 --> 00:31:35,000
 It is less.

520
00:31:35,000 --> 00:31:36,000
 It is less.

521
00:31:36,000 --> 00:31:37,000
 It is less.

522
00:31:37,000 --> 00:31:38,000
 It is less.

523
00:31:38,000 --> 00:31:39,000
 It is less.

524
00:31:39,000 --> 00:31:40,000
 It is less.

525
00:31:40,000 --> 00:31:41,000
 It is less.

526
00:31:41,000 --> 00:31:42,000
 It is less.

527
00:31:42,000 --> 00:31:43,000
 It is less.

528
00:31:43,000 --> 00:31:44,000
 It is less.

529
00:31:44,000 --> 00:31:45,000
 It is less.

530
00:31:45,000 --> 00:31:46,000
 It is less.

531
00:31:46,000 --> 00:31:47,000
 It is less.

532
00:31:47,000 --> 00:31:48,000
 It is less.

533
00:31:48,000 --> 00:31:49,000
 It is less.

534
00:31:49,000 --> 00:31:50,000
 It is less.

535
00:31:50,000 --> 00:31:51,000
 It is less.

536
00:31:51,000 --> 00:31:52,000
 It is less.

537
00:31:52,000 --> 00:31:53,000
 It is less.

538
00:31:53,000 --> 00:31:54,000
 It is less.

539
00:31:54,000 --> 00:31:55,000
 It is less.

540
00:31:55,000 --> 00:31:56,000
 It is less.

541
00:31:56,000 --> 00:32:08,000
 When it puts positive value in profits, it comes from a

542
00:32:08,000 --> 00:32:16,000
 political Grioubt function.

543
00:32:16,000 --> 00:32:22,000
 So that is why it is elements of climate, vaccination.

544
00:32:22,000 --> 00:32:29,000
 of the people are let us say not in the maps, so does the population automatically reduce.

545
00:32:29,000 --> 00:32:34,000
 So, we send to the became population would be lesser and the became related to the data.

546
00:32:34,000 --> 00:32:41,000
 Yes, why you are there?

547
00:32:41,000 --> 00:32:46,000
 So, you could always come up with some counter events, yes, but there may be some people

548
00:32:46,000 --> 00:32:49,000
 who would be late when some people would become.

549
00:32:49,000 --> 00:32:51,000
 They are not very good.

550
00:32:51,000 --> 00:32:52,000
 They are not.

551
00:32:52,000 --> 00:32:54,000
 I mean, these are different.

552
00:32:54,000 --> 00:32:59,000
 So, he is saying that the hour of the day is also an important factor.

553
00:32:59,000 --> 00:33:04,000
 How can you gain tellers which are expected more in a different country?

554
00:33:04,000 --> 00:33:11,000
 So, in the end we have to take an hour of the time, we have to take an hour of the time,

555
00:33:11,000 --> 00:33:16,000
 but for now let us assume that, the numbers are 9 and people are 3 X as well as hard.

556
00:33:16,000 --> 00:33:20,000
 So, in the night time you generally see the lot to be low, at least in the relative denominator,

557
00:33:20,000 --> 00:33:25,000
 great figure.

558
00:33:25,000 --> 00:33:30,000
 So, more people generally more energy, higher temperature, generally higher energy.

559
00:33:30,000 --> 00:33:33,000
 Again, it can be constructed in a different way, like this.

560
00:33:33,000 --> 00:33:37,000
 We have people temperature and just for the purpose of the illustration,

561
00:33:37,000 --> 00:33:40,000
 we have people in temperature and energy in kilo-hour hours,

562
00:33:40,000 --> 00:33:42,000
 but I think some of them are very awesome.

563
00:33:42,000 --> 00:33:47,000
 You could use tools, but you know, they are very absolutely, very good.

564
00:33:47,000 --> 00:33:49,000
 Now, what is the training center?

565
00:33:49,000 --> 00:33:55,000
 The first three rows and they consider them as pre-made side,

566
00:33:55,000 --> 00:34:00,000
 because they have the labels, also mentioned the output variable or response variable also maintenance.

567
00:34:00,000 --> 00:34:06,000
 Whereas the set shown below of the last two samples becomes a test set,

568
00:34:06,000 --> 00:34:10,000
 where the labels or the response variable or the target variable of the information

569
00:34:10,000 --> 00:34:17,000
 have not been mentioned and this is what you are trying to say, right?

570
00:34:17,000 --> 00:34:22,000
 So, we have thus far seen two different kinds of examples.

571
00:34:22,000 --> 00:34:29,000
 Let us try and make a little more abstractly because of two specific classes of problems.

572
00:34:29,000 --> 00:34:33,000
 I am going to write the first class of problems as classification.

573
00:34:33,000 --> 00:34:38,000
 Here in the output variable of concern is discrete in nature, right?

574
00:34:38,000 --> 00:34:41,000
 Does everyone here understand what is discrete?

575
00:34:41,000 --> 00:34:44,000
 So, discrete means it could be one of few classes.

576
00:34:44,000 --> 00:34:48,000
 It could either be a, so this one of the examples of discrete is binary,

577
00:34:48,000 --> 00:34:50,000
 whether it is on or off.

578
00:34:50,000 --> 00:34:53,000
 It could also be done to be one of three classes.

579
00:34:53,000 --> 00:34:56,000
 It could also be minor, but not enough, right?

580
00:34:57,000 --> 00:35:04,000
 Both formerly they say that why I belong to a set from one to C where we are C classes.

581
00:35:04,000 --> 00:35:09,000
 We have seen one example can I even tell you more examples of classification tasks in my new restaurant?

582
00:35:09,000 --> 00:35:16,000
 We have to take a few minutes.

583
00:35:16,000 --> 00:35:19,000
 Sorry, I have to take a few minutes.

584
00:35:19,000 --> 00:35:20,000
 No, I am not.

585
00:35:20,000 --> 00:35:21,000
 How many minutes?

586
00:35:21,000 --> 00:35:23,000
 Who will then?

587
00:35:24,000 --> 00:35:30,000
 So, the example that is given is, you want to predict who will then, right?

588
00:35:30,000 --> 00:35:34,000
 So, in fact, this reminds me of some very interesting simulation.

589
00:35:34,000 --> 00:35:38,000
 So, I think there is a very nice log by a late silver.

590
00:35:38,000 --> 00:35:43,000
 Those of you who follow sports as well as data,

591
00:35:43,000 --> 00:35:45,000
 should be really looking into that.

592
00:35:45,000 --> 00:35:50,000
 And some of these log is fact, relatively winners of the football champions,

593
00:35:50,000 --> 00:35:52,000
 the NBA, the NBA, etc.

594
00:35:52,000 --> 00:35:53,000
 are you looking for the matches?

595
00:35:53,000 --> 00:35:57,000
 And they have some, they have some confidence with the same boys,

596
00:35:57,000 --> 00:36:00,000
 but with the league, etc.

597
00:36:00,000 --> 00:36:03,000
 Okay, so what are the kinds of results you have?

598
00:36:03,000 --> 00:36:06,000
 Let's focus on how something is included.

599
00:36:06,000 --> 00:36:09,000
 When you also have tie your top,

600
00:36:09,000 --> 00:36:10,000
 or particular cases.

601
00:36:10,000 --> 00:36:12,000
 So, this is not full class.

602
00:36:12,000 --> 00:36:13,000
 Yes.

603
00:36:13,000 --> 00:36:19,000
 And what kind of input features do you think you have to get?

604
00:36:19,000 --> 00:36:22,000
 And everyone else can also take this from out there.

605
00:36:22,000 --> 00:36:26,000
 So, if you want to predict whether India and Sri Lanka are playing,

606
00:36:26,000 --> 00:36:29,000
 that will be something who will then.

607
00:36:29,000 --> 00:36:32,000
 So, how will you describe that?

608
00:36:32,000 --> 00:36:33,000
 Okay.

609
00:36:33,000 --> 00:36:37,000
 Playing 11th, playing 11th, fine.

610
00:36:37,000 --> 00:36:40,000
 So, you know, by playing 11th, you mean the names?

611
00:36:40,000 --> 00:36:41,000
 You probably not.

612
00:36:41,000 --> 00:36:44,000
 So, some notion of their ratings, right?

613
00:36:44,000 --> 00:36:48,000
 All the having said that, only yesterday I had read about the very interesting article.

614
00:36:49,000 --> 00:36:53,000
 So, there was some research paper which mentioned that some of those

615
00:36:53,000 --> 00:36:56,000
 really chess playing as a play over natural language process,

616
00:36:56,000 --> 00:36:59,000
 a natural language processing game.

617
00:36:59,000 --> 00:37:02,000
 I'm not looking at the inherent notion of what both means.

618
00:37:02,000 --> 00:37:05,000
 So, they just gave it a specific easy code, etc.

619
00:37:05,000 --> 00:37:08,000
 And that will also be looking for all the relevant test tasks.

620
00:37:08,000 --> 00:37:13,000
 So, those of bellboards where they might find this very interesting and shocking.

621
00:37:13,000 --> 00:37:14,000
 Okay.

622
00:37:14,000 --> 00:37:17,000
 For cricket, maybe we are looking at the player ratings.

623
00:37:17,000 --> 00:37:22,000
 Maybe the player ratings, maybe if it does develop something on that start.

624
00:37:22,000 --> 00:37:23,000
 Sorry?

625
00:37:23,000 --> 00:37:25,000
 So, it's a blast phase.

626
00:37:25,000 --> 00:37:26,000
 Yes.

627
00:37:26,000 --> 00:37:31,000
 If there are, if there are, oh, this is the way.

628
00:37:31,000 --> 00:37:35,000
 And in fact, if any of you play games like T by Naj,

629
00:37:35,000 --> 00:37:36,000
 then what do you do?

630
00:37:36,000 --> 00:37:37,000
 You will write.

631
00:37:37,000 --> 00:37:39,000
 They take you to the consideration of the game.

632
00:37:39,000 --> 00:37:41,000
 And they are able to simulate both seasons.

633
00:37:41,000 --> 00:37:42,000
 Okay.

634
00:37:42,000 --> 00:37:45,000
 Any other task, documentation, talk you can think of.

635
00:37:46,000 --> 00:37:47,000
 End of.

636
00:37:47,000 --> 00:37:48,000
 End of.

637
00:37:48,000 --> 00:37:49,000
 End of.

638
00:37:49,000 --> 00:37:50,000
 End of the scene.

639
00:37:50,000 --> 00:37:51,000
 Okay.

640
00:37:51,000 --> 00:37:55,000
 So, some as data and image, you want to classify whether it's an image or it's an inverse inverse

641
00:37:55,000 --> 00:37:57,000
 that are on the scene.

642
00:37:57,000 --> 00:37:59,000
 You have not two output classes.

643
00:37:59,000 --> 00:38:02,000
 And what is the input that you have given?

644
00:38:02,000 --> 00:38:03,000
 No.

645
00:38:03,000 --> 00:38:06,000
 The input would be an image.

646
00:38:06,000 --> 00:38:07,000
 Right?

647
00:38:07,000 --> 00:38:11,000
 Input now would be an image of some item somewhere.

648
00:38:12,000 --> 00:38:16,000
 So, for the previous, if you look, the kinds of inputs that we have are putting into the

649
00:38:16,000 --> 00:38:17,000
 matrix.

650
00:38:17,000 --> 00:38:22,000
 So, in sample, most corresponding to only, you know, let's say p-takers.

651
00:38:22,000 --> 00:38:26,000
 But now you have a, let's say p cross q matrix.

652
00:38:26,000 --> 00:38:31,000
 So, again, you have to figure out, earlier, you will put the p cross q matrix into that particular

653
00:38:31,000 --> 00:38:33,000
 scheme which we have.

654
00:38:33,000 --> 00:38:34,000
 Okay.

655
00:38:34,000 --> 00:38:35,000
 Any other example?

656
00:38:35,000 --> 00:38:36,000
 Okay.

657
00:38:36,000 --> 00:38:41,000
 Shouldn't we, will it be, we'll train to follow it.

658
00:38:41,000 --> 00:38:43,000
 We are, we'll train to the other.

659
00:38:43,000 --> 00:38:44,000
 Okay.

660
00:38:44,000 --> 00:38:46,000
 Will I get a loan or not?

661
00:38:46,000 --> 00:38:51,000
 Depending on some, some idea, will I pass the machine on reports or not?

662
00:38:51,000 --> 00:38:54,000
 Will I remain away from the kind of lecture or not?

663
00:38:54,000 --> 00:38:56,000
 What is the quality of code?

664
00:38:56,000 --> 00:38:57,000
 Good, bad.

665
00:38:57,000 --> 00:39:00,000
 And you can have multiple such classes.

666
00:39:00,000 --> 00:39:03,000
 What data do I get in this class?

667
00:39:03,000 --> 00:39:05,000
 I get a lot of data.

668
00:39:05,000 --> 00:39:07,000
 I get a lot of data.

669
00:39:07,000 --> 00:39:08,000
 I get a lot of data.

670
00:39:08,000 --> 00:39:09,000
 I get a lot of data.

671
00:39:09,000 --> 00:39:10,000
 I get a lot of data.

672
00:39:10,000 --> 00:39:11,000
 I get a lot of data.

673
00:39:11,000 --> 00:39:12,000
 I get a lot of data.

674
00:39:12,000 --> 00:39:13,000
 I get a lot of data.

675
00:39:13,000 --> 00:39:14,000
 I get a lot of data.

676
00:39:14,000 --> 00:39:15,000
 I get a lot of data.

677
00:39:15,000 --> 00:39:16,000
 I get a lot of data.

678
00:39:16,000 --> 00:39:17,000
 I get a lot of data.

679
00:39:17,000 --> 00:39:18,000
 I get a lot of data.

680
00:39:18,000 --> 00:39:19,000
 I get a lot of data.

681
00:39:19,000 --> 00:39:20,000
 I get a lot of data.

682
00:39:20,000 --> 00:39:21,000
 I get a lot of data.

683
00:39:21,000 --> 00:39:22,000
 I get a lot of data.

684
00:39:22,000 --> 00:39:23,000
 I get a lot of data.

685
00:39:23,000 --> 00:39:24,000
 I get a lot of data.

686
00:39:24,000 --> 00:39:25,000
 I get a lot of data.

687
00:39:25,000 --> 00:39:26,000
 I get a lot of data.

688
00:39:26,000 --> 00:39:27,000
 I get a lot of data.

689
00:39:27,000 --> 00:39:28,000
 I get a lot of data.

690
00:39:28,000 --> 00:39:29,000
 I get a lot of data.

691
00:39:29,000 --> 00:39:30,000
 I get a lot of data.

692
00:39:30,000 --> 00:39:38,000
 Just like video, this exact photographing, this exact time that have created it.

693
00:39:38,000 --> 00:39:39,000
 Right.

694
00:39:39,000 --> 00:39:47,000
 I get a lot of data here.

695
00:39:47,000 --> 00:39:54,000
 Right now we need to make that figure.

696
00:39:54,000 --> 00:39:55,000
 I get a lot of data.

697
00:39:55,000 --> 00:39:59,000
 So, what do you want to do today?

698
00:39:59,000 --> 00:40:03,000
 The price of a particular stock, any other example.

699
00:40:03,000 --> 00:40:05,000
 That's 4.5.

700
00:40:05,000 --> 00:40:06,000
 Okay.

701
00:40:06,000 --> 00:40:07,000
 How many brands of 4?

702
00:40:07,000 --> 00:40:09,000
 Okay, how many brands with our teams?

703
00:40:09,000 --> 00:40:12,000
 So, again, very interesting points of radius of 1.

704
00:40:12,000 --> 00:40:17,000
 Because again, if you look at all the sports, people have already come up with some rules of the family.

705
00:40:17,000 --> 00:40:21,000
 You are only 3 back to down, and you get half to 30, you would be double.

706
00:40:21,000 --> 00:40:26,000
 You can always verify those rules.

707
00:40:26,000 --> 00:40:30,000
 And before we get into the actual algorithms,

708
00:40:30,000 --> 00:40:35,000
 I wanted to talk about the performance measure P, which we discussed in the description.

709
00:40:35,000 --> 00:40:40,000
 We talked about experience P, task P, and performance measure P.

710
00:40:40,000 --> 00:40:45,000
 I thought it's more important to first understand what different metrics mean,

711
00:40:45,000 --> 00:40:49,000
 and what does it mean that we have done a good job in machine learning or not?

712
00:40:49,000 --> 00:40:52,000
 We started with some metrics of application.

713
00:40:52,000 --> 00:40:57,000
 Let's assume that we had a ground growth Y.

714
00:40:57,000 --> 00:41:00,000
 Ground growth means the correct labels that we've got.

715
00:41:00,000 --> 00:41:05,000
 Let's say we had a particular set of 2.0s.

716
00:41:05,000 --> 00:41:08,000
 For the first parameter, we know that it was good.

717
00:41:08,000 --> 00:41:10,000
 Some human and a redirect.

718
00:41:10,000 --> 00:41:13,000
 Second parameter, some unsighted is good, and the other 3, some said.

719
00:41:13,000 --> 00:41:16,000
 Some export label, then, was bad.

720
00:41:16,000 --> 00:41:20,000
 And then there is some algorithm, some function F that we've learned.

721
00:41:20,000 --> 00:41:25,000
 That means of predicting, good, good, good, good, good, bad.

722
00:41:25,000 --> 00:41:29,000
 And we call this vector as Y-act.

723
00:41:29,000 --> 00:41:33,000
 So we'll use this ad a lot in machine learning.

724
00:41:33,000 --> 00:41:36,000
 And generally signifies an estimate.

725
00:41:36,000 --> 00:41:41,000
 What is your estimate of these labels of the parameters?

726
00:41:41,000 --> 00:41:44,000
 So it is Y-act.

727
00:41:44,000 --> 00:41:47,000
 And number comes from the actual running set.

728
00:41:47,000 --> 00:41:49,000
 For now, that's the thing we have.

729
00:41:49,000 --> 00:41:51,000
 Some of them have created a problem.

730
00:41:51,000 --> 00:41:52,000
 We know the example.

731
00:41:52,000 --> 00:41:55,000
 And the prediction is made by the model.

732
00:41:55,000 --> 00:41:56,000
 Right?

733
00:41:56,000 --> 00:42:00,000
 So what are the different metrics we could use to tell the real system that I'm good at?

734
00:42:00,000 --> 00:42:01,000
 What is it?

735
00:42:01,000 --> 00:42:03,000
 You have done a good job in predicting.

736
00:42:03,000 --> 00:42:05,000
 What have you done bad?

737
00:42:05,000 --> 00:42:06,000
 I don't know.

738
00:42:06,000 --> 00:42:08,000
 It's bad job.

739
00:42:08,000 --> 00:42:11,000
 Okay, how many things you've done?

740
00:42:11,000 --> 00:42:14,000
 Okay, why?

741
00:42:14,000 --> 00:42:17,000
 I think we're in the meeting.

742
00:42:17,000 --> 00:42:19,000
 Sorry.

743
00:42:20,000 --> 00:42:22,000
 Depends on the answer.

744
00:42:22,000 --> 00:42:23,000
 Okay.

745
00:42:23,000 --> 00:42:29,000
 The answer is depends on the state of the art, which is a very valid answer because what

746
00:42:29,000 --> 00:42:36,000
 is fine to say is that you want to penalize the accuracy of the metric that we put up on it.

747
00:42:36,000 --> 00:42:40,000
 Just say 80% accurate, it does not mean that.

748
00:42:41,000 --> 00:42:48,000
 Only if the best report thus far was 60% and 80% accuracy means a lot.

749
00:42:48,000 --> 00:42:57,000
 We first look at a very simple metric on the accuracy, which basically tells us that how

750
00:42:57,000 --> 00:43:03,000
 many you look at the accuracy across Y-act, you look at the predictions across specific

751
00:43:03,000 --> 00:43:08,000
 data, rows and this, and how many times is Y-act equal to Y?

752
00:43:08,000 --> 00:43:11,000
 Divided by the length of Y-act or Y?

753
00:43:11,000 --> 00:43:12,000
 Right?

754
00:43:12,000 --> 00:43:16,000
 So how many times have we accurately or correctly predicted the linear?

755
00:43:16,000 --> 00:43:23,000
 The accuracy in this case is in terms of proportion is 0.6 in terms of percentage is 60%.

756
00:43:23,000 --> 00:43:24,000
 Right?

757
00:43:24,000 --> 00:43:28,000
 So it tells you one specific number, but this is often not enough.

758
00:43:28,000 --> 00:43:32,000
 And there are different cases in which we need to again contextualize.

759
00:43:32,000 --> 00:43:38,000
 Let's look at some specific types of data to motivate the algorithm.

760
00:43:38,000 --> 00:43:41,000
 Let's imagine that we have 1.01 samples.

761
00:43:41,000 --> 00:43:46,000
 Any of breaking something which is one scale of either, which is either word or bad.

762
00:43:46,000 --> 00:43:54,000
 And we have, and for examples where the ground growth or the actual samples are good, right?

763
00:43:54,000 --> 00:44:02,000
 And one sample where the actual quality is bad, right?

764
00:44:02,000 --> 00:44:08,000
 Now, there could be multiple cases in which such kind of data is good.

765
00:44:08,000 --> 00:44:15,000
 For example, the cancer screening, you would hope that in most cases, cancer screening,

766
00:44:15,000 --> 00:44:18,000
 we will have people's health is good.

767
00:44:18,000 --> 00:44:19,000
 They don't have cancer.

768
00:44:19,000 --> 00:44:23,000
 Unfortunately, for very small, they will be cancer.

769
00:44:23,000 --> 00:44:28,000
 But in general, if we look at some data, we will expect it to look something like this.

770
00:44:28,000 --> 00:44:31,000
 Go to good, go to good, go cancer, go cancer, go cancer, go cancer.

771
00:44:31,000 --> 00:44:32,000
 Right?

772
00:44:32,000 --> 00:44:35,000
 So you end up with an imbalance data set.

773
00:44:35,000 --> 00:44:42,000
 And maybe even with a RNA detector, someone is trying to detect problems on imagery or on some means.

774
00:44:42,000 --> 00:44:46,000
 Most of the time, you should have no bacteria, go for them, go for them, go for them.

775
00:44:46,000 --> 00:44:48,000
 And some plants, go for them.

776
00:44:48,000 --> 00:44:53,000
 Otherwise, people will be getting over twice a period of time.

777
00:44:55,000 --> 00:44:59,000
 Now, let's look at a different metric now.

778
00:44:59,000 --> 00:45:01,000
 It is called precision.

779
00:45:01,000 --> 00:45:06,000
 Let's look at any number of the same paper as we took earlier.

780
00:45:06,000 --> 00:45:12,000
 Now, before looking at this site, can someone tell them, can you understand that it's a competition?

781
00:45:12,000 --> 00:45:16,000
 In general, in this conference, they might not.

782
00:45:17,000 --> 00:45:19,000
 If it is a guarantee.

783
00:45:19,000 --> 00:45:20,000
 Sorry.

784
00:45:20,000 --> 00:45:21,000
 Reputability or anything?

785
00:45:21,000 --> 00:45:22,000
 Reputability, okay.

786
00:45:22,000 --> 00:45:25,000
 What does it mean to be very specific?

787
00:45:27,000 --> 00:45:28,000
 If it is.

788
00:45:28,000 --> 00:45:30,000
 It means less, okay.

789
00:45:30,000 --> 00:45:35,000
 Any other, on what does it mean to be verified?

790
00:45:35,000 --> 00:45:42,000
 What do you mean by when someone asks, can you speak of the type of thing or can you write

791
00:45:42,000 --> 00:45:48,000
 down the term of the type of thing?

792
00:45:48,000 --> 00:45:51,000
 Sorry, do the math.

793
00:45:51,000 --> 00:45:52,000
 Do the math?

794
00:45:52,000 --> 00:45:57,000
 What do you think of precision in terms of whatever you are writing or whatever you are

795
00:45:57,000 --> 00:46:00,000
 predicting or how to that you mean?

796
00:46:00,000 --> 00:46:01,000
 How true that is?

797
00:46:01,000 --> 00:46:03,000
 Or how much sense that does mean?

798
00:46:03,000 --> 00:46:10,000
 So, more English terms, I mean, limiters, it might mean that how much sense does it make

799
00:46:10,000 --> 00:46:11,000
 what you are writing.

800
00:46:11,000 --> 00:46:20,000
 So, now, when you go over common definition, you look at the times you predicted good, right?

801
00:46:20,000 --> 00:46:24,000
 You predicted how many times, 1, 2, 3 and 4.

802
00:46:24,000 --> 00:46:27,000
 Out of these, 4 times you predicted good.

803
00:46:27,000 --> 00:46:29,000
 How many times would you actually go?

804
00:46:29,000 --> 00:46:33,000
 Or was the ground truth or the same word, right?

805
00:46:33,000 --> 00:46:39,000
 So, you predicted good 4 times, but you will not predict precise in predicting good.

806
00:46:39,000 --> 00:46:45,000
 Your precision was thus far, thus, 2 or 4, which is where you are concerned.

807
00:46:45,000 --> 00:46:46,000
 Right?

808
00:46:46,000 --> 00:46:48,000
 There is everyone getting a definition.

809
00:46:48,000 --> 00:46:53,000
 How precise are you in predicting a particular class?

810
00:46:53,000 --> 00:46:57,000
 Now, precision can be defined in terms of the specific path.

811
00:46:57,000 --> 00:47:00,000
 You could also define precision for the path, right?

812
00:47:00,000 --> 00:47:03,000
 What is the precision for the path?

813
00:47:03,000 --> 00:47:10,000
 You predicted bad once, but for that specific time it was actually not bad.

814
00:47:10,000 --> 00:47:14,000
 So, you have got real precision in predicting bad.

815
00:47:14,000 --> 00:47:20,000
 So, precision for the path to 10, be 0 or 1, to be 0, precision for the path is now 2 or 4.

816
00:47:20,000 --> 00:47:21,000
 Right?

817
00:47:21,000 --> 00:47:26,000
 Or what you think you would write a fraction of relevant expenses amongst the degree.

818
00:47:26,000 --> 00:47:27,000
 Right?

819
00:47:27,000 --> 00:47:36,000
 Now, we come to another metric called the recon, which is of clearly similar notion.

820
00:47:36,000 --> 00:47:38,000
 Now, what is the word English?

821
00:47:38,000 --> 00:47:40,000
 What is the English word?

822
00:47:40,000 --> 00:47:44,000
 What does the English word be?

823
00:47:44,000 --> 00:47:49,000
 What does it mean that person X has the way to read the word?

824
00:47:49,000 --> 00:47:50,000
 Right?

825
00:47:50,000 --> 00:47:51,000
 Right?

826
00:47:51,000 --> 00:47:52,000
 Right?

827
00:47:53,000 --> 00:47:55,000
 Able to remember, right?

828
00:47:55,000 --> 00:47:56,000
 Something on that one.

829
00:47:56,000 --> 00:48:03,000
 So, the definition of the equal is how many times it was actually good?

830
00:48:03,000 --> 00:48:07,000
 How much of the, how much of that you were able to read by in a prediction?

831
00:48:07,000 --> 00:48:08,000
 Right?

832
00:48:08,000 --> 00:48:11,000
 So, it was good, fair, fair and fair.

833
00:48:11,000 --> 00:48:14,000
 I'll, you showed it to the average.

834
00:48:14,000 --> 00:48:18,000
 We have been able to read all two of these three.

835
00:48:18,000 --> 00:48:22,000
 So, three times the definition of the meter was good.

836
00:48:22,000 --> 00:48:24,000
 We have been able to read all two times.

837
00:48:24,000 --> 00:48:27,000
 That's the, it all is true at the 60 times.

838
00:48:27,000 --> 00:48:28,000
 Right?

839
00:48:28,000 --> 00:48:31,000
 We see a little bit of precision and we talk.

840
00:48:31,000 --> 00:48:36,000
 Everyone will be able to listen.

841
00:48:36,000 --> 00:48:41,000
 Well, again, we are trying to predict whether tissues cancel us or not.

842
00:48:41,000 --> 00:48:47,000
 In the ground truth, we see that we have 100 samples out of which only we can do.

843
00:48:47,000 --> 00:48:54,000
 Which only one of them has cancer, which is the last sample, which is shown over here.

844
00:48:54,000 --> 00:49:03,000
 And when we are predicting, we predict 99 times that the person or the specific sample is not cancerous.

845
00:49:03,000 --> 00:49:07,000
 And one time, which is the first sample, we predict it to be cancerous.

846
00:49:07,000 --> 00:49:08,000
 Right?

847
00:49:08,000 --> 00:49:15,000
 So, now let's try and understand the precision and recall for this set of predictions.

848
00:49:15,000 --> 00:49:19,000
 The accuracy of the system is fairly good.

849
00:49:19,000 --> 00:49:23,000
 Out of the total hundred times, we were accurate 98 times.

850
00:49:23,000 --> 00:49:26,000
 The only two times we are getting wrong is one.

851
00:49:26,000 --> 00:49:30,000
 The time, for the first sample, when we are predicting it to be cancerous, viral.

852
00:49:30,000 --> 00:49:33,000
 In ground truth, it is not cancerous.

853
00:49:33,000 --> 00:49:37,000
 And the other time we are getting it wrong is when we are predicting it to be not cancerous.

854
00:49:37,000 --> 00:49:40,000
 And the ground truth says it is cancerous, which is the last sample.

855
00:49:40,000 --> 00:49:42,000
 The accuracy is 98 times.

856
00:49:42,000 --> 00:49:45,000
 You have correctly identified one of the hundred times.

857
00:49:45,000 --> 00:49:48,000
 Now let's look at the recall.

858
00:49:48,000 --> 00:49:54,000
 Out of the times, the ground truth was true, which is the hundred sample.

859
00:49:54,000 --> 00:49:56,000
 Only a single sample.

860
00:49:56,000 --> 00:50:02,000
 Do we predict it to be cancerous in our prediction system?

861
00:50:02,000 --> 00:50:03,000
 No.

862
00:50:03,000 --> 00:50:09,000
 Sort of one time where it was actually cancerous, we are not able to recall that prediction

863
00:50:09,000 --> 00:50:12,000
 correctly, thus the recall is 0 over 1, which is 0.

864
00:50:12,000 --> 00:50:15,000
 Let's look at the precision now.

865
00:50:15,000 --> 00:50:22,000
 Out of the one time which we are predicting the tissue to be cancerous, was it actually

866
00:50:22,000 --> 00:50:23,000
 cancerous?

867
00:50:23,000 --> 00:50:24,000
 No.

868
00:50:24,000 --> 00:50:26,000
 In the ground truth, it was not cancerous.

869
00:50:26,000 --> 00:50:29,000
 Thus the precision is 0 over 1, which is 0.

870
00:50:29,000 --> 00:50:38,000
 There is another way to look at the previous example, which is known as confusion matrix.

871
00:50:38,000 --> 00:50:41,000
 We have four entries in this confusion matrix.

872
00:50:41,000 --> 00:50:46,000
 The ground truth could be either yes or no, which is cancerous or not cancerous.

873
00:50:46,000 --> 00:50:52,000
 And similarly, we could predict to be either cancerous or not cancerous.

874
00:50:52,000 --> 00:51:01,000
 We saw previously that out of the 90, out of the hundred instances, 98 times when the ground

875
00:51:01,000 --> 00:51:06,000
 truth was not cancerous, we were also able to predict it as not cancerous.

876
00:51:06,000 --> 00:51:14,000
 Thus the entry corresponding to predicted equal to no and ground truth equal to no is 98.

877
00:51:14,000 --> 00:51:22,000
 For one instance, if you look at the first sample, the prediction is yes, but the ground truth was no.

878
00:51:22,000 --> 00:51:29,000
 So the prediction is yes, the ground truth is no, which is the first row and the second column.

879
00:51:29,000 --> 00:51:36,000
 And then if we go back, we thought there was one sample where the ground truth was yes, but the prediction was not.

880
00:51:36,000 --> 00:51:43,000
 So one sample where the ground truth was yes, the prediction was no, which is the first column and the second row.

881
00:51:43,000 --> 00:51:52,000
 Now, can you think about precision and recall in terms of these quantities or in terms of the confusion matrix?

882
00:51:52,000 --> 00:51:59,000
 But before we do that, let us make the confusion matrix more generalizable.

883
00:51:59,000 --> 00:52:08,000
 So we now have four quantities. We have four numbers which are written as true positive, false positive, false negative and true negative.

884
00:52:08,000 --> 00:52:12,000
 Let's try and understand how do we remember these four names.

885
00:52:12,000 --> 00:52:20,000
 Let's look at the first true positive. The ground truth was positive and we're predicting it to be true.

886
00:52:20,000 --> 00:52:26,000
 We're truly predicting it to be positive. Thus, it is truly predicted as positive. That's true positive.

887
00:52:26,000 --> 00:52:37,000
 The first one in the second column is false positive. So the ground truth was not positive, but we are falsely predicting it to be positive.

888
00:52:37,000 --> 00:52:49,000
 Thus, false positive. The third element is false negative. The ground truth was yes or positive, but we're falsely predicting it to be negative.

889
00:52:49,000 --> 00:52:57,000
 So it is a false negative and the last entry is a true negative. The ground truth was a negative and the prediction was also negative.

890
00:52:57,000 --> 00:53:02,000
 Thus, you're truly predicting it to be negative.

891
00:53:02,000 --> 00:53:14,000
 Now, let's come back to the definitions of recurrence precision and try to write them in terms of the four quantities from the confusion matrix that we have just seen.

892
00:53:15,000 --> 00:53:23,000
 So when we talk about precision, we spoke about how correct we are when we predicted it to be the positive class.

893
00:53:23,000 --> 00:53:30,000
 Let's say yes in this case. Out of the total number of times we predicted it to yes, how accurate we were.

894
00:53:30,000 --> 00:53:39,000
 So the first row corresponds to the total number of times we're predicting it to yes. This becomes a denominator, which is true positive, plus false positive.

895
00:53:40,000 --> 00:53:49,000
 And the portion where we correct is the true positive, where we were predicting it to be positive or yes, when it is actually yes.

896
00:53:49,000 --> 00:53:58,000
 Thus, the numerator becomes true positive. Thus, the precision is given by true positive over true positive plus false positive.

897
00:53:59,000 --> 00:54:13,000
 Similarly, let's think about recall. For recall, we said that out of the instances which were true in the ground truth, how many are we able to recall.

898
00:54:13,000 --> 00:54:25,000
 So, thus the instances which were true in the ground truth becomes a denominator, which is the first column of this matrix, which corresponds to true positive plus false negative.

899
00:54:26,000 --> 00:54:34,000
 And the fraction which is identified correctly is the true positive. Or what we are able to recall is the true positive.

900
00:54:34,000 --> 00:54:40,000
 Thus, the recall becomes true positive over true positive plus false negative.

901
00:54:42,000 --> 00:54:48,000
 We have another metric called the F score, which combines the precision and recall in the following ways.

902
00:54:49,000 --> 00:54:57,000
 So, the definition is given by the formula is given by twice precision times recall, divided by precision plus recallits.

903
00:54:57,000 --> 00:55:01,000
 Sometimes useful to give a single number instead of giving a precision and a recall.

904
00:55:04,000 --> 00:55:13,000
 There is another interesting metric called Matthew's correlation coefficient. The formula looks fairly complicated at this point of time if you see.

905
00:55:14,000 --> 00:55:24,000
 But there is one particular reason why this coefficient is very useful. And to see that specific reason, let's try to work out a simple example now.

906
00:55:27,000 --> 00:55:34,000
 For the data that you have given below, where the ground truth positive and predicted positive is the largest number 90.

907
00:55:35,000 --> 00:55:44,000
 And the other three entries are also in the field in the matrix and fusion matrix. Can you calculate the precision recall F score and Matthew's coefficient?

908
00:55:48,000 --> 00:55:55,000
 Okay, let's talk about precision for now. The precision is out of the times you are predicting it to be positive how correct you are.

909
00:55:56,000 --> 00:56:01,000
 For that, we look at the row corresponding to predictive positive that becomes a denominator.

910
00:56:01,000 --> 00:56:09,000
 Thus, the total entries are 90 plus 4, which is 94. And how many of them are we correctly identifying that is 90?

911
00:56:09,000 --> 00:56:13,000
 Thus, precision becomes 90 over 94.

912
00:56:14,000 --> 00:56:23,000
 Similarly, if you look for recall, out of the entries which were positive in the ground truth, that becomes the first column.

913
00:56:23,000 --> 00:56:30,000
 That is 90 plus 1, which is 91 entries. How many are we able to recall correctly? That is 90.

914
00:56:30,000 --> 00:56:40,000
 Thus, recall becomes 90 over 91. And we can calculate the F score by twice precision times recall divided by precision plus recall.

915
00:56:40,000 --> 00:56:47,000
 Now, all of these numbers are giving an indication that we have done a very good job by the identification or prediction.

916
00:56:47,000 --> 00:56:49,000
 But does this seem to be a problem?

917
00:56:50,000 --> 00:57:02,000
 Yes, so the problem is that this was a very, very easy problem for classification because most of the instances were positive in the ground truth.

918
00:57:02,000 --> 00:57:09,000
 So, if you predicted everything to be positive, you will have a fairly high precision and recall and accuracy and F score.

919
00:57:09,000 --> 00:57:13,000
 But the Matthew's coefficient comes out to be fairly low.

920
00:57:14,000 --> 00:57:24,000
 What this is telling us is that you are not doing a substantially good job, identifying or predicting in such a case because the problem itself was fairly simple.

921
00:57:24,000 --> 00:57:40,000
 So, this is where we need to take all the metrics and all the results with the salt of grain because it is important to look at how easy or difficult it was when you could have predicted the most occurring class.

922
00:57:41,000 --> 00:57:48,000
 I, ground truth, has a vector of grain on the, and the prediction will also be a vector of grain on the, right?

923
00:57:48,000 --> 00:57:52,000
 We first met with which we look at the called the main squared error.

924
00:57:52,000 --> 00:57:59,000
 The way to remember this is to compose the three jobs, main squared error, and then put it in reverse bag.

925
00:57:59,000 --> 00:58:07,000
 You first compute the error, which is y i hat minus y i, right?

926
00:58:07,000 --> 00:58:10,000
 Predicted minus ground, called the i n sample.

927
00:58:10,000 --> 00:58:14,000
 You have computed the error and also shown the corresponding error.

928
00:58:21,000 --> 00:58:23,000
 And see the different curves now.

929
00:58:23,000 --> 00:58:28,000
 So, y i minus y, y i minus y i is the error term.

930
00:58:28,000 --> 00:58:33,000
 Then you have a squared term. So, y i minus y i hat minus y i.

931
00:58:33,000 --> 00:58:36,000
 For example, you squared the error, right?

932
00:58:36,000 --> 00:58:41,000
 And then you find it in the main overhead, which is the main overhead and sample, right?

933
00:58:41,000 --> 00:58:45,000
 Main squared error, policy error squared, 18 mean.

934
00:58:45,000 --> 00:58:53,000
 And a term is generally then used in the return of order root mean squared error, which is the root of the mean squared error.

935
00:58:53,000 --> 00:58:54,000
 Right?

936
00:58:55,000 --> 00:58:59,000
 There are others, very similar metric called the main absolute error.

937
00:58:59,000 --> 00:59:04,000
 Again, it starts with the inside, the first calculate the error y i hat and y i.

938
00:59:04,000 --> 00:59:07,000
 If the absolute value is a bit, then take the mean.

939
00:59:07,000 --> 00:59:08,000
 Right?

940
00:59:08,000 --> 00:59:11,000
 We can also come with a metric called the mean error.

941
00:59:11,000 --> 00:59:14,000
 We just first compute the error and take the mean squared.

942
00:59:14,000 --> 00:59:16,000
 Why is that a part right?

943
00:59:16,000 --> 00:59:17,000
 Use it as a metric.

944
00:59:17,000 --> 00:59:18,000
 Yeah, we can put it here.

945
00:59:18,000 --> 00:59:20,000
 Then we cancel each other out.

946
00:59:20,000 --> 00:59:28,000
 We can have a prediction, imagine the number of all 0, 0, 0, 0, 0, all you have for example all 0.

947
00:59:28,000 --> 00:59:31,000
 And I have predicted as plus and minus and plus and minus.

948
00:59:31,000 --> 00:59:33,000
 What is the mean error in the space?

949
00:59:33,000 --> 00:59:35,000
 The mean error is 0.

950
00:59:35,000 --> 00:59:38,000
 What does the mean absolute error?

951
00:59:38,000 --> 00:59:40,000
 What's the mean?

952
00:59:40,000 --> 00:59:41,000
 What's the mean?

953
00:59:41,000 --> 00:59:42,000
 What's the mean?

954
00:59:42,000 --> 00:59:44,000
 Yeah, I will say it would be 0.

955
00:59:44,000 --> 00:59:45,000
 What's the mean?

956
00:59:45,000 --> 00:59:49,000
 So, this is why it's also what it's known, what are the metrics?

957
00:59:49,000 --> 00:59:54,000
 You're trying to optimize them.

958
00:59:54,000 --> 01:00:00,000
 Why you might want to use mean squared error or mean absolute error times?

959
01:00:00,000 --> 01:00:01,000
 Right?

960
01:00:01,000 --> 01:00:03,000
 The expensive.

961
01:00:03,000 --> 01:00:07,000
 Maybe something more familiar.

962
01:00:07,000 --> 01:00:08,000
 Sorry.

963
01:00:08,000 --> 01:00:11,000
 No, no, Quézon.

964
01:00:11,000 --> 01:00:12,000
 It's something even familiar.

965
01:00:12,000 --> 01:00:14,000
 Can I use the strength of the mean?

966
01:00:14,000 --> 01:00:15,000
 I don't know.

967
01:00:15,000 --> 01:00:18,000
 The strength of how we use the mean.

968
01:00:18,000 --> 01:00:19,000
 Okay.

969
01:00:19,000 --> 01:00:25,000
 So, can you tell me how does the RMSC, so if, if Vi i hat is very far away from Vi, which

970
01:00:25,000 --> 01:00:30,000
 is going to produce more error?

971
01:00:30,000 --> 01:00:31,000
 Square.

972
01:00:31,000 --> 01:00:38,000
 So, can you say that squared errors, they can penalize back predictions much more than

973
01:00:38,000 --> 01:00:45,000
 the regular.

974
01:00:46,000 --> 01:00:51,000
 So, now let's quickly get into the first algorithm for today.

975
01:00:51,000 --> 01:00:55,000
 We're going to talk about decision trees.

976
01:00:55,000 --> 01:01:06,000
 We are in now solving a classification problem using our first evaluation contribution

977
01:01:06,000 --> 01:01:07,000
 tree.

978
01:01:07,000 --> 01:01:13,000
 So, we have some training data where we have the different days from D1 to D4E.

979
01:01:13,000 --> 01:01:17,000
 Should the day be included as an accurate or not?

980
01:01:17,000 --> 01:01:18,000
 That is the other assignment.

981
01:01:18,000 --> 01:01:20,000
 So, no wonder that we are out now.

982
01:01:20,000 --> 01:01:26,000
 We have the output of whether it's sunny or hot, etc. or hot, etc. or humidity and wind.

983
01:01:26,000 --> 01:01:28,000
 And whether or not we played it.

984
01:01:28,000 --> 01:01:29,000
 Right?

985
01:01:29,000 --> 01:01:33,000
 So, now you're trying to predict whether I should or learn the function between data

986
01:01:33,000 --> 01:01:35,000
 and something attributes are concerned.

987
01:01:35,000 --> 01:01:36,000
 Right?

988
01:01:36,000 --> 01:01:42,000
 These are, as we discussed in the four features, uh, four-way attributes.

989
01:01:42,000 --> 01:01:43,000
 And the output variable.

990
01:01:43,000 --> 01:01:48,000
 And this is profit equation because the output variable is concerned.

991
01:01:48,000 --> 01:01:49,000
 It's just free.

992
01:01:49,000 --> 01:01:52,000
 In this case, it's only in yes or so.

993
01:01:52,000 --> 01:01:53,000
 Right?

994
01:01:53,000 --> 01:02:00,000
 And because I've also used decision trees to follow regression here, where we try to predict

995
01:02:00,000 --> 01:02:03,000
 the house price given the square point of the energy.

996
01:02:03,000 --> 01:02:04,000
 Right?

997
01:02:04,000 --> 01:02:07,000
 Let's get back to the example which we were discussing.

998
01:02:07,000 --> 01:02:12,000
 I'm not going to talk about this for now.

999
01:02:12,000 --> 01:02:16,000
 So, one of the reasons why I'm about to put it differently is started.

1000
01:02:16,000 --> 01:02:21,000
 We remember last time, we were discussing things like over and test our crashes.

1001
01:02:21,000 --> 01:02:25,000
 Imagine if we are very, very complicated machinaling the bottom.

1002
01:02:25,000 --> 01:02:28,000
 Let's say some near-retrochemical bottom.

1003
01:02:28,000 --> 01:02:32,000
 There is often a very, there is often a case of very hard to do,

1004
01:02:32,000 --> 01:02:35,000
 and differentically, for these models.

1005
01:02:35,000 --> 01:02:40,000
 Not really to understand what the model is done, but the shift trees being one of the

1006
01:02:40,000 --> 01:02:45,000
 very simple models are very, very suited for such cases.

1007
01:02:45,000 --> 01:02:48,000
 Where you want the model to be educated.

1008
01:02:48,000 --> 01:02:51,000
 Now, imagine if you're a good one doctor.

1009
01:02:51,000 --> 01:02:52,000
 Right?

1010
01:02:52,000 --> 01:02:55,000
 You want to build an application, you want to build a machine learning, let's say,

1011
01:02:55,000 --> 01:02:58,000
 an answer-screen application for an author.

1012
01:02:58,000 --> 01:03:04,000
 If you have a doctor trust that either by sigmoid and relu and some phalency maps,

1013
01:03:04,000 --> 01:03:09,000
 and I have done some attention and 20 other technical terms, and even it works.

1014
01:03:09,000 --> 01:03:12,000
 Or they will say that, come up with a set of rules.

1015
01:03:12,000 --> 01:03:19,000
 If the patient has, you know, decaying cell function and patient has some swelling, etc.

1016
01:03:19,000 --> 01:03:21,000
 Then the patient is like you have to ask them.

1017
01:03:21,000 --> 01:03:23,000
 Which of them do you think they're doctor?

1018
01:03:23,000 --> 01:03:26,000
 Let me share it.

1019
01:03:26,000 --> 01:03:28,000
 Probably the second one.

1020
01:03:28,000 --> 01:03:30,000
 Because that is called interpretive.

1021
01:03:30,000 --> 01:03:32,000
 This is also about many of those things.

1022
01:03:32,000 --> 01:03:38,000
 If we go back to this example, and if you think in your life, if you were to play tennis,

1023
01:03:38,000 --> 01:03:42,000
 what is this kind of guy I'm using?

1024
01:03:42,000 --> 01:03:47,000
 One, I don't know if he's just got a picture of the force, he'll be able to get someone to play with me.

1025
01:03:47,000 --> 01:03:49,000
 He cannot play with me.

1026
01:03:49,000 --> 01:03:53,000
 But then of course, you'll not want to play with it, that's very hard.

1027
01:03:53,000 --> 01:03:56,000
 You do not want to play with it, it's very human.

1028
01:03:56,000 --> 01:04:05,000
 Looking at this, playing a meetup of a specific example, can you tell me some of the times you'll definitely play an operator?

1029
01:04:11,000 --> 01:04:13,000
 As I interpret it as overcast, I will always play.

1030
01:04:13,000 --> 01:04:15,000
 I mean, tell me something like that.

1031
01:04:15,000 --> 01:04:23,000
 Okay, in fact, when it is overcast, you see, you are not here always playing.

1032
01:04:23,000 --> 01:04:26,000
 Just look at these specific roles.

1033
01:04:26,000 --> 01:04:29,000
 So can you tell me how the taste and pull kind of rules?

1034
01:04:29,000 --> 01:04:31,000
 If it is overcast, I will definitely play.

1035
01:04:31,000 --> 01:04:34,000
 If it is not overcast, it is semi.

1036
01:04:34,000 --> 01:04:38,000
 And let's say that the temperature is mild, then I will also play.

1037
01:04:38,000 --> 01:04:44,000
 This is how we find to play a real structure, which is the output of the difference between people.

1038
01:04:45,000 --> 01:04:51,000
 So this is what we both learn from machine learning, as well as called different languages.

1039
01:04:51,000 --> 01:04:57,000
 So three, you can see you have some rules, you have some branches, and you have some groups.

1040
01:04:57,000 --> 01:05:00,000
 These are also telling you what is different.

1041
01:05:00,000 --> 01:05:06,000
 We saw previously, if it is overcast, we will always play with this.

1042
01:05:06,000 --> 01:05:11,000
 If it is, if the output is overcast, you play with this.

1043
01:05:11,000 --> 01:05:16,000
 But if the output is semi, and the humidity is lower, it is still clear.

1044
01:05:16,000 --> 01:05:22,000
 It is only when the humidity is high, and the output is sunny, I will not only play.

1045
01:05:22,000 --> 01:05:26,000
 Similarly, if the output is raining, but the wind is weak, I can still play.

1046
01:05:26,000 --> 01:05:30,000
 But if the wind is strong, maybe I will not be able to play.

1047
01:05:30,000 --> 01:05:33,000
 Or this specific example.

1048
01:05:33,000 --> 01:05:38,000
 So this is what we hope to learn using our own model.

1049
01:05:38,000 --> 01:05:45,000
 We have data, we have some performance images, what the accuracy of the fields are,

1050
01:05:45,000 --> 01:05:50,000
 and we have experience coming from this data.

1051
01:05:50,000 --> 01:05:56,000
 So interestingly, what is an optimum decision tree that we can learn?

1052
01:05:56,000 --> 01:05:58,000
 So what is the optimum tree that we can learn?

1053
01:05:58,000 --> 01:06:00,000
 We have learned one such tree.

1054
01:06:00,000 --> 01:06:02,000
 Could you have learned many such trees?

1055
01:06:02,000 --> 01:06:07,000
 Could you have learned tree where the output appears above one of the attributes appears below?

1056
01:06:07,000 --> 01:06:10,000
 So there is a very old table.

1057
01:06:10,000 --> 01:06:12,000
 Does everyone recognize the same?

1058
01:06:12,000 --> 01:06:14,000
 Rather, M is the best.

1059
01:06:14,000 --> 01:06:16,000
 Where have you started?

1060
01:06:16,000 --> 01:06:20,000
 Ciala, Ciala, which is your algorithm's book.

1061
01:06:20,000 --> 01:06:22,000
 It is by the same author.

1062
01:06:22,000 --> 01:06:25,000
 This was published in the 19th century.

1063
01:06:25,000 --> 01:06:30,000
 Where the measure of constructing optimum binding to this tree is NP-complete.

1064
01:06:30,000 --> 01:06:34,000
 Which means that we can have so many different trees.

1065
01:06:34,000 --> 01:06:37,000
 We do not want to tell which is the best.

1066
01:06:37,000 --> 01:06:39,000
 This is a should be nothing to learn.

1067
01:06:39,000 --> 01:06:44,000
 In such cases, what do you typically do?

1068
01:06:44,000 --> 01:06:49,000
 When I talk about the possibility tree, is it not?

1069
01:06:49,000 --> 01:06:54,000
 It is called the possibility and when you have the principal tree.

1070
01:06:54,000 --> 01:07:00,000
 So my answer is a concern for all the property trees, but that operation is then the computer.

1071
01:07:00,000 --> 01:07:10,000
 So we need some of the solution to tell which is the good tree.

1072
01:07:10,000 --> 01:07:13,000
 That is the ability to end up creating good trees.

1073
01:07:13,000 --> 01:07:18,000
 And which is good in the same way to classify or predict after it.

1074
01:07:18,000 --> 01:07:21,000
 But we cannot enumerate all possibilities.

1075
01:07:21,000 --> 01:07:25,000
 So we end up using something called seperidial.

1076
01:07:26,000 --> 01:07:28,000
 So greedy means literally greedy.

1077
01:07:28,000 --> 01:07:37,000
 And the intuition is that at each level of the tree, we choose an attribute that gives us the biggest estimated performance gain.

1078
01:07:37,000 --> 01:07:40,000
 We are looking at some performance standards.

1079
01:07:40,000 --> 01:07:46,000
 We want something which is given as the best performance gain, but we are only able to estimate it.

1080
01:07:46,000 --> 01:07:50,000
 We are not getting the entire accurate performance gain.

1081
01:07:50,000 --> 01:07:52,000
 We cannot get that.

1082
01:07:53,000 --> 01:07:56,000
 But greedy ones can be really bad.

1083
01:07:56,000 --> 01:07:59,000
 Imagine that this is you.

1084
01:07:59,000 --> 01:08:02,000
 You want to now, you are very impatient.

1085
01:08:02,000 --> 01:08:04,000
 You want to reach them early, you are very concrete.

1086
01:08:04,000 --> 01:08:07,000
 You see that there is no car over here.

1087
01:08:07,000 --> 01:08:11,000
 You try and take a left.

1088
01:08:11,000 --> 01:08:17,000
 And then you, because you have only seen the same spot, you are not able to see this, these huge tracks.

1089
01:08:18,000 --> 01:08:20,000
 You take a left over here.

1090
01:08:20,000 --> 01:08:23,000
 And then you perpetually caught behind these various global graphs.

1091
01:08:23,000 --> 01:08:25,000
 They are moving at 30 degrees.

1092
01:08:25,000 --> 01:08:30,000
 It would have been better if you have just stayed at this point, right, in the long run.

1093
01:08:30,000 --> 01:08:33,000
 So what we have done here is to take a very greedy position.

1094
01:08:33,000 --> 01:08:39,000
 We have not considered the global picture or we have not seen very much into the future.

1095
01:08:39,000 --> 01:08:42,000
 We have just seen what is the best I can do right now.

1096
01:08:42,000 --> 01:08:46,000
 We just take a left and then I will be caught in these global graphs.

1097
01:08:46,000 --> 01:08:50,000
 So this is just to show that greedy is not optimal.

1098
01:08:50,000 --> 01:08:54,000
 Now we come up with the first algorithm.

1099
01:08:54,000 --> 01:09:02,000
 This particular algorithm is called IDP of some other variations of a decision-free algorithm.

1100
01:09:02,000 --> 01:09:05,000
 We have created three specific arguments.

1101
01:09:05,000 --> 01:09:09,000
 The first is examples target attribute and arguments.

1102
01:09:09,000 --> 01:09:13,000
 Can anyone tell me what you think the target attribute is?

1103
01:09:14,000 --> 01:09:16,000
 So the output.

1104
01:09:16,000 --> 01:09:21,000
 So the output attribute is what the response variable or the output variable will actually

1105
01:09:21,000 --> 01:09:23,000
 tell us somehow.

1106
01:09:23,000 --> 01:09:26,000
 What do you think are the attributes?

1107
01:09:26,000 --> 01:09:31,000
 The features here, which were outlook, event, humidity, etc.

1108
01:09:31,000 --> 01:09:32,000
 are those specific features.

1109
01:09:32,000 --> 01:09:34,000
 What are the examples?

1110
01:09:34,000 --> 01:09:41,000
 Examples are the set of examples are basically the matrix that we have, right, which contains

1111
01:09:41,000 --> 01:09:45,000
 the target attribute, suddenly, and the input.

1112
01:09:45,000 --> 01:09:48,000
 We will start with the algorithm by grading root node.

1113
01:09:48,000 --> 01:09:52,000
 So in the previous case, we wrote the first cause outcome.

1114
01:09:52,000 --> 01:09:57,000
 We will first of all this in chance period with an empty root node.

1115
01:09:57,000 --> 01:10:01,000
 So say that if all the examples are positive or negative or yes or no,

1116
01:10:01,000 --> 01:10:05,000
 then return root to the label class and make it up to you.

1117
01:10:05,000 --> 01:10:09,000
 You understand what this means.

1118
01:10:10,000 --> 01:10:20,000
 So if all of the examples were yes or no, do we need a decision tree or can we directly

1119
01:10:20,000 --> 01:10:22,000
 always predict the address?

1120
01:10:22,000 --> 01:10:24,000
 We can always predict it to the yes, right.

1121
01:10:24,000 --> 01:10:26,000
 There is no addition involved.

1122
01:10:26,000 --> 01:10:33,000
 There is no specific attribute with the steady many, you are all this limited.

1123
01:10:33,000 --> 01:10:39,000
 If all the examples, if the attributes is empty, then return root with the most common

1124
01:10:39,000 --> 01:10:41,000
 value of target attribute in examples.

1125
01:10:41,000 --> 01:10:43,000
 We will come to this later.

1126
01:10:43,000 --> 01:10:45,000
 But for now, let's look at the recursive procedure.

1127
01:10:45,000 --> 01:10:50,000
 This is a very nice, intuitive algorithm, which can work with the algorithm.

1128
01:10:50,000 --> 01:10:57,000
 You first become an attribute A, which best classifies the example.

1129
01:10:57,000 --> 01:11:02,000
 Now if we go back to this tree, we will fit the first attributes as output.

1130
01:11:02,000 --> 01:11:09,000
 So the inclusion of the outlook will help us get the best estimated performance gain

1131
01:11:09,000 --> 01:11:17,000
 or does the best R to be viewed for classified examples.

1132
01:11:17,000 --> 01:11:22,000
 How does the notion of best coming will look at an event?

1133
01:11:22,000 --> 01:11:28,000
 Now the output A was chosen as, the output was chosen as A.

1134
01:11:28,000 --> 01:11:31,000
 What are the values that the R to be adopted by the table?

1135
01:11:32,000 --> 01:11:37,000
 Sunny, Sunny over task, alright, right.

1136
01:11:37,000 --> 01:11:44,000
 Now for each of these attributes, can it call the same concept because of your time?

1137
01:11:44,000 --> 01:11:47,000
 That is the simple definition to your algorithm.

1138
01:11:47,000 --> 01:11:51,000
 At each level, you keep on taking the same variable.

1139
01:11:51,000 --> 01:12:00,000
 Let's, so we set the root to be K and for each value of A,

1140
01:12:00,000 --> 01:12:10,000
 which was Sunny over task and really, you then add new pre-launch

1141
01:12:10,000 --> 01:12:14,000
 and you also reduce your number of examples from R, right.

1142
01:12:14,000 --> 01:12:21,000
 You restrict the set of examples where the active A was the particular value of B.

1143
01:12:21,000 --> 01:12:29,000
 And if examples is empty, you add new to the level of most common value of time.

1144
01:12:30,000 --> 01:12:35,000
 Otherwise, you call the same procedure after having removed the set of items,

1145
01:12:35,000 --> 01:12:39,000
 or the remaining subset of the data.

1146
01:12:39,000 --> 01:12:45,000
 So this is only complicated just code of commit will get into the details of it.

1147
01:12:45,000 --> 01:12:53,000
 But before that, we had talked about having the best estimated performance gain, right.

1148
01:12:53,000 --> 01:12:55,000
 How do we quantify R?

1149
01:12:55,000 --> 01:12:58,000
 If you come up with a metric of the common with the set of still measure,

1150
01:12:58,000 --> 01:13:03,000
 Mona's identity, what do you think entropy means in general?

1151
01:13:03,000 --> 01:13:06,000
 You would have started from the monad.

1152
01:13:06,000 --> 01:13:11,000
 A randomness or some impurity in sample.

1153
01:13:11,000 --> 01:13:16,000
 If you see a set like this, can it tell you what is the entropy of this?

1154
01:13:16,000 --> 01:13:24,000
 Randomness or disorder or the amount of unclarity in parallel is the right.

1155
01:13:25,000 --> 01:13:27,000
 Do you know the formula?

1156
01:13:27,000 --> 01:13:30,000
 Have you studied the formula of entropy?

1157
01:13:30,000 --> 01:13:33,000
 First we have to do the first one.

1158
01:13:33,000 --> 01:13:37,000
 So definition of entropy is given.

1159
01:13:37,000 --> 01:13:41,000
 You have five rows and nine rows, right.

1160
01:13:41,000 --> 01:13:45,000
 Imagine all of these 14 were yes, right.

1161
01:13:45,000 --> 01:13:48,000
 We have an disorder in the system.

1162
01:13:48,000 --> 01:13:51,000
 You have anything which is uncertain.

1163
01:13:51,000 --> 01:13:55,000
 So then in such cases, we will say the entropy is zero, right.

1164
01:13:55,000 --> 01:14:01,000
 If the answer was seven, yes, and seven rows, what could you think is the entropy?

1165
01:14:01,000 --> 01:14:08,000
 It would be very high, right, because you are very unsure about whether it should be a yes or a no.

1166
01:14:08,000 --> 01:14:14,000
 So if formulas are given in terms of minus p log, the summation minus p log p,

1167
01:14:14,000 --> 01:14:19,000
 for the different classes, we had probability of no as five by 14,

1168
01:14:19,000 --> 01:14:23,000
 we had 14 examples, five of which were no.

1169
01:14:23,000 --> 01:14:28,000
 And nine of them are yes, and log base to probability of no.

1170
01:14:28,000 --> 01:14:34,000
 So if you do the calculation, it comes out pretty 0.9, which is the daily high number.

1171
01:14:34,000 --> 01:14:37,000
 We also get a curve by this.

1172
01:14:37,000 --> 01:14:42,000
 So probability of yes was zero, the entropy is zero, right.

1173
01:14:42,000 --> 01:14:46,000
 So this means that there is no disorder in all the examples negative.

1174
01:14:46,000 --> 01:14:51,000
 And if you look at the other extreme, the probability of class or yes is one,

1175
01:14:51,000 --> 01:14:55,000
 there is a gain or entropy because there is no uncertainty.

1176
01:14:55,000 --> 01:15:02,000
 The maximum uncertainty or ability happens when the probability of class is the same as probability of that.

1177
01:15:02,000 --> 01:15:03,000
 Right?

1178
01:15:03,000 --> 01:15:08,000
 So we will start up with calculating an entropy of a set.

1179
01:15:08,000 --> 01:15:15,000
 And we now want to use an attitude A, which is able to give us the biggest performance gain.

1180
01:15:15,000 --> 01:15:20,000
 So can you think of in terms of starting with entropy as a starting point?

1181
01:15:20,000 --> 01:15:27,000
 What manipulation or what are the statistical measures we need to tell which is the best attribute?

1182
01:15:27,000 --> 01:15:30,000
 Please get it.

1183
01:15:30,000 --> 01:15:32,000
 Please get it.

1184
01:15:32,000 --> 01:15:34,000
 One is the objective.

1185
01:15:34,000 --> 01:15:40,000
 Okay, but how do you calculate the entropy of an amplitude?

1186
01:15:40,000 --> 01:15:44,000
 For me, to think of it in terms of we have to choose an attribute.

1187
01:15:44,000 --> 01:15:50,000
 So before, before it started up with the stationary learning, you had several examples.

1188
01:15:50,000 --> 01:15:53,000
 You can calculate the entropy of that sample.

1189
01:15:53,000 --> 01:15:58,000
 Now you want to choose an attribute which will lower the entropy.

1190
01:15:58,000 --> 01:15:59,000
 Right?

1191
01:15:59,000 --> 01:16:03,000
 So basically we go at this point.

1192
01:16:03,000 --> 01:16:07,000
 So we said that there was a lot of uncertainty in whether or not the weight end is on.

1193
01:16:07,000 --> 01:16:08,000
 Right?

1194
01:16:08,000 --> 01:16:09,000
 Which means that there is a lot of memory.

1195
01:16:09,000 --> 01:16:15,000
 But if I choose output as overclass, I know that I will definitely get it.

1196
01:16:15,000 --> 01:16:16,000
 Right?

1197
01:16:16,000 --> 01:16:21,000
 So there is no element of entropy involved there for those specific examples.

1198
01:16:21,000 --> 01:16:22,000
 Right?

1199
01:16:22,000 --> 01:16:26,000
 So we will now see that we are trying to choose an attribute.

1200
01:16:26,000 --> 01:16:28,000
 Subject will choose an image.

1201
01:16:28,000 --> 01:16:31,000
 The entropy becomes lower.

1202
01:16:31,000 --> 01:16:32,000
 Right?

1203
01:16:32,000 --> 01:16:34,000
 There is a disorder in this system.

1204
01:16:34,000 --> 01:16:37,000
 So that concept is known as information gain.

1205
01:16:38,000 --> 01:16:41,000
 I just look at the, I just show the formula.

1206
01:16:41,000 --> 01:16:53,000
 Information gain is known as the formula is given in terms of production and entropy.

1207
01:16:53,000 --> 01:16:58,000
 By partitioning a set of examples, S on an attribute gain.

1208
01:16:58,000 --> 01:17:01,000
 So an attribute gain would have taken different values.

1209
01:17:01,000 --> 01:17:05,000
 For example, the output traffic in sunny, rainy or overclass.

1210
01:17:05,000 --> 01:17:14,000
 We then write the gain on a set of examples S subject to an attribute A as defined by

1211
01:17:14,000 --> 01:17:16,000
 the entropy which is the initial system.

1212
01:17:16,000 --> 01:17:23,000
 Of all the examples, minus the weighted entropy, weighted by the number of samples we have

1213
01:17:23,000 --> 01:17:24,000
 for a particular value.

1214
01:17:24,000 --> 01:17:29,000
 But in fact, in the entropy of the subsets, that may get it.

1215
01:17:29,000 --> 01:17:30,000
 Right?

1216
01:17:30,000 --> 01:17:32,000
 I will not go into the details.

1217
01:17:32,000 --> 01:17:36,000
 Now let us assume that at this point of time we have four data examples.

1218
01:17:36,000 --> 01:17:39,000
 We choose an output as the first node.

1219
01:17:39,000 --> 01:17:42,000
 The entropy of this set is zero.

1220
01:17:42,000 --> 01:17:44,000
 The weighted entropy of this set is also zero.

1221
01:17:44,000 --> 01:17:47,000
 This side will have some weighted entropy.

1222
01:17:47,000 --> 01:17:49,000
 So let us stop at this point with the input.

1223
01:17:49,000 --> 01:17:55,000
 We will understand that we are trying to choose an attribute which reduces the entropy.

1224
01:17:55,000 --> 01:18:00,000
 Let us leave at 11am on our target.

1225
01:18:00,000 --> 01:18:04,000
 Thank you.

