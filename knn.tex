\documentclass[usenames,dvipsnames]{beamer}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{makecell}
\usepackage[labelformat=empty]{caption}
\usepackage{color, colortbl}
\usepackage{booktabs}
\usepackage{pdfpages}
\usepackage{multirow}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\newcommand*{\mathcolor}{}
\def\mathcolor#1#{\mathcoloraux{#1}}
\newcommand*{\mathcoloraux}[3]{%
	\protect\leavevmode
	\begingroup
	\color#1{#2}#3%
	\endgroup
}

\usepackage{tikz}

\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=brown!30]

\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]

\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

\tikzstyle{arrow} = [thick,->,>=stealth]


\newcommand{\DoTikzmark}[1]{%
	\tikz[remember picture] \coordinate[shift={(0,.7ex)}](#1);%
}

\newcommand{\colrow}[3][]{%
	\tikz[overlay,remember picture, line width=10pt]
	\draw[shorten >=-.1em, shorten <=-.1em, #1] (#2)--(#3);
}


\newcolumntype{g}{>{\columncolor{Lavender}}c}
\newcolumntype{t}{>{\columncolor{Tan}}c}

%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}
	


\usetheme{metropolis}           % Use metropolis theme


\title{K-Nearest Neighbors}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle

\begin{frame}{Lazy vs Eager Strategies}
\begin{table}[]
\begin{tabular}{|l|p{3.5cm}|p{3.5cm}|}
\hline
           & Lazy                                     & Eager                                   \\
           \hline
Train Time & $0$                                      & $\neq 0$                                \\
\hline
Test       & Long (due to comparison with train data) & Quick (as only ``parameters'' are involved) \\
\hline
Memory     & Store/Memorise entire data               & Store only learnt parameters            \\
\hline
Utility    & Useful for online settings               &                                         \\
\hline
Examples   & KNN                                      & Linear Regression, Decision Tree       \\ \hline
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Parametric vs Non-Parametric Models}
\begin{table}[]
\begin{tabular}{|l|p{3.5cm}|p{3.5cm}|}
\hline
            & Parametric                                               & Non-Parametric                                                   \\ \hline
Parameter   & Number of parameters is fixed w.r.t dataset size         & Number of parameters grows w.r.t. to an increase in dataset size \\ \hline
Speed       & Quicker (as the number of parameters are less)           & Longer (as number of parameters are less)                        \\ \hline
Assumptions & Strong Assumptions (like linearity in Linear Regression) & Very few (sometimes no) assumptions                              \\ \hline
Examples    & Linear Regression                                        & KNN, Decision Tree                     \\ \hline                         
\end{tabular}
\end{table}
\end{frame}

{
\setbeamercolor{background canvas}{bg=}
\includepdf[page=-]{knn/knn-intuition.pdf}
}

\begin{frame}{Important Considerations}
\begin{itemize}
\item<1-> What are the \textbf{features} that will be considered for data similarity?
\item<2-> What is the \textbf{distance metric} that will be used to calculate data similarity?
\item<3-> \checkmark What is the \textbf{aggregation function} that is going to be used?
\item<4-> What are the \textbf{number of neighbors} that you are going to take into consideration?
\item<5-> \checkmark \textbf{computational complexity} 
\end{itemize}
\end{frame}

\begin{frame}{Important Considerations: Distance Metric}
The Distance Metric acts as a \emph{measure of similarity} between the points.

{\centering
\only<2>{\includegraphics[height=0.6\textheight]{knn/knn_ed.png}
\captionof{figure}{Euclidean Distance}}
\only<3>{\includegraphics[height=0.6\textheight]{knn/knn_hd.pdf}
\captionof{figure}{Hamming Distance}}
\only<4>{\includegraphics[height=0.6\textheight]{knn/knn_md.pdf}
\captionof{figure}{Manhattan Distance}}}

\only<5>{
Choose the best distance metric \textbf{based on the properties of the data}. 

Euclidean is a good distance measure to use if the input variables are similar in type (e.g. all measured widths and heights). Manhattan distance is a good measure to use if the input variables are not similar in type (such as age, gender, height, etc.).

\href{https://colab.research.google.com/drive/1HMPn0mpMAe4XFe5Zvh4oExgi5evkgjTi}{More Info}}
\end{frame}

\begin{frame}{Important Considerations: Value of \emph{K}}
Choosing the correct value of \emph{K} is difficult.
\pause

\textbf{Low values of K} will result in each point having a very high influence on the final output $\implies$ noise will influence the result
\pause

\textbf{High values of K} will result in smoother decision boundaries \\$\implies$ lower variance but also higher bias
\end{frame}

\begin{frame}{KNN Algorithm}
\begin{itemize}
\item<1-> Keep the entire dataset: ${(x,y)}$
\item<2-> For a query vector $q$:
\begin{enumerate}
\item<3-> Find the k-closest data point(s) $x^*$
\item<4-> Predict $y^*$
\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}{Curse of Dimensionality}
With an increase in the number of dimensions:
\begin{enumerate}
\item<2-> the distance between points starts to increase
\item[]<2> {\centering \includegraphics[height=0.6\textheight]{knn/curse_dist.pdf}}
\captionof{figure}{For a unifromly random dataset}
\end{enumerate}
\end{frame}

\begin{frame}{Curse of Dimensionality}
With an increase in the number of dimensions:
\begin{enumerate}
\item the distance between points starts to increase
\item the variation in distances between points starts to decrease
\item[] {\centering \includegraphics[height=0.6\textheight]{knn/curse_spread.pdf}}
\captionof{figure}{For a unifromly random dataset}
\end{enumerate}
\end{frame}

\begin{frame}{Curse of Dimensionality}
With an increase in the number of dimensions:
\begin{enumerate}
\item the distance between points starts to increase
\item the variation in distances between points starts to decrease
\end{enumerate}
Due to this, distance metrics lose their efficacy as a similarity metric.
\end{frame}


\end{document}